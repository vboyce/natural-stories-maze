---
title: "Provo prep"
output: github_document
---


```{r}
library(tidyverse)
library(readr)
```
What we're trying to get out of this:
- List of words/sentences/paragraphs that are reconstructable to each other and labelled by word/sentence/item number.
- Surprisals, frequency, etc for each word

Read in a bunch of things.

Fundamentally want sentences which should match paragraphs, then we can check then line up and separate on spaces to make words. (will also need punct. stripped words for some purposes.)

# New attempt
Texts were copied from the predictability norms and then hand broken into sentences (because this is actually faster than anything I know how to do automatically). Some light editing, such as turning "and/or" into "or" and removing internal quote marks was done.
```{r}
sentences <- read_csv("edited_sentences.csv") %>% rename(Sentence=Text)

paragraphs <- sentences %>% 
  pivot_wider(names_from="Sentence_Number", values_from="Sentence") %>%
  unite("Text", `1`:`4`, sep=" ", na.rm=T)

nums <- 1:55 %>% as.character()

words <- sentences %>%
  separate(Sentence, into=nums, sep=fixed(" ")) %>%
  pivot_longer(cols=`1`:`55`,names_to = "Word_In_Sentence_Number", values_to="Word") %>% 
  filter(!is.na(Word)) %>% 
  left_join(sentences, by=c("Text_ID", "Sentence_Number")) %>% 
  left_join(paragraphs, by=c("Text_ID")) %>% 
  group_by(Text_ID) %>% 
  mutate(Word_In_Paragraph_Number=row_number()) %>% 
  type_convert() %>% 
  select(Text_ID, Sentence_Number, Word_In_Sentence_Number, Word_In_Paragraph_Number, Word, Sentence, Text) %>% 
  write_csv("words.csv")


```
## Get data
We want word frequency, surprisal, and length for each word.

For word frequency, want to take the words, and use word freq. Frequency is zipfian - log10 of number of occurances in 10^9 words.
```{r}

freq <- read_csv("freq.csv", col_names = c("Text_ID", "Sentence_Number", "Word_In_Sentence_Number","Word", "freq")) %>% 
  mutate(freq=ifelse(freq==0, NA, freq))

labelled <- words %>% left_join(freq)

```

For surprisal.
We tokenize in python using nltk. However, tokenize on words behaves differently than tokenize on sentences, so we have to hand fix two problems. (On sentences, "Mr." and "B." are tokenized as single characters, but if you tokenize by word it gets split b/c period is at end?). 

This whole adventure means we can add up the surprisals for multi-token words.

```{r}

sents <- words %>% select(Text_ID, Sentence_Number, Sentence) %>% unique() %>% write_csv("sents.csv")

tokens <- read_csv("tokens2.csv", col_names=c("word","1","2")) %>% 
  bind_cols(freq) %>% 
  pivot_longer(cols="1":"2",names_to="position", values_to="token") %>% 
  filter(!is.na(token))
  
surp <- read_tsv("surp.txt", col_names=c("token_unk","surp")) %>%
  filter(token_unk!="<eos>") %>%
  bind_cols(tokens) %>% 
  mutate(surp=ifelse(surp==0, NA, surp)) %>% 
  mutate(surp=ifelse(token_unk=="<unk>", NA, surp)) %>% 
  select(surp,Word, Text_ID, Sentence_Number, Word_In_Sentence_Number, freq) %>% 
  group_by(Text_ID, Sentence_Number, Word_In_Sentence_Number, Word, freq) %>% 
  summarize(surp=sum(surp)) %>% 
  ungroup() %>% 
  mutate(word_len=str_length(Word)) %>% 
  select(Text_ID, Sentence_Number, Word_In_Sentence_Number, Word, freq, surp, word_len) %>%
  left_join(words, by=c("Text_ID", "Sentence_Number", "Word_In_Sentence_Number", "Word")) %>% 
    mutate(surp_cent=surp-mean(surp, na.rm=T),
           word_len_cent=word_len-mean(word_len,na.rm=T),
           freq_cent=freq-mean(freq, na.rm=T)) %>% #centered
  write_rds("surprisal.rds")
  
```
For both surprisal and frequency, values of 0 were converted to NAs because the sources use 0 as a unknown marker, so those values are untrustworthy and should not be included. 

Both centered and non-centered versions of the variables are included because the non-centered are more interpretable for graphs (potentially), but the analyses want centered for interpretable intercepts.
## Maze prep & re-prep


```{r}

for_maze <- sents %>% ungroup() %>%  mutate(label=str_c(Text_ID, Sentence_Number, sep="_"), num=row_number()) %>% select(label, num, Sentence) %>% write_delim("provo_pre_maze.txt", delim=";", col_names=F, quote_escape = F)


```

Run them using  ./automate_new.py provo_pre_maze.txt provo_out.txt --model gulordava --freq wordfreq --format basic --num_to_test 100 --min_abs 25 --min_rel 15

Takes maze materials and writes them into paragraphs. 
```{r}
maze <- read_delim("provo_out.txt", delim=";", col_names = c("label", "number", "sentence", "distractors", "blah")) %>%
  select(-blah, -number) %>%
  separate(label, into=c("Text_ID", "Sentence_Number")) %>%
  pivot_wider(names_from=c("Sentence_Number"), values_from=c("sentence", "distractors"), values_fill=list(sentence="",distractors=""))%>% 
  unite("Text", sentence_1:sentence_4, sep=" ") %>% 
  unite("Distractor", distractors_1:distractors_4, sep=" ") %>%
  mutate(Text=trimws(Text), Distractor=trimws(Distractor))

write_lines(with(maze, sprintf('["critical","%s","%s","%s"],', Text_ID, Text, Distractor)), path="critical.txt")

```

