@String{cogsciConference2021 = {Proceedings of the 43rd Annual Meeting of the {Cognitive} {Science} {Society}}}
@String{tics = {Trends in Cognitive Sciences}}
@String{cogsci = {Cognitive Science}}
@String{pnas = {Proceedings of the National Academy of Sciences}}
@String{jml = {Journal of Memory and Language}}
@String{naacl2019 = {Proceedings of the 18th {Annual} {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}}}
@String{emnlp2018 = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}}}
@String{tacl = {Transactions of the Association for Computational Linguistics}}
@String{naacl2 = {Proceedings of the Second Meeting of the North American Chapter of the {Association} for {Computational} {Linguistics}}}


@article{barrRandomEffectsStructure2013,
  ids = {barr-etal:2013jml},
  title = {Random Effects Structure for Confirmatory Hypothesis Testing: {{Keep}} It Maximal},
  shorttitle = {Random Effects Structure for Confirmatory Hypothesis Testing},
  author = {Barr, Dale J. and Levy, Roger and Scheepers, Christoph and Tily, Harry J.},
  year = {2013},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {68},
  number = {3},
  pages = {255--278},
  issn = {0749-596X},
  abstract = {Linear mixed-effects models (LMEMs) have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we argue that researchers using LMEMs for confirmatory hypothesis testing should minimally adhere to the standards that have been in place for many decades. Through theoretical arguments and Monte Carlo simulation, we show that LMEMs generalize best when they include the maximal random effects structure justified by the design. The generalization performance of LMEMs including data-driven random effects structures strongly depends upon modeling criteria and sample size, yielding reasonable results on moderately-sized samples when conservative criteria are used, but with little or no power advantage over maximal models. Finally, random-intercepts-only LMEMs used on within-subjects and/or within-items data from populations where subjects and/or items vary in their sensitivity to experimental manipulations always generalize worse than separate F1 and F2 tests, and in many cases, even worse than F1 alone. Maximal LMEMs should be the `gold standard' for confirmatory hypothesis testing in psycholinguistics and beyond.},
  date-added = {2011-09-08 22:51:09 +0000},
  date-modified = {2019-02-14 12:57:58 -0500},
  langid = {english},
  rogerslocalurl = {papers/barr-etal-2013-jml.pdf},
  topic = {Statistical modeling},
  keywords = {Generalization,Linear mixed-effects models,Monte Carlo simulation,read,Statistics},
  file = {/home/vboyce/Zotero/storage/DMUW47IM/Barr et al. - 2013 - Random effects structure for confirmatory hypothes.pdf;/home/vboyce/Zotero/storage/8YUFVVYF/S0749596X12001180.html}
}

@article{bartekSearchOnlineLocality2011,
  title = {In Search of On-Line Locality Effects in Sentence Comprehension.},
  author = {Bartek, Brian and Lewis, Richard L and Vasishth, Shravan and Smith, Mason R},
  year = {2011},
  journal = {Journal of Experimental Psychology: Human Perception \& Performance},
  volume = {37},
  number = {5},
  pages = {1178},
  publisher = {{American Psychological Association}},
  date-added = {2019-06-17 10:18:24 -0400},
  date-modified = {2019-06-17 10:18:53 -0400},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/C2B4XG2G/Bartek et al. - 2011 - In search of on-line locality effects in sentence .pdf}
}

@article{batesFittingLinearMixedeffects2015,
  title = {Fitting Linear Mixed-Effects Models Using Lme4},
  author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {67},
  number = {1},
  pages = {1--48},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/H9PL9IKR/Bates et al. - 2015 - Fitting linear mixed-effects models using span cl.pdf}
}

@article{boyceMazeMadeEasy2020,
  ids = {boyceMazeMadeEasy2020a},
  title = {Maze {{Made Easy}}: {{Better}} and Easier Measurement of Incremental Processing Difficulty},
  shorttitle = {Maze {{Made Easy}}},
  author = {Boyce, Veronica and Futrell, Richard and Levy, Roger P.},
  year = {2020},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {111},
  pages = {104082},
  issn = {0749-596X},
  abstract = {Behavioral measures of incremental language comprehension difficulty form a crucial part of the empirical basis of psycholinguistics. The two most common methods for obtaining these measures have significant limitations: eye tracking studies are resource-intensive, and self-paced reading can yield noisy data with poor localization. These limitations are even more severe for web-based crowdsourcing studies, where eye tracking is infeasible and self-paced reading is vulnerable to inattentive participants. Here we make a case for broader adoption of the Maze task, involving sequential forced choice between each successive word in a sentence and a contextually inappropriate distractor. We leverage natural language processing technology to automate the most researcher-laborious part of Maze \textendash{} generating distractor materials \textendash{} and show that the resulting A(uto)-Maze method has dramatically superior statistical power and localization for well-established syntactic ambiguity resolution phenomena. We make our code freely available online for widespread adoption of A-maze by the psycholinguistics community.},
  langid = {english},
  keywords = {A-maze,G-maze,Maze task,read,Self-paced reading,Sentence processing},
  file = {/home/vboyce/Zotero/storage/5HFVHZZI/Boyce et al. - 2020 - Maze Made Easy Better and easier measurement of i.pdf;/home/vboyce/Zotero/storage/ZA3FXTF2/S0749596X19301147.html}
}

@article{burknerAdvancedBayesianMultilevel2018,
  title = {Advanced Bayesian Multilevel Modeling with the r Package Brms},
  author = {B{\"u}rkner, Paul-Christian},
  year = {2018},
  journal = {The R Journal},
  volume = {10},
  number = {1},
  pages = {395--411},
  keywords = {read}
}

@article{chmielewski2020,
  title = {An {{MTurk Crisis}}? {{Shifts}} in {{Data Quality}} and the {{Impact}} on {{Study Results}}},
  shorttitle = {An {{MTurk Crisis}}?},
  author = {Chmielewski, Michael and Kucker, Sarah C.},
  year = {2020},
  month = may,
  journal = {Social Psychological and Personality Science},
  volume = {11},
  number = {4},
  pages = {464--473},
  publisher = {{SAGE Publications Inc}},
  issn = {1948-5506},
  doi = {10.1177/1948550619875149},
  abstract = {Amazon?s Mechanical Turk (MTurk) is arguably one of the most important research tools of the past decade. The ability to rapidly collect large amounts of high-quality human subjects data has advanced multiple fields, including personality and social psychology. Beginning in summer 2018, concerns arose regarding MTurk data quality leading to questions about the utility of MTurk for psychological research. We present empirical evidence of a substantial decrease in data quality using a four-wave naturalistic experimental design: pre-, during, and post-summer 2018. During and to some extent post-summer 2018, we find significant increases in participants failing response validity indicators, decreases in reliability and validity of a widely used personality measure, and failures to replicate well-established findings. However, these detrimental effects can be mitigated by using response validity indicators and screening the data. We discuss implications and offer suggestions to ensure data quality.},
  file = {/home/vboyce/Zotero/storage/43ISG8D3/Chmielewski and Kucker - 2020 - An MTurk Crisis Shifts in Data Quality and the Im.pdf}
}

@article{daiTransformerXLAttentiveLanguage2019,
  title = {Transformer-{{XL}}: {{Attentive Language Models Beyond}} a {{Fixed-Length Context}}},
  shorttitle = {Transformer-{{XL}}},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  year = {2019},
  month = jun,
  journal = {arXiv:1901.02860 [cs, stat]},
  eprint = {1901.02860},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,read,Statistics - Machine Learning},
  file = {/home/vboyce/Zotero/storage/SFQYHUHA/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf;/home/vboyce/Zotero/storage/U5VBLMJA/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf;/home/vboyce/Zotero/storage/DPDIFG6Q/1901.html;/home/vboyce/Zotero/storage/G2XVZRLZ/1901.html}
}

@article{eyal2021,
  title = {Data Quality of Platforms and Panels for Online Behavioral Research},
  author = {Eyal, Peer and David, Rothschild and Andrew, Gordon and Zak, Evernden and Ekaterina, Damer},
  year = {2021},
  month = sep,
  journal = {Behav Res},
  issn = {1554-3528},
  doi = {10.3758/s13428-021-01694-3},
  abstract = {We examine key aspects of data quality for online behavioral research between selected platforms (Amazon Mechanical Turk, CloudResearch, and Prolific) and panels (Qualtrics and Dynata). To identify the key aspects of data quality, we first engaged with the behavioral research community to discover which aspects are most critical to researchers and found that these include attention, comprehension, honesty, and reliability. We then explored differences in these data quality aspects in two studies (N\,\textasciitilde\,4000), with or without data quality filters (approval ratings). We found considerable differences between the sites, especially in comprehension, attention, and dishonesty. In Study 1 (without filters), we found that only Prolific provided high data quality on all measures. In Study 2 (with filters), we found high data quality among CloudResearch and Prolific. MTurk showed alarmingly low data quality even with data quality filters. We also found that while reputation (approval rating) did not predict data quality, frequency and purpose of usage did, especially on MTurk: the lowest data quality came from MTurk participants who report using the site as their main source of income but spend few hours on it per week. We provide a framework for future investigation into the ever-changing nature of data quality in online research, and how the evolving set of platforms and panels performs on these key aspects.},
  langid = {english},
  file = {/home/vboyce/Zotero/storage/36NPZYML/Eyal et al. - 2021 - Data quality of platforms and panels for online be.pdf}
}

@article{forsterMazeTaskMeasuring2009,
  ids = {forster09},
  title = {The Maze Task: {{Measuring}} Forced Incremental Sentence Processing Time},
  shorttitle = {The Maze Task},
  author = {Forster, Kenneth I. and Guerrera, Christine and Elliot, Lisa},
  year = {2009},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {41},
  number = {1},
  pages = {163--171},
  publisher = {{Springer}},
  issn = {1554-3528},
  abstract = {The maze task is an online measure of sentence processing time that provides an alternative to the standard moving window version of self-paced reading. Rather than each word of the sentence being presented in succession, two words are presented at the same time, and the participant must choose which word is a grammatical continuation of the sentence. This procedure forces the reader into an incremental mode of processing in which each word must be fully integrated with the preceding context before the next word can be considered. Previous research with this technique has not considered whether it is sufficiently sensitive to syntactic complexity effects or to garden path effects. Four experiments are reported demonstrating that reliable differences in processing time for subject relatives and object relatives can be obtained, and that this technique generates garden path effects that correspond closely with the data from eyetracking experiments, but without the spillover effects that are sometimes obtained with eyetracking. It is also shown that the task is sensitive to word frequency effects, producing estimates well in excess of those found with eyetracking.},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/YCADBDXD/Forster et al. - 2009 - The maze task Measuring forced incremental senten.pdf}
}

@article{frazierMakingCorrectingErrors1982,
  title = {Making and Correcting Errors during Sentence Comprehension: {{Eye}} Movements in the Analysis of Structurally Ambiguous Sentences},
  author = {Frazier, Lyn and Rayner, Keith},
  year = {1982},
  journal = {Cognitive Psychology},
  volume = {14},
  pages = {178--210},
  keywords = {read}
}

@article{freedman85,
  title = {The Psychological Status of Overgenerated Sentences},
  author = {Freedman, Sandra E and Forster, Kenneth I},
  year = {1985},
  journal = {Cognition},
  volume = {19},
  number = {2},
  pages = {101--131},
  publisher = {{Elsevier}},
  keywords = {read}
}

@article{futrellNaturalStoriesCorpus2020,
  title = {The {{Natural Stories}} Corpus: A Reading-Time Corpus of {{English}} Texts Containing Rare Syntactic Constructions},
  shorttitle = {The {{Natural Stories}} Corpus},
  author = {Futrell, Richard and Gibson, Edward and Tily, Harry J. and Blank, Idan and Vishnevetsky, Anastasia and Piantadosi, Steven T. and Fedorenko, Evelina},
  year = {2020},
  month = sep,
  journal = {Lang Resources \& Evaluation},
  issn = {1574-0218},
  abstract = {It is now a common practice to compare models of human language processing by comparing how well they predict behavioral and neural measures of processing difficulty, such as reading times, on corpora of rich naturalistic linguistic materials. However, many of these corpora, which are based on naturally-occurring text, do not contain many of the low-frequency syntactic constructions that are often required to distinguish between processing theories. Here we describe a new corpus consisting of English texts edited to contain many low-frequency syntactic constructions while still sounding fluent to native speakers. The corpus is annotated with hand-corrected Penn Treebank-style parse trees and includes self-paced reading time data and aligned audio recordings. We give an overview of the content of the corpus, review recent work using the corpus, and release the data.},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/X393ZSV8/Futrell et al. - 2020 - The Natural Stories corpus a reading-time corpus .pdf}
}

@inproceedings{gauthierSyntaxGymOnlinePlatform2020,
  title = {{{SyntaxGym}}: {{An Online Platform}} for {{Targeted Evaluation}} of {{Language Models}}},
  shorttitle = {{{SyntaxGym}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  author = {Gauthier, Jon and Hu, Jennifer and Wilcox, Ethan and Qian, Peng and Levy, Roger},
  year = {2020},
  month = jul,
  pages = {70--76},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-demos.10},
  abstract = {Targeted syntactic evaluations have yielded insights into the generalizations learned by neural network language models. However, this line of research requires an uncommon confluence of skills: both the theoretical knowledge needed to design controlled psycholinguistic experiments, and the technical proficiency needed to train and deploy large-scale language models. We present SyntaxGym, an online platform designed to make targeted evaluations accessible to both experts in NLP and linguistics, reproducible across computing environments, and standardized following the norms of psycholinguistic experimental design. This paper releases two tools of independent value for the computational linguistics community: 1. A website, syntaxgym.org, which centralizes the process of targeted syntactic evaluation and provides easy tools for analysis and visualization; 2. Two command-line tools, `syntaxgym` and `lm-zoo`, which allow any user to reproduce targeted syntactic evaluations and general language model inference on their own machine.},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/6U3F2K4H/Gauthier et al. - 2020 - SyntaxGym An Online Platform for Targeted Evaluat.pdf}
}

@inproceedings{goodkindPredictivePowerWord2018,
  title = {Predictive Power of Word Surprisal for Reading Times Is a Linear Function of Language Model Quality},
  booktitle = {Proceedings of the 8th {{Workshop}} on {{Cognitive Modeling}} and {{Computational Linguistics}} ({{CMCL}} 2018)},
  author = {Goodkind, Adam and Bicknell, Klinton},
  year = {2018},
  month = jan,
  pages = {10--18},
  publisher = {{Association for Computational Linguistics}},
  address = {{Salt Lake City, Utah}},
  doi = {10.18653/v1/W18-0102},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/S8789CBZ/Goodkind and Bicknell - 2018 - Predictive power of word surprisal for reading tim.pdf}
}

@article{grodnerConsequencesSerialNature2005,
  title = {Some Consequences of the Serial Nature of Linguistic Input},
  author = {Grodner, Daniel and Gibson, Edward},
  year = {2005},
  journal = {Cognitive Science},
  volume = {29},
  number = {2},
  pages = {261--290},
  keywords = {read}
}

@inproceedings{gulordava18,
  title = {Colorless Green Recurrent Networks Dream Hierarchically},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
  year = {2018},
  pages = {1195--1205},
  keywords = {read}
}

@techreport{hauser2018,
  type = {Preprint},
  title = {Common {{Concerns}} with {{MTurk}} as a {{Participant Pool}}: {{Evidence}} and {{Solutions}}},
  shorttitle = {Common {{Concerns}} with {{MTurk}} as a {{Participant Pool}}},
  author = {Hauser, David and Paolacci, Gabriele and Chandler, Jesse J.},
  year = {2018},
  month = sep,
  institution = {{PsyArXiv}},
  doi = {10.31234/osf.io/uq45c},
  abstract = {In this chapter, we outline the common concerns with MTurk as a participant pool, review the evidence for those concerns, and discuss solutions. We close with a Table of considerations that researchers should make when fielding a study on MTurk},
  langid = {english},
  file = {/home/vboyce/Zotero/storage/9F84KZ7T/Hauser et al. - 2018 - Common Concerns with MTurk as a Participant Pool .pdf}
}

@article{huSystematicAssessmentSyntactic2020,
  title = {A {{Systematic Assessment}} of {{Syntactic Generalization}} in {{Neural Language Models}}},
  author = {Hu, Jennifer and Gauthier, Jon and Qian, Peng and Wilcox, Ethan and Levy, Roger P.},
  year = {2020},
  month = may,
  journal = {arXiv:2005.03692 [cs]},
  eprint = {2005.03692},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {State-of-the-art neural network models have achieved dizzyingly low perplexity scores on major language modeling benchmarks, but it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge. Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations. We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 syntactic test suites. We find that model architecture clearly influences syntactic generalization performance: Transformer models and models with explicit hierarchical structure reliably outperform pure sequence models in their predictions. In contrast, we find no clear influence of the scale of training data on these syntactic generalization tests. We also find no clear relation between a model's perplexity and its syntactic generalization performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,read},
  file = {/home/vboyce/Zotero/storage/44HMX9UV/Hu et al. - 2020 - A Systematic Assessment of Syntactic Generalizatio.pdf;/home/vboyce/Zotero/storage/C2BI4VFL/2005.html}
}

@article{klieglLengthFrequencyPredictability2004,
  title = {Length, Frequency, and Predictability Effects of Words on Eye Movements in Reading},
  author = {Kliegl, Reinhold and Grabner, Ellen and Rolfs, Martin and Engbert, Ralf},
  year = {2004},
  month = jan,
  journal = {European Journal of Cognitive Psychology},
  volume = {16},
  number = {1-2},
  pages = {262--284},
  issn = {0954-1446, 1464-0635},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/Q3DVUMLV/Kliegl et al. - 2004 - Length, frequency, and predictability effects of w.pdf}
}

@article{koornneefUseVerbbasedImplicit2006,
  title = {On the Use of Verb-Based Implicit Causality in Sentence Comprehension : {{Evidence}} from Self-Paced Reading and Eye Tracking},
  author = {Koornneef, Arnout W. and {van Berkum}, Jos J.A.},
  year = {2006},
  journal = {Journal of Memory and Language},
  volume = {54},
  number = {4},
  pages = {445--465},
  keywords = {read}
}



@article{levyIntegratingSurprisalUncertaininput2011,
  title = {Integrating Surprisal and Uncertain-Input Models in Online Sentence Comprehension: Formal Techniques and Empirical Results},
  author = {Levy, Roger},
  year = {2011},
  pages = {11},
  abstract = {A system making optimal use of available information in incremental language comprehension might be expected to use linguistic knowledge together with current input to revise beliefs about previous input. Under some circumstances, such an error-correction capability might induce comprehenders to adopt grammatical analyses that are inconsistent with the true input. Here we present a formal model of how such input-unfaithful garden paths may be adopted and the difficulty incurred by their subsequent disconfirmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty. We also present a behavioral experiment confirming the key empirical predictions of the theory.},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/JZJUJ3HB/Levy - Integrating surprisal and uncertain-input models i.pdf}
}

@article{lukeLimitsLexicalPrediction2016,
  title = {Limits on Lexical Prediction during Reading},
  author = {Luke, {{Steven G.}} and Christianson, Kiel},
  year = {2016},
  month = aug,
  journal = {Cognitive Psychology},
  volume = {88},
  pages = {22--60},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2016.06.002},
  abstract = {Efficient language processing may involve generating expectations about upcoming input. To investigate the extent to which prediction might facilitate reading, a large-scale survey provided cloze scores for all 2689 words in 55 different text passages. Highly predictable words were quite rare (5\% of content words), and most words had a more-expected competitor. An eye-tracking study showed sensitivity to cloze probability but no mis-prediction cost. Instead, the presence of a more-expected competitor was found to be facilitative in several measures. Further, semantic and morphosyntactic information was highly predictable even when word identity was not, and this information facilitated reading above and beyond the predictability of the full word form. The results are consistent with graded prediction but inconsistent with full lexical prediction. Implications for theories of prediction in language comprehension are discussed.},
  langid = {english},
  keywords = {Cloze probability,Eye movements,Prediction,read,Reading,Word recognition},
  file = {/home/vboyce/Zotero/storage/CUJRNZ8B/Luke and Christianson - 2016 - Limits on lexical prediction during reading.pdf;/home/vboyce/Zotero/storage/TBX372S9/S0010028516301384.html}
}

@article{macdonaldInteractionLexicalSyntactic1993,
  title = {The Interaction of Lexical and Syntactic Ambiguity},
  author = {MacDonald, Maryellen C.},
  year = {1993},
  journal = {Journal of Memory and Language},
  volume = {32},
  pages = {692--715},
  keywords = {read}
}

@article{peer2017,
  title = {Beyond the {{Turk}}: {{Alternative}} Platforms for Crowdsourcing Behavioral Research},
  shorttitle = {Beyond the {{Turk}}},
  author = {Peer, Eyal and Brandimarte, Laura and Samat, Sonam and Acquisti, Alessandro},
  year = {2017},
  month = may,
  journal = {Journal of Experimental Social Psychology},
  volume = {70},
  pages = {153--163},
  issn = {0022-1031},
  doi = {10.1016/j.jesp.2017.01.006},
  abstract = {The success of Amazon Mechanical Turk (MTurk) as an online research platform has come at a price: MTurk has suffered from slowing rates of population replenishment, and growing participant non-naivety. Recently, a number of alternative platforms have emerged, offering capabilities similar to MTurk but providing access to new and more na\"ive populations. After surveying several options, we empirically examined two such platforms, CrowdFlower (CF) and Prolific Academic (ProA). In two studies, we found that participants on both platforms were more na\"ive and less dishonest compared to MTurk participants. Across the three platforms, CF provided the best response rate, but CF participants failed more attention-check questions and did not reproduce known effects replicated on ProA and MTurk. Moreover, ProA participants produced data quality that was higher than CF's and comparable to MTurk's. ProA and CF participants were also much more diverse than participants from MTurk.},
  langid = {english},
  keywords = {Amazon Mechanical Turk,CrowdFlower,Crowdsourcing,Data quality,Online research,Prolific Academic},
  file = {/home/vboyce/Zotero/storage/BN56SU8G/Peer et al. - 2017 - Beyond the Turk Alternative platforms for crowdso.pdf;/home/vboyce/Zotero/storage/DTL7DLNH/S0022103116303201.html}
}

@article{radfordLanguageModelsAre,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  pages = {24},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/H4SDJ6ZG/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf}
}

@article{raynerEffectsFrequencyPredictability2004,
  ids = {rayner-etal:2004},
  title = {The {{Effects}} of {{Frequency}} and {{Predictability}} on {{Eye Fixations}} in {{Reading}}: {{Implications}} for the {{E-Z Reader Model}}.},
  shorttitle = {The {{Effects}} of {{Frequency}} and {{Predictability}} on {{Eye Fixations}} in {{Reading}}},
  author = {Rayner, Keith and Ashby, Jane and Pollatsek, Alexander and Reichle, Erik D.},
  year = {2004},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {30},
  number = {4},
  pages = {720--732},
  issn = {1939-1277, 0096-1523},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/MAE44FYT/Rayner et al. - 2004 - The Effects of Frequency and Predictability on Eye.pdf}
}

@article{raynerEyeMovementsReading1998,
  title = {Eye Movements in Reading and Information Processing: 20 Years of Research},
  author = {Rayner, Keith},
  year = {1998},
  journal = {Psychological Bulletin},
  volume = {124},
  number = {3},
  pages = {372--422},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/XDKGFWGQ/Rayner - 1998 - Eye movements in reading and information processin.pdf}
}

@inproceedings{shainDeconvolutionalTimeSeries2018,
  title = {Deconvolutional {{Time Series Regression}}: {{A Technique}} for {{Modeling Temporally Diffuse Effects}}},
  shorttitle = {Deconvolutional {{Time Series Regression}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Shain, Cory and Schuler, William},
  year = {2018},
  pages = {2679--2689},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1288},
  abstract = {Psycholinguists frequently use linear models to study time series data generated by human subjects. However, time series may violate the assumptions of these models through temporal diffusion, where stimulus presentation has a lingering influence on the response as the rest of the experiment unfolds. This paper proposes a new statistical model that borrows from digital signal processing by recasting the predictors and response as convolutionallyrelated signals, using recent advances in machine learning to fit latent impulse response functions (IRFs) of arbitrary shape. A synthetic experiment shows successful recovery of true latent IRFs, and psycholinguistic experiments reveal plausible, replicable, and finegrained estimates of latent temporal dynamics, with comparable or improved prediction quality to widely-used alternatives.},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/XPJJ78TD/Shain and Schuler - 2018 - Deconvolutional Time Series Regression A Techniqu.pdf}
}

@inproceedings{shainLargescaleStudyEffects2019,
  title = {A Large-Scale Study of the Effects of Word Frequency and Predictability in Naturalistic Reading},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Shain, Cory},
  year = {2019},
  month = jun,
  pages = {4086--4094},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  abstract = {A number of psycholinguistic studies have factorially manipulated words' contextual predictabilities and corpus frequencies and shown separable effects of each on measures of human sentence processing, a pattern which has been used to support distinct mechanisms underlying prediction on the one hand and lexical retrieval on the other. This paper examines the generalizability of this finding to more realistic conditions of sentence processing by studying effects of frequency and predictability in three large-scale naturalistic reading corpora. Results show significant effects of word frequency and predictability in isolation but no effect of frequency over and above predictability, and thus do not provide evidence of distinct mechanisms. The non-replication of separable effects in a naturalistic setting raises doubts about the existence of such a distinction in everyday sentence comprehension. Instead, these results are consistent with previous claims that apparent effects of frequency are underlyingly effects of predictability.},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/T98EH7ZL/Shain - 2019 - A large-scale study of the effects of word frequen.pdf}
}

@inproceedings{sloggettAmazeAnyOther2020,
  title = {A-Maze by Any Other Name},
  booktitle = {{{CUNY}}},
  author = {Sloggett, Shayne and Handel, Nicholas Van and Rysling, Amanda},
  year = {2020},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/6LT2ANWC/Sloggett et al. - A-maze by any other name.pdf}
}
@inproceedings{smith-levy:2008,
  year = {2008},
  title = {Optimal Processing Times in Reading: a Formal Model and Empirical Investigation},
  pages = {595â€“600},
  booktitle = {Proceedings of the 30th Annual Meeting of the Cognitive Science Society},
  author = {Smith, Nathaniel J. and Levy, Roger},
  address = {Washington, DC}
}

@article{smithEffectWordPredictability2013,
  ids = {smith-levy:2013},
  title = {The Effect of Word Predictability on Reading Time Is Logarithmic},
  author = {Smith, Nathaniel J. and Levy, Roger},
  year = {2013},
  month = sep,
  journal = {Cognition},
  volume = {128},
  number = {3},
  pages = {302--319},
  issn = {00100277},
  date-added = {2010-07-29 08:41:38 -0700},
  date-modified = {2019-02-14 16:00:55 -0500},
  langid = {english},
  rogerslocalurl = {papers/smith-levy-2013-cognition.pdf},
  topic = {Sentence processing,Statistical modeling,Data analysis,Eye movements in reading},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/W7F44KDE/Smith and Levy - 2013 - The effect of word predictability on reading time .pdf}
}

@article{staubEyeMovementsProcessing2010,
  title = {Eye Movements and Processing Difficulty in Object Relative Clauses},
  author = {Staub, Adrian},
  year = {2010},
  journal = {Cognition},
  volume = {116},
  pages = {71--86},
  keywords = {read}
}

@article{traxlerProcessingSubjectObject2002,
  title = {Processing Subject and Object Relative Clauses: {{Evidence}} from Eye Movements},
  author = {Traxler, Matthew J. and Morris, Robin K. and Seely, Rachel E.},
  year = {2002},
  journal = {Journal of Memory and Language},
  volume = {47},
  pages = {69--90},
  keywords = {read}
}

@article{vaniUsingInterpolatedMaze2021,
  title = {Using the {{Interpolated Maze Task}} to {{Assess Incremental Processing}} in {{English Relative Clauses}}},
  author = {Vani, Pranali and Wilcox, Ethan Gotlieb and Levy, Roger},
  year = {2021},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {43},
  number = {43},
  issn = {1069-7977},
  abstract = {In English, Subject Relative Clauses are processed more quickly than Object Relative Clauses, but open questions remain about where in the clause slowdown occurs. The surprisal theory of incremental processing, under which processing difficulty corresponds to probabilistic expectations about upcoming material, predicts that slowdown should occur immediately on material that disambiguates the subject from object relative clause. However, evidence from eye tracking and self-paced reading studies suggests that slowdown occurs downstream of RC-disambiguating material, on the relative clause verb. These methods, however, suffer from well-known spillover effects which makes their results difficult to interpret. To address these issues, we introduce and deploy a novel variant of the Maze task for reading times (Forster, Guerrera, \&amp; Elliot, 2009), called the Interpolated Maze in two English web-based experiments. In Experiment 1, we find that the locus of reading-time differences between SRCs and ORCs falls on immediate disambiguating definite determiner. Experiment 2 provides a control, showing that ORCs are read more slowly than lexically-matching, non-anomalous material. These results provide new evidence for the locus of processing difficulty in relative clauses and support the surprisal theory of incremental processing.},
  langid = {english},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/SN4J2YI6/Vani et al. - 2021 - Using the Interpolated Maze Task to Assess Increme.pdf;/home/vboyce/Zotero/storage/MR57FEX9/3x34x7dz.html}
}

@article{wilcoxPredictivePowerNeural2020,
  ids = {wilcoxPredictivePowerNeural},
  title = {On the {{Predictive Power}} of {{Neural Language Models}} for {{Human Real-Time Comprehension Behavior}}},
  author = {Wilcox, Ethan and Gauthier, Jon and Hu, Jennifer and Qian, Peng and Levy, Roger},
  year = {2020},
  month = jun,
  journal = {arXiv:2006.01912 [cs]},
  eprint = {2006.01912},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {Human reading behavior is tuned to the statistics of natural language: the time it takes human subjects to read a word can be predicted from estimates of the word's probability in context. However, it remains an open question what computational architecture best characterizes the expectations deployed in real time by humans that determine the behavioral signatures of reading. Here we test over two dozen models, independently manipulating computational architecture and training dataset size, on how well their next-word expectations predict human reading time behavior on naturalistic text corpora. We find that across model architectures and training dataset sizes the relationship between word log-probability and reading time is (near-)linear. We next evaluate how features of these models determine their psychometric predictive power, or ability to predict human reading behavior. In general, the better a model's next-word expectations, the better its psychometric predictive power. However, we find nontrivial differences across model architectures. For any given perplexity, deep Transformer models and n-gram models generally show superior psychometric predictive power over LSTM or structurally supervised neural models, especially for eye movement data. Finally, we compare models' psychometric predictive power to the depth of their syntactic knowledge, as measured by a battery of syntactic generalization tests developed using methods from controlled psycholinguistic experiments. Once perplexity is controlled for, we find no significant relationship between syntactic knowledge and predictive power. These results suggest that different approaches may be required to best model human real-time language comprehension behavior in naturalistic reading versus behavior for controlled linguistic materials designed for targeted probing of syntactic knowledge.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,read},
  file = {/home/vboyce/Zotero/storage/2MB3WMZM/Wilcox et al. - 2020 - On the Predictive Power of Neural Language Models .pdf;/home/vboyce/Zotero/storage/DBT82KM6/Wilcox et al. - On the Predictive Power of Neural Language Models .pdf;/home/vboyce/Zotero/storage/JMREYM9Z/2006.html}
}

@inproceedings{wilcoxTargetedAssessmentIncremental2021,
  title = {A {{Targeted Assessment}} of {{Incremental Processing}} in {{Neural Language Models}} and {{Humans}}},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Wilcox, Ethan and Vani, Pranali and Levy, Roger},
  year = {2021},
  month = aug,
  pages = {939--952},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.76},
  abstract = {We present a targeted, scaled-up comparison of incremental processing in humans and neural language models by collecting by-word reaction time data for sixteen different syntactic test suites across a range of structural phenomena. Human reaction time data comes from a novel online experimental paradigm called the Interpolated Maze task. We compare human reaction times to by-word probabilities for four contemporary language models, with different architectures and trained on a range of data set sizes. We find that across many phenomena, both humans and language models show increased processing difficulty in ungrammatical sentence regions with human and model `accuracy' scores a la Marvin and Linzen (2018) about equal. However, although language model outputs match humans in direction, we show that models systematically under-predict the difference in magnitude of incremental processing difficulty between grammatical and ungrammatical sentences. Specifically, when models encounter syntactic violations they fail to accurately predict the longer reading times observed in the human data. These results call into question whether contemporary language models are approaching human-like performance for sensitivity to syntactic violations.},
  keywords = {read},
  file = {/home/vboyce/Zotero/storage/EYZP5LWC/Wilcox et al. - 2021 - A Targeted Assessment of Incremental Processing in.pdf}
}

@article{witzelComparisonsOnlineReading2012a,
  ids = {witzelComparisonsOnlineReading2012},
  title = {Comparisons of Online Reading Paradigms: {{Eye}} Tracking, Moving-Window, and Maze},
  author = {Witzel, Naoko and Witzel, Jeffrey and Forster, Kenneth},
  year = {2012},
  journal = {Journal of Psycholinguistic Research},
  volume = {41},
  number = {2},
  pages = {105--128},
  publisher = {{Springer}},
  keywords = {read}
}


@article{marslen-wilson:1975,
	author = {William Marslen-Wilson},
	journal = {Science},
	number = {4198},
	pages = {226--228},
	title = {Sentence perception as an interactive parallel process},
	volume = {189},
	year = {1975}}

@article{tanenhaus-etal:1995,
	author = {Michael K. Tanenhaus and Michael J. Spivey-Knowlton and Kathleen Eberhard and Julie C. Sedivy},
	date-modified = {2009-04-18 16:34:40 -0700},
	journal = {Science},
	pages = {1632--1634},
	read = {No},
	title = {Integration of visual and linguistic information in spoken language comprehension},
	volume = {268},
	year = {1995},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxA3Li4vLi4vRHJvcGJveCAoUGVyc29uYWwpL3BhcGVycy90YW5lbmhhdXMtZXRhbC0xOTk1LnBkZk8RAYgAAAAAAYgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////xd0YW5lbmhhdXMtZXRhbC0xOTk1LnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAIAAwAACiBjdQAAAAAAAAAAAAAAAAAGcGFwZXJzAAIAPy86VXNlcnM6cmxldnk6RHJvcGJveCAoUGVyc29uYWwpOnBhcGVyczp0YW5lbmhhdXMtZXRhbC0xOTk1LnBkZgAADgAwABcAdABhAG4AZQBuAGgAYQB1AHMALQBlAHQAYQBsAC0AMQA5ADkANQAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAPVVzZXJzL3JsZXZ5L0Ryb3Bib3ggKFBlcnNvbmFsKS9wYXBlcnMvdGFuZW5oYXVzLWV0YWwtMTk5NS5wZGYAABMAAS8AABUAAgAM//8AAAAIAA0AGgAkAF4AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAB6g==}}

@article{levy:2008,
	author = {Roger Levy},
	date-modified = {2019-02-14 13:05:50 -0500},
	doi = {10.1016/j.cognition.2007.05.006},
	googlescholarurl = {http://scholar.google.com/citations?view_op=view_citation&hl=en&citation_for_view=Hfm7ruwAAAAJ:u5HHmVD_uO8C},
	journal = {Cognition},
	number = {3},
	pages = {1126--1177},
	rogerslocalurl = {papers/levy-2008-cognition.pdf},
	title = {Expectation-Based Syntactic Comprehension},
	topic = {Sentence processing,Syntax},
	volume = {106},
	year = {2008},
	bdsk-url-1 = {http://ling.ucsd.edu/~rlevy/papers/levy_cognition_may_2007.pdf}}

@article{kutas-hillyard:1980,
	author = {Marta Kutas and Steven A. Hillyard},
	journal = {Science},
	number = {4427},
	pages = {203--205},
	title = {Reading Senseless Sentences: Brain Potentials Reflect Semantic Incongruity},
	volume = {207},
	year = {1980}}

@article{osterhout-holcomb:1992jml,
	abstract = {Event-related brain potentials elicited by syntactic anomaly. L , PJ Journal of Memory and Language(Print) 31:66, 785-806, Elsevier, . },
	author = {L Osterhout and P Holcomb},
	date-added = {2009-03-04 13:45:01 -0800},
	date-modified = {2018-01-11 15:49:43 +0000},
	journal = jml,
	month = {Jan},
	number = {6},
	pages = {785--606},
	pmid = {13719581079620864629related:dcr4bU2-Zb4J},
	rating = {0},
	title = {Event-related brain potentials elicited by syntactic anomaly},
	uri = {papers://5D12F92B-A977-4736-9145-F8D0C5F47C46/Paper/p3293},
	url = {http://cat.inist.fr/?aModele=afficheN&cpsidt=4397093},
	volume = {31},
	year = {1992},
	bdsk-url-1 = {http://cat.inist.fr/?aModele=afficheN&cpsidt=4397093}}

@incollection{mitchell:2004online-methods,
	author = {Mitchell, Don C.},
	booktitle = {The on-line study of sentence comprehension: Eye-tracking, ERP and beyond},
	date-added = {2017-12-18 21:14:04 +0000},
	date-modified = {2017-12-18 21:15:20 +0000},
	editor = {Carreiras, Manuel, and {Clifton Jr.}, Charles},
	pages = {15--32},
	publisher = {London: Routledge},
	title = {On-line methods in language processing: Introduction and historical review},
	year = {2004}}

@article{orth2022processing,
  title={Processing profile for quantifiers in verb phrase ellipsis: Evidence for grammatical economy},
  author={Orth, Wesley and Yoshida, Masaya},
  journal={Proceedings of the Linguistic Society of America},
  volume={7},
  number={1},
  pages={5210},
  year={2022}
}

@article{demberg-keller:2008,
	author = {Vera Demberg and Frank Keller},
	journal = {Cognition},
	number = {2},
	pages = {193--210},
	title = {Data from Eye-tracking Corpora as Evidence for Theories of Syntactic Processing Complexity},
	volume = {109},
	year = {2008}}

@article{luke-christianson:2016-limits,
	author = {Luke, Steven G and Christianson, Kiel},
	date-added = {2020-11-11 18:28:50 -0500},
	date-modified = {2020-11-11 18:29:05 -0500},
	journal = cogpsych,
	pages = {22--60},
	publisher = {Elsevier},
	title = {Limits on lexical prediction during reading},
	volume = {88},
	year = {2016}}

@article{ungerer2021using,
  title={Using structural priming to test links between constructions: English caused-motion and resultative sentences inhibit each other},
  author={Ungerer, Tobias},
  journal={Cognitive Linguistics},
  volume={32},
  number={3},
  pages={389--420},
  year={2021},
  publisher={De Gruyter Mouton}
}

@article{chacon2021limits,
  title={Limits on semantic prediction in the processing of extraction from adjunct clauses},
  author={Dustin Alfonso Chac{\'o}n and Annika Kort and Peter O'Neill and Trey Sorensen},
  year={2021},
  publisher={PsyArXiv}
}

@incollection{mitchell:1984,
	author = {Don C. Mitchell},
	booktitle = {New methods in reading comprehension},
	date-added = {2009-10-11 15:41:36 -0700},
	date-modified = {2009-10-11 15:43:55 -0700},
	editor = {D. Kieras and M. A. Just},
	publisher = {Hillsdale, NJ: Earlbaum},
	title = {An Evaluation of Subject-Paced Reading Tasks and Other Methods for Investigating Immediate Processes in Reading},
	year = {1984}}

@book{wood:2017GAMs,
	author = {Wood, Simon},
	date-added = {2015-07-14 22:48:32 +0000},
	date-modified = {2015-07-15 12:28:18 +0000},
	publisher = {CRC press},
	title = {Generalized additive models: an introduction with {R}},
	edition = {2},
	year = {2017}}

@article{kliegl-etal:2006,
	author = {Reinhold Kliegl and Antje Nuthmann and Ralf Engbert},
	date-added = {2010-07-29 08:26:17 -0700},
	date-modified = {2010-07-29 08:27:08 -0700},
	journal = jepgen,
	number = {1},
	pages = {12--35},
	title = {Tracking the Mind During Reading: The Influence of Past, Present, and Future Words on Fixation Durations},
	volume = {135},
	year = {2006}}


@article{wilcox-etal:2022-using-computational-models,
	author = {Wilcox, Ethan and Futrell, Richard and Levy, Roger P.},
	date-added = {2022-02-13 12:31:16 -0500},
	date-modified = {2022-07-05 16:05:13 -0400},
	journal = {Linguistic Inquiry},
	rogerslocalurl = {https://ling.auf.net/lingbuzz/006327/current.pdf?_s=FC6XTg21is_inJL7},
	title = {Using Computational Models to Test Syntactic Learnability},
	year = {In press},
	bdsk-url-1 = {https://ling.auf.net/lingbuzz/006327/current.pdf?_s=FC6XTg21is_inJL7}}


@misc{levinson:2022-beyond-surprising,
	author = {Lisa Levinson},
	date-added = {2022-08-24 18:40:02 -0400},
	date-modified = {2022-08-24 18:40:54 -0400},
	howpublished = {Presentation at the 35th Annual Conference on Human Sentence Processing},
	title = {Beyond Surprising: {English} Event Structure in the Maze},
	year = {2022}}


@inproceedings{hale:2001,
	address = {Pittsburgh, Pennsylvania},
	author = {John Hale},
	booktitle = naacl2,
	date-modified = {2013-04-29 05:57:13 +0000},
	month = {2--7 June},
	pages = {159--166},
	title = {A probabilistic {Earley} parser as a Psycholinguistic Model},
	url = {http://acl.ldc.upenn.edu/N/N01/N01-1021.pdf},
	year = {2001},
	bdsk-url-1 = {http://acl.ldc.upenn.edu/N/N01/N01-1021.pdf}}


@misc{van-lieburg-etal:2022-using-the-maze-task,
	author = {Rianne van Lieburg and Robert J. Hartsuiker and Sarah Bernolet},
	date-added = {2022-08-24 18:42:54 -0400},
	date-modified = {2022-08-24 18:44:24 -0400},
	howpublished = {Presentation at the 35th Annual Conference on Human Sentence Processing},
	title = {Using the {Maze} Task paradigm to test structural priming in comprehension in {L1} and {L2} speakers of {English}},
	year = {2022}}


@article{linzen-etal:2016tacl,
	author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
	date-added = {2017-11-12 00:07:08 +0000},
	date-modified = {2019-03-05 17:31:56 -0500},
	journal = tacl,
	title = {Assessing the ability of {LSTMs} to learn syntax-sensitive dependencies},
	year = {2016}}


@inproceedings{marvin-linzen:2018-targeted,
	abstract = {We present a data set for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM{'}s accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.},
	address = {Brussels, Belgium},
	author = {Marvin, Rebecca and Linzen, Tal},
	booktitle = emnlp2018,
	date-added = {2019-05-31 17:57:21 -0400},
	date-modified = {2019-05-31 20:27:18 -0400},
	month = oct # {-} # nov,
	pages = {1192--1202},
	publisher = {Association for Computational Linguistics},
	title = {Targeted Syntactic Evaluation of Language Models},
	url = {https://www.aclweb.org/anthology/D18-1151},
	year = {2018},
	bdsk-url-1 = {https://www.aclweb.org/anthology/D18-1151}}


@article{warstadt-etal:2020-BLiMP,
	abstract = {We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs{---}that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4{\%}. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.},
	author = {Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R.},
	date-added = {2020-11-03 17:40:30 -0500},
	date-modified = {2020-11-03 17:40:35 -0500},
	doi = {10.1162/tacl_a_00321},
	journal = tacl,
	pages = {377--392},
	title = {{BL}i{MP}: The Benchmark of Linguistic Minimal Pairs for {E}nglish},
	url = {https://www.aclweb.org/anthology/2020.tacl-1.25},
	volume = {8},
	year = {2020},
	bdsk-url-1 = {https://www.aclweb.org/anthology/2020.tacl-1.25},
	bdsk-url-2 = {https://doi.org/10.1162/tacl_a_00321}}


@inproceedings{futrell-etal:2019-neural-language-models,
	arxivurl = {https://arxiv.org/abs/1903.03260},
	author = {Richard Futrell and Ethan Wilcox and Takashi Morita and Peng Qian and Miguel Ballesteros and Roger Levy},
	booktitle = naacl2019,
	date-added = {2019-02-22 13:47:24 -0500},
	date-modified = {2020-07-23 15:03:47 -0400},
	pages = {32--42},
	rogerslocalurl = {https://www.aclweb.org/anthology/N19-1004.pdf},
	title = {Neural language models as psycholinguistic subjects: Representations of syntactic state},
	topic = {Machine learning, Sentence processing},
	url = {https://www.aclweb.org/anthology/N19-1004},
	year = {2019},
	bdsk-url-1 = {https://www.aclweb.org/anthology/N19-1004}}


@article{levy-fedorenko-gibson:2013jml,
	author = {Roger Levy and Evelina Fedorenko and Edward Gibson},
	date-added = {2011-09-08 19:00:35 +0000},
	date-modified = {2019-02-14 13:03:15 -0500},
	doi = {http://dx.doi.org/10.1016/j.jml.2012.10.005},
	journal = jml,
	number = {4},
	pages = {461--495},
	rogerslocalurl = {papers/levy-fedorenko-gibson-2013-jml-clean.pdf},
	title = {The syntactic complexity of {Russian} relative clauses},
	topic = {Sentence processing,Syntax},
	volume = {69},
	year = {2013},
	bdsk-url-1 = {http://dx.doi.org/10.1016/j.jml.2012.10.005}}


@article{lewis-etal:2006,
	author = {Richard L. Lewis and Shravan Vasishth and Julie {Van Dyke}},
	date-modified = {2013-01-20 20:41:59 +0000},
	journal = tics,
	number = {10},
	pages = {447--454},
	title = {Computational principles of working memory in sentence comprehension},
	url = {http://ling.ucsd.edu/~rlevy/lign274/papers/lewis-etal-2006.pdf},
	volume = {10},
	year = {2006},
	bdsk-url-1 = {http://ling.ucsd.edu/~rlevy/lign274/papers/lewis-etal-2006.pdf}}



@article{levy-etal:2009pnas,
	author = {Roger Levy and Klinton Bicknell and Tim Slattery and Keith Rayner},
	date-added = {2009-12-07 16:04:49 -0800},
	date-modified = {2019-02-14 13:02:55 -0500},
	doi = {http://dx.doi.org/10.1073/pnas.0907664106},
	journal = pnas,
	number = {50},
	pages = {21086--21090},
	rogerslocalurl = {papers/levy-etal-2009-pnas.pdf},
	title = {Eye Movement Evidence that Readers Maintain and Act on Uncertainty about Past Linguistic Input},
	topic = {Sentence processing,Eye movements in reading},
	volume = {106},
	year = {2009},
	bdsk-file-1 = {YnBsaXN0MDDSAQIDBFxyZWxhdGl2ZVBhdGhZYWxpYXNEYXRhXxA3Li4vLi4vRHJvcGJveCAoUGVyc29uYWwpL3BhcGVycy9sZXZ5LWV0YWwtMjAwOS1wbmFzLnBkZk8RAYgAAAAAAYgAAgAADE1hY2ludG9zaCBIRAAAAAAAAAAAAAAAAAAAAAAAAABCRAAB/////xdsZXZ5LWV0YWwtMjAwOS1wbmFzLnBkZgAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAD/////AAAAAAAAAAAAAAAAAAIAAwAACiBjdQAAAAAAAAAAAAAAAAAGcGFwZXJzAAIAPy86VXNlcnM6cmxldnk6RHJvcGJveCAoUGVyc29uYWwpOnBhcGVyczpsZXZ5LWV0YWwtMjAwOS1wbmFzLnBkZgAADgAwABcAbABlAHYAeQAtAGUAdABhAGwALQAyADAAMAA5AC0AcABuAGEAcwAuAHAAZABmAA8AGgAMAE0AYQBjAGkAbgB0AG8AcwBoACAASABEABIAPVVzZXJzL3JsZXZ5L0Ryb3Bib3ggKFBlcnNvbmFsKS9wYXBlcnMvbGV2eS1ldGFsLTIwMDktcG5hcy5wZGYAABMAAS8AABUAAgAM//8AAAAIAA0AGgAkAF4AAAAAAAACAQAAAAAAAAAFAAAAAAAAAAAAAAAAAAAB6g==}}


@article{van-schijndel-linzen-2021:single-stage-models,
	author = {Van Schijndel, Marten and Linzen, Tal},
	date-added = {2021-09-11 06:16:07 -0400},
	date-modified = {2021-09-11 06:16:18 -0400},
	journal = {Cognitive Science},
	number = {6},
	pages = {e12988},
	publisher = {Wiley Online Library},
	title = {Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty},
	volume = {45},
	year = {2021}}

@incollection{levy:2013sentenceProcessing,
	author = {Roger Levy},
	booktitle = {Sentence Processing},
	date-added = {2013-04-20 21:37:56 +0000},
	date-modified = {2019-02-14 13:31:07 -0500},
	editor = {Roger P. G. {van Gompel}},
	pages = {78--114},
	publisher = {Hove: Psychology Press},
	rogerslocalurl = {papers/levy-2013-memory-and-surprisal-corrected.pdf},
	title = {Memory and Surprisal in Human Sentence Comprehension},
	topic = {Sentence processing},
	year = {2013}}

@inproceedings{mccoy-etal-2019-right,
    title = "Right for the Wrong Reasons: Diagnosing Syntactic Heuristics in Natural Language Inference",
    author = "McCoy, Tom  and
      Pavlick, Ellie  and
      Linzen, Tal",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P19-1334",
    doi = "10.18653/v1/P19-1334",
    pages = "3428--3448",
    abstract = "A machine learning system can score well on a given test set by relying on heuristics that are effective for frequent example types but break down in more challenging cases. We study this issue within natural language inference (NLI), the task of determining whether one sentence entails another. We hypothesize that statistical NLI models may adopt three fallible syntactic heuristics: the lexical overlap heuristic, the subsequence heuristic, and the constituent heuristic. To determine whether models have adopted these heuristics, we introduce a controlled evaluation set called HANS (Heuristic Analysis for NLI Systems), which contains many examples where the heuristics fail. We find that models trained on MNLI, including BERT, a state-of-the-art model, perform very poorly on HANS, suggesting that they have indeed adopted these heuristics. We conclude that there is substantial room for improvement in NLI systems, and that the HANS dataset can motivate and measure progress in this area.",
}

@inproceedings{chaves-2020-dont,
    title = "What Don{'}t {RNN} Language Models Learn About Filler-Gap Dependencies?",
    author = "Chaves, Rui",
    booktitle = "Proceedings of the Society for Computation in Linguistics 2020",
    month = jan,
    year = "2020",
    address = "New York, New York",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.scil-1.1",
    pages = "1--11",
}

