
@article{barrRandomEffectsStructure2013,
  ids = {barr-etal:2013jml},
  title = {Random Effects Structure for Confirmatory Hypothesis Testing: {{Keep}} It Maximal},
  shorttitle = {Random Effects Structure for Confirmatory Hypothesis Testing},
  author = {Barr, Dale J. and Levy, Roger and Scheepers, Christoph and Tily, Harry J.},
  year = {2013},
  month = apr,
  volume = {68},
  pages = {255--278},
  issn = {0749-596X},
  abstract = {Linear mixed-effects models (LMEMs) have become increasingly prominent in psycholinguistics and related areas. However, many researchers do not seem to appreciate how random effects structures affect the generalizability of an analysis. Here, we argue that researchers using LMEMs for confirmatory hypothesis testing should minimally adhere to the standards that have been in place for many decades. Through theoretical arguments and Monte Carlo simulation, we show that LMEMs generalize best when they include the maximal random effects structure justified by the design. The generalization performance of LMEMs including data-driven random effects structures strongly depends upon modeling criteria and sample size, yielding reasonable results on moderately-sized samples when conservative criteria are used, but with little or no power advantage over maximal models. Finally, random-intercepts-only LMEMs used on within-subjects and/or within-items data from populations where subjects and/or items vary in their sensitivity to experimental manipulations always generalize worse than separate F1 and F2 tests, and in many cases, even worse than F1 alone. Maximal LMEMs should be the `gold standard' for confirmatory hypothesis testing in psycholinguistics and beyond.},
  date-added = {2011-09-08 22:51:09 +0000},
  date-modified = {2019-02-14 12:57:58 -0500},
  file = {/home/vboyce/Zotero/storage/DMUW47IM/Barr et al. - 2013 - Random effects structure for confirmatory hypothes.pdf;/home/vboyce/Zotero/storage/8YUFVVYF/S0749596X12001180.html},
  journal = {Journal of Memory and Language},
  keywords = {Generalization,Linear mixed-effects models,Monte Carlo simulation,read,Statistics},
  language = {en},
  number = {3},
  rogerslocalurl = {papers/barr-etal-2013-jml.pdf},
  topic = {Statistical modeling}
}

@article{bartekSearchOnlineLocality2011,
  title = {In Search of On-Line Locality Effects in Sentence Comprehension.},
  author = {Bartek, Brian and Lewis, Richard L and Vasishth, Shravan and Smith, Mason R},
  year = {2011},
  volume = {37},
  pages = {1178},
  publisher = {{American Psychological Association}},
  date-added = {2019-06-17 10:18:24 -0400},
  date-modified = {2019-06-17 10:18:53 -0400},
  journal = {Journal of Experimental Psychology: Human Perception \& Performance},
  keywords = {read},
  number = {5}
}

@article{batesFittingLinearMixedeffects2015,
  title = {Fitting Linear Mixed-Effects Models Using Lme4},
  author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
  year = {2015},
  volume = {67},
  pages = {1--48},
  file = {/home/vboyce/Zotero/storage/H9PL9IKR/Bates et al. - 2015 - Fitting linear mixed-effects models using span cl.pdf},
  journal = {Journal of Statistical Software},
  keywords = {read},
  number = {1}
}

@article{boyceMazeMadeEasy2020,
  ids = {boyceMazeMadeEasy2020a},
  title = {Maze {{Made Easy}}: {{Better}} and Easier Measurement of Incremental Processing Difficulty},
  shorttitle = {Maze {{Made Easy}}},
  author = {Boyce, Veronica and Futrell, Richard and Levy, Roger P.},
  year = {2020},
  month = apr,
  volume = {111},
  pages = {104082},
  issn = {0749-596X},
  abstract = {Behavioral measures of incremental language comprehension difficulty form a crucial part of the empirical basis of psycholinguistics. The two most common methods for obtaining these measures have significant limitations: eye tracking studies are resource-intensive, and self-paced reading can yield noisy data with poor localization. These limitations are even more severe for web-based crowdsourcing studies, where eye tracking is infeasible and self-paced reading is vulnerable to inattentive participants. Here we make a case for broader adoption of the Maze task, involving sequential forced choice between each successive word in a sentence and a contextually inappropriate distractor. We leverage natural language processing technology to automate the most researcher-laborious part of Maze \textendash{} generating distractor materials \textendash{} and show that the resulting A(uto)-Maze method has dramatically superior statistical power and localization for well-established syntactic ambiguity resolution phenomena. We make our code freely available online for widespread adoption of A-maze by the psycholinguistics community.},
  file = {/home/vboyce/Zotero/storage/5HFVHZZI/Boyce et al. - 2020 - Maze Made Easy Better and easier measurement of i.pdf;/home/vboyce/Zotero/storage/ZA3FXTF2/S0749596X19301147.html},
  journal = {Journal of Memory and Language},
  keywords = {A-maze,G-maze,Maze task,read,Self-paced reading,Sentence processing},
  language = {en}
}

@article{burknerAdvancedBayesianMultilevel2018,
  title = {Advanced Bayesian Multilevel Modeling with the r Package Brms},
  author = {B{\"u}rkner, Paul-Christian},
  year = {2018},
  volume = {10},
  pages = {395--411},
  journal = {The R Journal},
  keywords = {read},
  number = {1}
}

@article{daiTransformerXLAttentiveLanguage2019,
  title = {Transformer-{{XL}}: {{Attentive Language Models Beyond}} a {{Fixed}}-{{Length Context}}},
  shorttitle = {Transformer-{{XL}}},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc V. and Salakhutdinov, Ruslan},
  year = {2019},
  month = jun,
  abstract = {Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80\% longer than RNNs and 450\% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.},
  archiveprefix = {arXiv},
  eprint = {1901.02860},
  eprinttype = {arxiv},
  file = {/home/vboyce/Zotero/storage/SFQYHUHA/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf;/home/vboyce/Zotero/storage/U5VBLMJA/Dai et al. - 2019 - Transformer-XL Attentive Language Models Beyond a.pdf;/home/vboyce/Zotero/storage/DPDIFG6Q/1901.html;/home/vboyce/Zotero/storage/G2XVZRLZ/1901.html},
  journal = {arXiv:1901.02860 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,read,Statistics - Machine Learning},
  primaryclass = {cs, stat}
}

@article{forsterMazeTaskMeasuring2009,
  ids = {forster09},
  title = {The Maze Task: {{Measuring}} Forced Incremental Sentence Processing Time},
  shorttitle = {The Maze Task},
  author = {Forster, Kenneth I. and Guerrera, Christine and Elliot, Lisa},
  year = {2009},
  month = feb,
  volume = {41},
  pages = {163--171},
  publisher = {{Springer}},
  issn = {1554-3528},
  abstract = {The maze task is an online measure of sentence processing time that provides an alternative to the standard moving window version of self-paced reading. Rather than each word of the sentence being presented in succession, two words are presented at the same time, and the participant must choose which word is a grammatical continuation of the sentence. This procedure forces the reader into an incremental mode of processing in which each word must be fully integrated with the preceding context before the next word can be considered. Previous research with this technique has not considered whether it is sufficiently sensitive to syntactic complexity effects or to garden path effects. Four experiments are reported demonstrating that reliable differences in processing time for subject relatives and object relatives can be obtained, and that this technique generates garden path effects that correspond closely with the data from eyetracking experiments, but without the spillover effects that are sometimes obtained with eyetracking. It is also shown that the task is sensitive to word frequency effects, producing estimates well in excess of those found with eyetracking.},
  file = {/home/vboyce/Zotero/storage/YCADBDXD/Forster et al. - 2009 - The maze task Measuring forced incremental senten.pdf},
  journal = {Behavior Research Methods},
  keywords = {read},
  language = {en},
  number = {1}
}

@article{frazierMakingCorrectingErrors1982,
  title = {Making and Correcting Errors during Sentence Comprehension: {{Eye}} Movements in the Analysis of Structurally Ambiguous Sentences},
  author = {Frazier, Lyn and Rayner, Keith},
  year = {1982},
  volume = {14},
  pages = {178--210},
  journal = {Cognitive Psychology},
  keywords = {read}
}

@article{freedman85,
  title = {The Psychological Status of Overgenerated Sentences},
  author = {Freedman, Sandra E and Forster, Kenneth I},
  year = {1985},
  volume = {19},
  pages = {101--131},
  publisher = {{Elsevier}},
  journal = {Cognition},
  keywords = {read},
  number = {2}
}

@article{futrellNaturalStoriesCorpus2020,
  title = {The {{Natural Stories}} Corpus: A Reading-Time Corpus of {{English}} Texts Containing Rare Syntactic Constructions},
  shorttitle = {The {{Natural Stories}} Corpus},
  author = {Futrell, Richard and Gibson, Edward and Tily, Harry J. and Blank, Idan and Vishnevetsky, Anastasia and Piantadosi, Steven T. and Fedorenko, Evelina},
  year = {2020},
  month = sep,
  issn = {1574-0218},
  abstract = {It is now a common practice to compare models of human language processing by comparing how well they predict behavioral and neural measures of processing difficulty, such as reading times, on corpora of rich naturalistic linguistic materials. However, many of these corpora, which are based on naturally-occurring text, do not contain many of the low-frequency syntactic constructions that are often required to distinguish between processing theories. Here we describe a new corpus consisting of English texts edited to contain many low-frequency syntactic constructions while still sounding fluent to native speakers. The corpus is annotated with hand-corrected Penn Treebank-style parse trees and includes self-paced reading time data and aligned audio recordings. We give an overview of the content of the corpus, review recent work using the corpus, and release the data.},
  file = {/home/vboyce/Zotero/storage/X393ZSV8/Futrell et al. - 2020 - The Natural Stories corpus a reading-time corpus .pdf},
  journal = {Lang Resources \& Evaluation},
  keywords = {read},
  language = {en}
}

@inproceedings{gauthierSyntaxGymOnlinePlatform2020,
  title = {{{SyntaxGym}}: {{An Online Platform}} for {{Targeted Evaluation}} of {{Language Models}}},
  shorttitle = {{{SyntaxGym}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{System Demonstrations}}},
  author = {Gauthier, Jon and Hu, Jennifer and Wilcox, Ethan and Qian, Peng and Levy, Roger},
  year = {2020},
  month = jul,
  pages = {70--76},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-demos.10},
  abstract = {Targeted syntactic evaluations have yielded insights into the generalizations learned by neural network language models. However, this line of research requires an uncommon confluence of skills: both the theoretical knowledge needed to design controlled psycholinguistic experiments, and the technical proficiency needed to train and deploy large-scale language models. We present SyntaxGym, an online platform designed to make targeted evaluations accessible to both experts in NLP and linguistics, reproducible across computing environments, and standardized following the norms of psycholinguistic experimental design. This paper releases two tools of independent value for the computational linguistics community: 1. A website, syntaxgym.org, which centralizes the process of targeted syntactic evaluation and provides easy tools for analysis and visualization; 2. Two command-line tools, `syntaxgym` and `lm-zoo`, which allow any user to reproduce targeted syntactic evaluations and general language model inference on their own machine.},
  file = {/home/vboyce/Zotero/storage/6U3F2K4H/Gauthier et al. - 2020 - SyntaxGym An Online Platform for Targeted Evaluat.pdf},
  keywords = {read}
}

@article{grodnerConsequencesSerialNature2005,
  title = {Some Consequences of the Serial Nature of Linguistic Input},
  author = {Grodner, Daniel and Gibson, Edward},
  year = {2005},
  volume = {29},
  pages = {261--290},
  journal = {Cognitive Science},
  keywords = {read},
  number = {2}
}

@inproceedings{gulordava18,
  title = {Colorless Green Recurrent Networks Dream Hierarchically},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
  year = {2018},
  pages = {1195--1205},
  keywords = {read}
}

@article{klieglLengthFrequencyPredictability2004,
  title = {Length, Frequency, and Predictability Effects of Words on Eye Movements in Reading},
  author = {Kliegl, Reinhold and Grabner, Ellen and Rolfs, Martin and Engbert, Ralf},
  year = {2004},
  month = jan,
  volume = {16},
  pages = {262--284},
  issn = {0954-1446, 1464-0635},
  file = {/home/vboyce/Zotero/storage/Q3DVUMLV/Kliegl et al. - 2004 - Length, frequency, and predictability effects of w.pdf},
  journal = {European Journal of Cognitive Psychology},
  keywords = {read},
  language = {en},
  number = {1-2}
}

@article{koornneefUseVerbbasedImplicit2006,
  title = {On the Use of Verb-Based Implicit Causality in Sentence Comprehension : {{Evidence}} from Self-Paced Reading and Eye Tracking},
  author = {Koornneef, Arnout W. and {van Berkum}, Jos J.A.},
  year = {2006},
  volume = {54},
  pages = {445--465},
  journal = {Journal of Memory and Language},
  keywords = {read},
  number = {4}
}

@article{levyEyeMovementEvidence2009,
  title = {Eye Movement Evidence That Readers Maintain and Act on Uncertainty about Past Linguistic Input},
  author = {Levy, Roger and Bicknell, Klinton and Slattery, Tim and Rayner, Keith},
  year = {2009},
  month = dec,
  volume = {106},
  pages = {21086--21090},
  issn = {0027-8424, 1091-6490},
  file = {/home/vboyce/Zotero/storage/GNR7VKCC/Levy et al. - 2009 - Eye movement evidence that readers maintain and ac.pdf},
  journal = {PNAS},
  keywords = {read},
  language = {en},
  number = {50}
}

@article{levyIntegratingSurprisalUncertaininput2011,
  title = {Integrating Surprisal and Uncertain-Input Models in Online Sentence Comprehension: Formal Techniques and Empirical Results},
  author = {Levy, Roger},
  year = {2011},
  pages = {11},
  abstract = {A system making optimal use of available information in incremental language comprehension might be expected to use linguistic knowledge together with current input to revise beliefs about previous input. Under some circumstances, such an error-correction capability might induce comprehenders to adopt grammatical analyses that are inconsistent with the true input. Here we present a formal model of how such input-unfaithful garden paths may be adopted and the difficulty incurred by their subsequent disconfirmation, combining a rational noisy-channel model of syntactic comprehension under uncertain input with the surprisal theory of incremental processing difficulty. We also present a behavioral experiment confirming the key empirical predictions of the theory.},
  file = {/home/vboyce/Zotero/storage/JZJUJ3HB/Levy - Integrating surprisal and uncertain-input models i.pdf},
  keywords = {read},
  language = {en}
}

@article{macdonaldInteractionLexicalSyntactic1993,
  title = {The Interaction of Lexical and Syntactic Ambiguity},
  author = {MacDonald, Maryellen C.},
  year = {1993},
  volume = {32},
  pages = {692--715},
  journal = {Journal of Memory and Language},
  keywords = {read}
}

@article{radfordLanguageModelsAre,
  title = {Language {{Models}} Are {{Unsupervised Multitask Learners}}},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  pages = {24},
  abstract = {Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset - matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations.},
  file = {/home/vboyce/Zotero/storage/H4SDJ6ZG/Radford et al. - Language Models are Unsupervised Multitask Learner.pdf},
  keywords = {read},
  language = {en}
}

@article{raynerEffectsFrequencyPredictability2004,
  ids = {rayner-etal:2004},
  title = {The {{Effects}} of {{Frequency}} and {{Predictability}} on {{Eye Fixations}} in {{Reading}}: {{Implications}} for the {{E}}-{{Z Reader Model}}.},
  shorttitle = {The {{Effects}} of {{Frequency}} and {{Predictability}} on {{Eye Fixations}} in {{Reading}}},
  author = {Rayner, Keith and Ashby, Jane and Pollatsek, Alexander and Reichle, Erik D.},
  year = {2004},
  volume = {30},
  pages = {720--732},
  issn = {1939-1277, 0096-1523},
  file = {/home/vboyce/Zotero/storage/MAE44FYT/Rayner et al. - 2004 - The Effects of Frequency and Predictability on Eye.pdf},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  keywords = {read},
  language = {en},
  number = {4}
}

@article{raynerEyeMovementsReading1998,
  title = {Eye Movements in Reading and Information Processing: 20 Years of Research},
  author = {Rayner, Keith},
  year = {1998},
  volume = {124},
  pages = {372--422},
  journal = {Psychological Bulletin},
  keywords = {read},
  number = {3}
}

@inproceedings{shainLargescaleStudyEffects2019,
  title = {A Large-Scale Study of the Effects of Word Frequency and Predictability in Naturalistic Reading},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Shain, Cory},
  year = {2019},
  month = jun,
  pages = {4086--4094},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  abstract = {A number of psycholinguistic studies have factorially manipulated words' contextual predictabilities and corpus frequencies and shown separable effects of each on measures of human sentence processing, a pattern which has been used to support distinct mechanisms underlying prediction on the one hand and lexical retrieval on the other. This paper examines the generalizability of this finding to more realistic conditions of sentence processing by studying effects of frequency and predictability in three large-scale naturalistic reading corpora. Results show significant effects of word frequency and predictability in isolation but no effect of frequency over and above predictability, and thus do not provide evidence of distinct mechanisms. The non-replication of separable effects in a naturalistic setting raises doubts about the existence of such a distinction in everyday sentence comprehension. Instead, these results are consistent with previous claims that apparent effects of frequency are underlyingly effects of predictability.},
  file = {/home/vboyce/Zotero/storage/T98EH7ZL/Shain - 2019 - A large-scale study of the effects of word frequen.pdf},
  keywords = {read}
}

@inproceedings{sloggettAmazeAnyOther2020,
  title = {A-Maze by Any Other Name},
  booktitle = {{{CUNY}}},
  author = {Sloggett, Shayne and Handel, Nicholas Van and Rysling, Amanda},
  year = {2020},
  file = {/home/vboyce/Zotero/storage/6LT2ANWC/Sloggett et al. - A-maze by any other name.pdf},
  keywords = {read},
  language = {en}
}

@article{smithEffectWordPredictability2013,
  ids = {smith-levy:2013},
  title = {The Effect of Word Predictability on Reading Time Is Logarithmic},
  author = {Smith, Nathaniel J. and Levy, Roger},
  year = {2013},
  month = sep,
  volume = {128},
  pages = {302--319},
  issn = {00100277},
  date-added = {2010-07-29 08:41:38 -0700},
  date-modified = {2019-02-14 16:00:55 -0500},
  file = {/home/vboyce/Zotero/storage/W7F44KDE/Smith and Levy - 2013 - The effect of word predictability on reading time .pdf},
  journal = {Cognition},
  keywords = {read},
  language = {en},
  number = {3},
  rogerslocalurl = {papers/smith-levy-2013-cognition.pdf},
  topic = {Sentence processing,Statistical modeling,Data analysis,Eye movements in reading}
}

@article{staubEyeMovementsProcessing2010,
  title = {Eye Movements and Processing Difficulty in Object Relative Clauses},
  author = {Staub, Adrian},
  year = {2010},
  volume = {116},
  pages = {71--86},
  journal = {Cognition},
  keywords = {read}
}

@article{traxlerProcessingSubjectObject2002,
  title = {Processing Subject and Object Relative Clauses: {{Evidence}} from Eye Movements},
  author = {Traxler, Matthew J. and Morris, Robin K. and Seely, Rachel E.},
  year = {2002},
  volume = {47},
  pages = {69--90},
  journal = {Journal of Memory and Language},
  keywords = {read}
}

@article{wilcoxPredictivePowerNeural2020,
  ids = {wilcoxPredictivePowerNeural},
  title = {On the {{Predictive Power}} of {{Neural Language Models}} for {{Human Real}}-{{Time Comprehension Behavior}}},
  author = {Wilcox, Ethan Gotlieb and Gauthier, Jon and Hu, Jennifer and Qian, Peng and Levy, Roger},
  year = {2020},
  month = jun,
  abstract = {Human reading behavior is tuned to the statistics of natural language: the time it takes human subjects to read a word can be predicted from estimates of the word's probability in context. However, it remains an open question what computational architecture best characterizes the expectations deployed in real time by humans that determine the behavioral signatures of reading. Here we test over two dozen models, independently manipulating computational architecture and training dataset size, on how well their next-word expectations predict human reading time behavior on naturalistic text corpora. We find that across model architectures and training dataset sizes the relationship between word log-probability and reading time is (near-)linear. We next evaluate how features of these models determine their psychometric predictive power, or ability to predict human reading behavior. In general, the better a model's next-word expectations, the better its psychometric predictive power. However, we find nontrivial differences across model architectures. For any given perplexity, deep Transformer models and n-gram models generally show superior psychometric predictive power over LSTM or structurally supervised neural models, especially for eye movement data. Finally, we compare models' psychometric predictive power to the depth of their syntactic knowledge, as measured by a battery of syntactic generalization tests developed using methods from controlled psycholinguistic experiments. Once perplexity is controlled for, we find no significant relationship between syntactic knowledge and predictive power. These results suggest that different approaches may be required to best model human real-time language comprehension behavior in naturalistic reading versus behavior for controlled linguistic materials designed for targeted probing of syntactic knowledge.},
  archiveprefix = {arXiv},
  eprint = {2006.01912},
  eprinttype = {arxiv},
  file = {/home/vboyce/Zotero/storage/2MB3WMZM/Wilcox et al. - 2020 - On the Predictive Power of Neural Language Models .pdf;/home/vboyce/Zotero/storage/DBT82KM6/Wilcox et al. - On the Predictive Power of Neural Language Models .pdf;/home/vboyce/Zotero/storage/JMREYM9Z/2006.html},
  journal = {arXiv:2006.01912 [cs]},
  keywords = {Computer Science - Computation and Language,read},
  primaryclass = {cs}
}

@article{witzelComparisonsOnlineReading2012a,
  ids = {witzelComparisonsOnlineReading2012},
  title = {Comparisons of Online Reading Paradigms: {{Eye}} Tracking, Moving-Window, and Maze},
  author = {Witzel, Naoko and Witzel, Jeffrey and Forster, Kenneth},
  year = {2012},
  volume = {41},
  pages = {105--128},
  publisher = {{Springer}},
  journal = {Journal of Psycholinguistic Research},
  keywords = {read},
  number = {2}
}


