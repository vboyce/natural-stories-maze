---
title             : "A-maze of Natural Stories: Texts are comprehensible during the Maze task"
shorttitle        : "A-maze of Natural Stories"

author: 
  - name          : "Veronica Boyce"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "TODO"
    email         : "vboyce@stanford.edu"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Formal Analysis
      - Investigation
      - Software
      - Writing - Original Draft Preparation
      - Writing - Review & Editing
  - name          : "Roger Levy"
    affiliation   : "2"
    role:
      - Conceptualization
      - Formal Analysis
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Stanford University"
  - id            : "2"
    institution   : "Massachusetts Institute of Technology"
header-includes:
 - \usepackage{setspace}\singlespacing

authornote: |
  TODO
  
abstract: |
  TODO Easy to use, reliable methods are important to science. In studying how people understand language in real time, we use incremental processing methods to measure word-by-word reading time. However, neither of the common methods (eye-tracking and self-paced reading) can run over the web and produce localized effects. Another method, called the Maze task, seems to be able to do this, especially since a technique called A-maze makes the task much faster to construct. Due to task limitations, Maze has not been used on long, naturalistic passages, only short targeted materials. Here we present an adaptation of the Maze method suitable for long materials; we test this method on the Natural Stories corpus and find that participants can comprehend what they read while doing the task and that the reading time patterns are broadly similar to those found with other methods. We find support for the localization of reading time effects during the Maze task, as well as extending the range of materials Maze is suitable for. 
  
keywords          : "TODO"
wordcount         : "X"

bibliography      : ["r-references.bib","refs.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
library("citr")
library(patchwork)
library(here)
library(tidyverse)
library(readr)
library(brms)
library(lme4)
library(rstan)
library(tidybayes)
library(knitr)
library(matrixStats)
library(tidyverse)
library(readr)
library(brms)
library(lme4)
library(rstan)
library(tidybayes)
library(knitr)
library(mgcv)
library(mgcViz)
library(tidymv)
library(rsample)
library(cowplot)
library(scales)
r_refs("r-references.bib")
theme_set(theme_bw())
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
options(knitr.table.format = "pdf")
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```
# Intro
[TODO: hook about toolbox of methods]
A diversity of robust methods provides for converging evidence from different methods (either within or between types), and choices allow for a good fit to the scope and goals of research. But we also want these methods to be reliable. Some of that is theoretical, but we also want empirical evidence that what the method captures is relevant to what we want to study, and that this connection is true across as broad a range of materials as we hope to use it for. Robustness is important to establishing how good a new method is. What range of tasks can this new tool accomplish and how well? (Could really extend on a toolbox analogy.)

[incrementality; so need incremental methods] 
One question in psycholinguistics is how real-time comprehension works; as we read or listen, we process language incrementally, building and changing our mental model of a sentence. How long we spend on each word is due to both qualities of the word itself and how it fits with the sentence context so far TODO citations

We process language incrementally, building a model of the sentence as we hear or read word.  When a word is unexpected and does not fit the model of the sentence so far, our understanding of the sentence may need to be revised. Incremental processing methods measure this process, tracking how long each word takes to read as a proxy for how hard it is to process and incoroporate into the mental conception of meaning. Fluent language uses process language rapidly and flexibly adapt to many unexpected words. This makes it harder to capture reading time effects because they may be obscured by the overall robust reading process. Different techniques handle this localization problem differently, and present different trade-offs. The two most commonly used methods are eye-tracking and self-paced reading, but neither of these techniques are ideal at localization. One measure of localization is whether current or past word reading time is more predictive. SPR and eye-tarcking both don't seem great. 

[eye-track and SPR, failure to localize] TODO: possibly shorten? In eye-tracking, participants read text freely from a screen, while the eye movements and fixations are tracked with an infrared camera CITE, check for truth. Eye-tracking is naturalistic and yields fine-grained data about how long participants spent on each word when, but it's expensive to run requiring both specialized equipment and researcher time to run experiments. Self-paced reading (commonly abbreviated SPR) in contrast can be run online with no specialzed equipment or direct researcher supervision, and so it's possible to collect data quickly and cheaply. In SPR, participants see one word of the text on screen at a time and press a button to contine to the next word instead; the time between button presses when a word was on screen is used as the reading time. The problem with SPR, especially when run online, is that effects from one word tend to show up as longer reading times spread out over several later words, in what is known as spillover effects. Both of these methods suffer somewhat from analytic flexibilty as processing difficulty may manifest in several ways in eye-tracking and is hard to localize in SPR.

[Maze]An alternative that seems to have superior localization is the Maze task, which adopts an unnatural way of reading to force incremental processing. In the Maze task, participants see two words at a time, one of which continues the sentence, and one of which does not, and they have to choose which word is the correct continuation. The time to choose the correct word is taken as the dependent measure and interpreted as representing the difficulty of integrating the word into the sentence so far. If participants make a mistake, the sentence discontinues. Theoretically, participants must fully integrate each word into the sentence in order to confidently select it and move on. This is suppported by studies finding localized effects without much spillover. 

[A-Maze] However, traditional Maze materials are effort-instensive to construct, because of the need to select infelicitious words as distractors for each spot of each sentence; this may have kept this method from widespread adoption. 

CITE US propose a solution to this problem of generating Maze distractors. In A-maze, distractors are automatically generated by feeding the correct sentences through a program that uses NLP language models to select high-surprisal distractors. The quality of the automatically generated distractors is not up to that of hand-generated distractors, in that the distractors ocassionally are good fits in context. Nonetheless, Boyce et al found that materials with A-maze distractors had similar results to G-maze distractors.  TODO: talk about the paradigm more
[Summary of 2020 paper]

[Summary other uses of A-maze - Slogett]
Slogett also found that it works similarly. 

While new and untested, A-maze seems like a potentially powerful addition to the toolkit. However, like the other Maze tasks it has been limited in its application to single-sentence items probing minimal comparisons in constructed sentences. It's only been used on minimal pair types of sentences where each sentence is independent. In particular, it hasn't been used on longer, naturalistic texts, so we don't know how in compares to other methods here. And hasn't been used on multi-sentence issue of generating Maze materials has been proposed. An alternative version of the Maze task, dubbed A-maze, was recently introduced that gets around this problem by automatically generating distrators using a program and neural nets (Boyce et al, 2020). While the quality of the automatically generated distractors is not quite up that of hand-generated (as judged by their being occassional distractors that seem completely acceptable continuations), the results from A-maze match those of G-maze. Boyce et al provide an interface for running the Maze task in a browser and a way of generating distractors that are usually, although not always, sufficiently bad. 

In a comparison between these A-maze materials and traditional hand-crafted distractors (from Witzel 2012), they find that A-maze yields comparable results to G-maze (either lab or web), and in one condition recovers effects that G-maze did not. A-maze also outperforms self-paced reading on this syntactic disambiguation paradigm. Sloggett (cite) also directly compared A-maze and G-maze on a disambiguaton paradigm and similarly found that A-maze and G-maze yielded similar results. 

These demonstrations make A-maze seem quite promising, as it has the advantages of the traditional Maze task, without it's drawback. This method seems worth exploring further. 

[why do longer Maze] While some psycholinguistic phenomena are well-explored within individual targeted sentences, there's also a need for reading time data of more naturalistic passages. This is important as a comparison for models of language understanding, for correlating with NLP language models. In addition, some phenomena such as discourse effects, only work in the context of passages. THus, it would be useful/versatile if Maze could be used in these other contexts. 

[short-coming, where's the question we need to answer] One evident shortcoming of the Maze task is that it has been used on only a limited set of materials. Some of this is due to the fact that it isn't used as widely as other incremental processing methods, and some is due to inherent limitations of the paradigm. In particular, because Maze tasks discontinue after participants make mistakes, the farther into an item a word is, the fewer participants see it. This makes it hard to run long materials using Maze, and prevents Maze from being used on long text passages where SPR or eye-tracking could be used. 

[Our contribution is...] We find a way around this problem by creating a version of the Maze task interface where participants correct their mistakes and continue with the sentence rather than being discontinued. This allows for multi-sentence items to be run using Maze, which we verify by running Maze on the Natural Stories corpus. With this data, we are able to confirm that participants can comprehend what they read using Maze and investigate the effects of surprisal on RT in Maze. 


# Error-correction maze

[review the terminate thing, and why it was there] The advantage of the Maze task is that it forces incremental processing and automatically excludes inattentive participants by terminating a sentence when a participant makes an error. Thus, only participants who have processed the sentence up that point and are paying attention contribute data at critical regions later in the sentence. However, this means we don't have data after a participant makes a mistake in an item. In traditional G-maze tasks, with hand-crafted distractors and attentive participants, this is a small issue as the low per-word error rate does not compound too badly over the individual sentences.

[ how this causes problems for 2020 and theoretically] 
However, this data loss was worse with A-maze materials and crowd-sourced participants [@boyceMazeMadeEasy2020]. The high errors could be from participants guessing randomly or from auto-generated distractors that in fact fit the sentence. @boyceMazeMadeEasy2020 noted that some distractors, especially early in the sentence were problematic and caused considerable data loss.

Given the appeals of using A-maze, it would be adventageous to get around this problem. Additionally, even with attentive participants and good distractors, the Maze paradigm will eventually run into problems as error rates compound over long materials, and few participants will make it to the end. For instance, with a 1% error rate, `r round(.99**15*100)`% would complete each 15-word sentence, but only `r round(.99**50*100)`% of participants would complete each 50 word vignette, and `r round(.99**200*100)`% each 200 word passages.  In order to run longer materials, we need something to do when participants make a mistake, other than terminate the entire item.

To resolve this, we introduce an *error-correction* variant of Maze, where we let participants continue on when they make a mistake. To do this, we present them with an error message, and wait until they select the correct option, before continuing the sentence as normal. We make this "error-correction" Maze available as an option in a modification of the Ibex Maze implementation introduced in CITE ME (TODO repo link). The code records both the RT to the first click and also the total RT until the correct answer is selected as separate values. TODO diagram goes here

[ why this error thing seems really cool and could work] 
Our goal is that this error-correction will minimize the problems of having occassional poor auto-generated distractors by letting participants continue on and still read the whole item. It will also enable us to run long materials on Maze, thus expanding the range of questions that Maze can be used on. 


# Natural Stories corpus

We test this error-correction Maze on the Natural Stories corpus. The Natural Stories corpus [@futrellNaturalStoriesCorpus2020] consists of 10 passages each roughly 1000 words long which are designed to read fluently to native speakers. At the same time, the passages contain copious punctuation (including quoted speech), many proper nouns, and low frequency grammatical constructions. Taken together, these properties make this a severe test of our process. The materials are on the difficult side for our participants and for the language model we use to generate distractors. If participants can succeed at the Maze task on this set of materials, we think they are likely to succeed on basically any naturalistic text. 

The corpus also comes with binary-choice comprehension questions, 6 per story, which we use to assess comprehension. Using this corpus on the A-maze task allows us to address, first whether participants will read and understand long passages using A-maze with error correction, and second, whether the resulting RTs profiles will show expected patterns such as a linear relationship with surprisal. 

# Methods

```{r participants}

data <- read_rds(paste0(here(),"/Data/cleaned.rds"))

data_filt <- data %>% 
  filter(native %in% c("ENG", "English", "ENGLISH", "english")) #I peeked at what people put that semantically maps to english

data_stories <- data_filt %>% 
  select(type, subject) %>% 
  unique() %>% 
  group_by(type) %>% 
  tally() %>% 
  filter(type!="practice")

data_error_summ <- data_filt %>% 
  mutate(correct.num=ifelse(correct=="yes", 1,0)) %>% 
  group_by(subject, num_correct) %>%
  filter(type!="practice") %>% 
  filter(rt<5000) %>% 
  summarize(mean_rt=mean(rt),
            pct_correct=mean(correct.num)) %>% 
  mutate(good_comp=ifelse(num_correct>4,"5 or 6 correct","4 or fewer correct")) %>% 
    mutate(accurate=ifelse(pct_correct>.8,">80% correct", "<80% correct"),
           is.attentive=ifelse(pct_correct>.8,1,0))

some <- data_filt %>% select(subject, num_correct) %>% 
  unique() %>% 
  left_join(data_error_summ, by=c("subject", "num_correct")) %>% 
  mutate(accurate=ifelse(pct_correct>.8,">80% correct", "<80% correct")) %>% 
  group_by(num_correct,accurate)

data_good <- data_filt %>% 
  left_join(data_error_summ, by=c("subject", "num_correct")) %>% 
  filter(type!="practice") %>% 
  filter(pct_correct>.8) %>% 
  mutate(is.correct=ifelse(correct=="yes",1,0))

data_non_error <- data_good %>% 
  filter(is.correct==1) %>% 
  filter(word_num>0)

data_before <- data_good %>% 
  mutate(word_num_mistake=ifelse(correct=="no", word_num,NA)) %>% 
  group_by(sentence, subject) %>% 
  fill(word_num_mistake) %>% ungroup() %>% 
  mutate(after_mistake=word_num-word_num_mistake,
         after_mistake=ifelse(is.na(after_mistake),0,after_mistake)) %>% 
  filter(after_mistake==0) %>% 
  filter(is.correct==1) %>% 
  filter(word_num>0)

data_sentence <- data_before <- data_good %>% 
  mutate(word_num_mistake=ifelse(correct=="no", word_num,NA)) %>% 
  group_by(sentence, subject) %>% 
  fill(word_num_mistake) %>% ungroup() %>% 
  mutate(after_mistake=word_num-word_num_mistake,
         after_mistake=ifelse(is.na(after_mistake),0,after_mistake)) %>% 
  group_by(sentence, subject) %>% 
  summarize(total_mistake=sum(after_mistake)) %>% 
  mutate(sent.correct=ifelse(total_mistake==0,1,0))
```

We constructed A-maze distractors for the Natural stories corpus (CITE) and recruited 100 MTurkers to each read a story in the Maze paradigm. 

## Materials (good)
We took the texts of the Natural Stories corpus (CITE) and split it into sentences because the A-Maze process works on the sentence level. (This involved some judgment calls due to quoted speech). We used the original comprehension questions provided in the Natural Stories corpus. To familiarize participants with the task, we wrote a short practice passage and corresponding comprehension questions.
We then ran the sentences of the items through the A-maze generation process. We updated the code of the generation process to fix some issues identified in that paper; the code used is available at REPO. The underlying method did not change, but features such as appropriately handling more types of punctuation (needed for the Natural Stories corpus) were added. We ran the materials using FOOBAR paramters. We used the auto-generated distractors as they were, without checking them for quality. Materials are available at REPO.
TODO other notable changes to the codebase??

## Participants
We recruited 100 participants from Amazon Mechanical Turk, and paid each participant 3.50 dollars. We estimated that reading a 1000 word story would take 17 minutes at 1 word/second (based on previous work), so we expected the task to take around 20 minutes total. In fact, it appears to have taken most attentive participants around 20 minutes to read the story, and thus closer to 25 minutes for the overall study. We excluded data from those who did not report English as their native language, leaving `r data_filt %>% select(subject) %>% unique() %>% nrow()` participants. 

## Procedure
Participants first gave their informed consent and saw task instructions. Then they read a short practice story in the Maze paradigm and answered 2 binary-choice practice comprehension questions, before reading the main story in the A-maze task. After the story, they answered the 6 main comprehension questions, commented on their experience, answered optional demographic questions, saw an debriefing and were given a code to enter into Mturk for payment. The experiment was implemented in Ibex (CITE) and the experimental code is availabe at REPO.  

## Data analysis
Analyses were done in R (CITE), using the following CURATE packages. We used `r cite_r("r-references.bib")` for all our analyses.

# Results

## Reading stories in the Maze task (good)

(ref:error-cap) A. Correlation between a participant's accuracy on the Maze task (fraction of words selected correctly) and their average reaction time (in ms). Many participants (marked in green) chose the correct word >80% of the time; others (in red) appear to be randomly guessing and were excluded from further analysis. B. Performance on comprehension questions. Participants who had >80% task accuracy tended to do well on comprehension questions; those who were at chance on the task were also at chance on comprehension questions. 

```{r errors, out.height="25%", fig.width=6, fig.height=2, fig.pos='t', fig.cap="(ref:error-cap)"}
error_plot <- ggplot(data_error_summ, aes(x=pct_correct, y=mean_rt, color=accurate))+
  geom_point(size=1)+
  labs(x="Fraction words selected correctly",
       y="Mean Reaction Time (ms)")+
  coord_cartesian(xlim=c(0.4,1), ylim=c(0,1600), expand=F)+
    scale_color_manual(values=c(">80% correct"="darkgreen","<80% correct"="darkred"))+
  guides(color=FALSE)

comp_plot <- ggplot(some, aes(x=num_correct,fill=accurate))+
  geom_bar(position="dodge")+
  facet_grid(.~accurate)+
  labs(x="Comprehension questions correct (out of 6)", y="Participants")+
  scale_fill_manual(values=c(">80% correct"="darkgreen","<80% correct"="darkred"))+
guides(fill=FALSE)

error_plot+comp_plot+plot_annotation(tag_levels="A")

```

Our first concern was whether this error-correction paradigm on long passages would work. It did; some participants were able to complete the Maze task with high accuracy and understand the story while doing so. 

Participant accuracy reflects how well participants can navigate the task and whether the auto-generated distractors are of acceptable quality. We calculated the per-word error rate for each participant and graphed it against their average reaction time. (To avoid biasing the average if a participant took a pause before returning to the task, RTs greater than 5 seconds were excluded.) As seen in Figure \@ref(fig:errors)A, there is a cluster of participants who make relatively few errors (marked in green), with some reaching 99% accuracy. This indicates that the distractors were generally appropriate and that some participants can maintain focus on the task for the whole story. These careful participants tend to take around 1 second for each word selection, which is much slower than other paradigms CITE SPR something. We also have a cluster of participants (in red) who we infer sped through the task clicking randomly. This distribution is likely due to the mix of workers on Mturk; as we did not use qualification cutoffs. 

Another check is whether participants comprehended the story. We counted how many of the binary-choice comprehension questions each participant got right (out of 6), and group them by their performance on the Maze task. As seen in @ref(fig:errors)B, most participants were were accurate on the task also did well on comprehension questions, while participants who were at chance on the Maze task were also at chance on the comprehension questions. Participants usually answered quickly (within 10 seconds), so we do not believe they were looking up the answers on the internet. These are not especially hard questions, and we can't rule out that some participants may have been able to guess the answers without reading the story. Nonetheless, this provides preliminary evidence that people can understand and remember details of stories they read during the Maze task. 

Given that Maze task accuracy and comprehension question accuracy are correlated, but task accuracy is finer-grained, we use task performance as our exclusion metric. This easy to calculate speed and accuracy comparision makes it easy to see who was paying attention to the task; we exclude the random guessers and only analyse data from participants with at least 80% accuracy (in the gap between high-performers and low-performers). 


 
## RT and surprisal

```{r}
subject_att <- data_error_summ %>% 
  select(subject, is.attentive)

data_low_error <- data_filt %>% 
  left_join(subject_att, by="subject") %>% 
  filter(is.attentive==1) %>% 
  filter(type!="practice")

  data_error_free <- data_low_error %>% 
    mutate(word_num_mistake=ifelse(correct=="no", word_num,NA)) %>% 
    group_by(sentence, subject) %>% fill(word_num_mistake) %>% ungroup() %>% 
    mutate(after_mistake=word_num-word_num_mistake,
           after_mistake=ifelse(is.na(after_mistake),0,after_mistake)) %>% 
    filter(correct=="yes") %>% 
    filter(!after_mistake %in% c(1,2))
  
data_no_first <- data_error_free %>% filter(word_num!=0)

data_ready <- data_no_first %>% filter(rt>100 & rt<5000) %>% 
  select(subject, word_num, word, rt, sentence, type)

data_post_only <- data_no_first %>% filter(rt>100 & rt<5000) %>% 
  filter(!after_mistake==0) %>% 
    select(subject, word_num, word, rt, sentence, type)

data_pre_error <- data_no_first %>% filter(rt>100 & rt<5000) %>% 
  filter(after_mistake==0) %>% 
  select(subject, word_num, word, rt, sentence, type)

data_stories <- data_ready %>% select(type, subject) %>% 
  unique() %>% 
  group_by(type) %>% 
  tally()

data_anything_goes <- data_filt %>% 
  filter(type!="practice") %>% 
  filter(correct=="yes") %>% 
    select(subject, word_num, word, rt, sentence, type)

```

The relationship between a word's predictability and it's reading time is of interest. CITE It's well-established that for eye-tracking and SPR, RTs are roughly linear in term of a words surprisal (negative log probability). We wanted to check if this linear relationship also holds for Maze. Two other common word-features that affect RT are word length and a word's overall frequency. CITE

To assess this, we create a set of predictors of frequency, word length, and surprisals from several models. For length, we use the length of characters (excluding end punctuation). For a measure of unigram frequency, we tokenize the CITE gulordava training data (this tokenizes off punctuation, but preserves case) and tally up instances. For the models, we use the log2 frequency of the expected occurances in 1 billion words (thus frequency is log, but higher values indicate higher frequency words). For surprisals, we get per-word surprisals for each of 3 different language models: a Kneser-Ney smoothed 5-gram, GRNN CITE, and Transformer-XL CITE. We center, but do not rescale these predictors. For all of these predictors, we consider both the predictor at the current word as well as lagged predictors from the previous word. CITE some SPR stuff This is an additional test of localization. The model formula we use is rt ~ surp* length +  freq * length + past_surp * past_length + past_freq * past_length.

We only include words that were in the vocabularies of all three models as a single token and for which we have frequency information. This has the effect of excluding words that had punctuation as well as some uncommon or proper nouns. We also exclude the first word of every sentence (which had a dummy distractor). We do not analyse RTs for words where the RT was <100 or >5000 ms (<100 is likely a recording error, >5000 is likely the participant getting distracted). We also omit words where the participant made a mistake and the two words after a mistake. 

### GAM
For GAM models, we center length and frequency but not surprisal. We want surprisal interpretable, but we also will be plotting it (at least for the bootstrapping) at length and frequencies set to 0, so they need to be centered. (Not sure this last piece is actually true/matters). 





### GAMs

## Try 1
Using only pre-error data.

Does not have hierarchical effects. Has smooths for the surprisals, and tensor effects/interactions for the freq x length

```{r}
#mean by subject for each word
labelled_pre_error <- read_rds(paste0(here(),"/Analysis/pre_error.rds"))
d_lm <- labelled_pre_error %>% group_by(word, txl_center, ngram_center, grnn_center, freq_center, length_center,
                                        past_c_txl, past_c_ngram, past_c_grnn, past_c_freq, past_c_length, Word_ID) %>% 
  summarize(mean_rt=mean(rt))
```

```{r for gam}

ngram_data <- labelled_pre_error %>% select(rt, surprisal=ngram_surp, prev_surp=past_ngram, 
                                            freq=freq_center, len=length_center,
                                            prev_freq=past_c_freq, prev_len=past_c_length
                                            ) %>% mutate(model="5-gram")

grnn_data <- labelled_pre_error %>% select(rt, surprisal=grnn_surp, prev_surp=past_grnn, 
                                            freq=freq_center, len=length_center,
                                            prev_freq=past_c_freq, prev_len=past_c_length
                                            )%>% mutate(model="GRNN")

txl_data <- labelled_pre_error %>% select(rt, surprisal=txl_surp, prev_surp=past_txl, 
                                            freq=freq_center, len=length_center,
                                            prev_freq=past_c_freq, prev_len=past_c_length
                                            ) %>% mutate(model="TXL")

all_data <- ngram_data %>% union(grnn_data) %>% union(txl_data) %>% 
  select(surprisal, prev_surp,model) %>% 
  pivot_longer(cols=`surprisal`:`prev_surp`) %>% 
  mutate(s=ifelse(name=="surprisal", "Current","Previous"))
```

```{r gams, eval=F}
gam_ngram_1 <- gam(rt ~ s(surprisal, bs="cr", k=20)+ te(freq, len, bs="cr")+s(prev_surp, bs="cr", k=20)+te(prev_freq, prev_len, bs="cr"), data=ngram_data, method="REML")

gam_grnn_1 <- gam(rt ~ s(surprisal, bs="cr", k=20)+ te(freq, len, bs="cr")+s(prev_surp, bs="cr", k=20)+te(prev_freq, prev_len, bs="cr"), data=grnn_data, method="REML")

gam_txl_1 <- gam(rt ~ s(surprisal, bs="cr", k=20)+ te(freq, len, bs="cr")+s(prev_surp, bs="cr", k=20)+te(prev_freq, prev_len, bs="cr"), data=txl_data, method="REML")

```

```{r plot-gam, eval=F}


a <- get_gam_predictions(model=gam_ngram_1, series=surprisal, series_length=100) %>% select(surprisal, rt, CI_upper, CI_lower) %>% unique() %>% mutate(model="5-gram", s="Current")
b <- get_gam_predictions(model=gam_ngram_1, series=prev_surp, series_length=100) %>% select(surprisal=prev_surp, rt, CI_upper, CI_lower) %>% unique() %>% mutate(model="5-gram", s="Previous")
c <- get_gam_predictions(model=gam_grnn_1, series=surprisal, series_length=100) %>% select(surprisal, rt, CI_upper, CI_lower) %>% unique() %>% mutate(model="GRNN", s="Current")
d <- get_gam_predictions(model=gam_grnn_1, series=prev_surp, series_length=100) %>% select(surprisal=prev_surp, rt, CI_upper, CI_lower) %>% unique() %>% mutate(model="GRNN", s="Previous")
e <- get_gam_predictions(model=gam_txl_1, series=surprisal, series_length=100) %>% select(surprisal, rt, CI_upper, CI_lower) %>% unique() %>% mutate(model="TXL", s="Current")
f <- get_gam_predictions(model=gam_txl_1, series=prev_surp, series_length=100) %>% select(surprisal=prev_surp, rt, CI_upper, CI_lower) %>% unique() %>% mutate(model="TXL", s="Previous")
```

```{r, eval=F}
all <- a %>% union(b) %>% union(c) %>% union(d) %>% union(e) %>% union(f)

gam1 <-  ggplot(all, aes(x=surprisal, y=rt, ymin=CI_lower, ymax=CI_upper))+
  geom_line()+
  geom_ribbon(alpha=.3)+
   #geom_density(data=all_data, aes(x=value), fill="gray",)+
  facet_grid(s~model)+
  coord_cartesian(ylim=c(700,1300), xlim=c(0,28))+
  labs(x="Surprisal (bits)", y="Reaction Time (ms)")+theme(axis.ticks.x=element_blank(), axis.title.x=element_blank(), axis.text.x=element_blank(), plot.margin=margin(t=0,r=0,b=0,l=0,unit="pt"))


dens2 <-   ggplot(all_data, aes(x=value))+
  geom_density(fill="gray",)+
  facet_grid(.~model)+
  labs(x="Surprisal (bits)", y="")+
  theme(axis.text.y = element_blank(), strip.text=element_blank(), axis.ticks.y =element_blank(), axis.title.y=element_blank(), panel.grid=element_blank(), plot.margin = unit(c(0, 0, 0, 0), "cm"))

bot <- plot_grid(NA, dens2, NA, nrow=1, rel_widths = c(.16, 1, .08))

 plot_grid(gam1, bot, nrow=2, rel_heights = c(1, .3))


#confirm that this really is just a prettied up version of what the usual output is
#plot(gam_txl_1, seWithMean = TRUE, shift = coef(gam_txl_1)[1], select=1)

```



### Frequentist LMs



## Models

We want to be able to do nested model comparision, so we want models with 
- only length, frequency fx
- each 1 surprisal model as predictor
- all the surprisals

For now, no interactions between surprisal and others, and if we include a predictor, include both current and lagged of it. 

```{r}

no_surp <- lmer(mean_rt ~ freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

ngram_only <- lmer(mean_rt ~ ngram_center + past_c_ngram + freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

grnn_only <- lmer(mean_rt ~ grnn_center + past_c_grnn + freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

txl_only <- lmer(mean_rt ~ txl_center + past_c_txl + freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

all_surp <- lmer(mean_rt ~ ngram_center + past_c_ngram + 
                   grnn_center + past_c_grnn +
                   txl_center + past_c_txl +
                   freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)


ngram_grnn <-  lmer(mean_rt ~ ngram_center + past_c_ngram + 
                   grnn_center + past_c_grnn +
                   freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

ngram_txl <-  lmer(mean_rt ~ ngram_center + past_c_ngram + 
                   txl_center + past_c_txl +
                   freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

grnn_txl <-  lmer(mean_rt ~ grnn_center + past_c_grnn +
                   txl_center + past_c_txl +
                   freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

no_lag <-  lmer(mean_rt ~ ngram_center +
                   grnn_center +
                   txl_center + 
                   freq_center*length_center+(1|word), data=d_lm)

no_surp_lag <-  lmer(mean_rt ~ ngram_center +  
                   grnn_center + 
                   txl_center +                    freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

  only_surp_lag <-  lmer(mean_rt ~ ngram_center + past_c_ngram + 
                   grnn_center + past_c_grnn +
                   txl_center + past_c_txl +
                   freq_center*length_center+(1|word), data=d_lm)
```

```{r, include=F}

anova(all_surp, txl_only)
anova(all_surp, grnn_only)
anova(all_surp, ngram_only)

anova(txl_only, no_surp)
anova(grnn_only, no_surp)
anova(ngram_only, no_surp)

anova(all_surp, ngram_grnn)
anova(all_surp, ngram_txl)
anova(all_surp, grnn_txl)

anova(ngram_grnn, ngram_only)
anova(ngram_grnn, grnn_only)
anova(ngram_txl, ngram_only)
anova(ngram_txl, txl_only)
anova(grnn_txl, grnn_only)
anova(grnn_txl, txl_only)

anova(all_surp, no_lag)

anova(all_surp, no_surp_lag)

anova(no_surp_lag, no_lag) 

anova(all_surp, only_surp_lag)

anova(only_surp_lag, no_lag)
```
In general, adding to the model makes it better. 

The model with all 3 surprisal predictors is better than any model with only one. Any one surprisal predictor is better than no surprisals predictors. 

However, adding ngram predictors to a model that already has txl & grnn does not help. 
In other cases, adding the 3rd surprisal source to the other two does help. 

Ngram+Grnn is not better than Grnn only. Otherwise, pairs are better than singletons. This suggests that Ngram's info is a subset of GRNN, but not a subset of TXL. 

Past surprisal predictors don't help (with or without past freq,length effects in the models), but past freq,length do (with or without past surprisal predictors). 


## What if we don't exclude participants

One potentially useful comparison with past SPR/eye-tracking is if we are more inclusive of participants, do we get data that looks more like theirs. (Perhaps our exclusion criteria are more robust?)

This is all data from participants who speak English that wasn't a mistake. (So, includes bad participants and post-mistake data.)

```{r freq_all}
#mean by subject for each word
labelled_anything_goes <- read_rds(paste0(here(),"/Analysis/non-selective.rds"))
d_lm2 <- labelled_anything_goes %>% group_by(word, txl_center, ngram_center, grnn_center, freq_center, length_center,
                                        past_c_txl, past_c_ngram, past_c_grnn, past_c_freq, past_c_length, Word_ID) %>% 
  summarize(mean_rt=mean(rt))
```

```{r}

no_surp <- lmer(mean_rt ~ freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm2)

ngram_only <- lmer(mean_rt ~ ngram_center + past_c_ngram + freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm2)

grnn_only <- lmer(mean_rt ~ grnn_center + past_c_grnn + freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm2)

txl_only <- lmer(mean_rt ~ txl_center + past_c_txl + freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm2)

#summary(no_surp)
#summary(ngram_only)
#summary(grnn_only)
#summary(txl_only)

```

BRM models from this same very permissive data. 
```{r brm-models}

any_grnn <- read_rds(paste0(here(),"/Analysis/brm_grnn_any_center.rds"))
any_txl <- read_rds(paste0(here(),"/Analysis/brm_txl_any_center.rds"))
any_ngram <- read_rds(paste0(here(),"/Analysis/brm_ngram_any_center.rds"))

#summary(any_grnn)
#summary(any_txl)
#summary(any_ngram)


```

```{r}

show_summary <- function(model){
  intervals <- gather_draws(model, `b_.*`, regex=T) %>% mean_qi()
  
  stats <- gather_draws(model, `b_.*`, regex=T) %>% 
    mutate(above_0=ifelse(.value>0, 1,0)) %>% 
    group_by(.variable) %>% 
    summarize(pct_above_0=mean(above_0)) %>% 
    mutate(`P` = signif(2*pmin(pct_above_0,1-pct_above_0), digits=2)) %>% 
    left_join(intervals, by=".variable") %>% 
    mutate(lower=round(.lower, digits=1),
           upper=round(.upper, digits=1),
           E=round(.value, digits=1),
           `CI`=str_c("[",lower,", ", upper,"]"),
           Term=str_sub(.variable, 3, -1),
           ) %>% 
    select(Term, `E`, `CI`,`P`)
  
  stats
}
a <- show_summary(any_txl) %>% mutate(model="TXL") %>% mutate(Term=str_replace(Term,"txl","surp"))
b <- show_summary(any_grnn) %>% mutate(model="GRNN") %>% mutate(Term=str_replace(Term,"grnn","surp"))
c <- show_summary(any_ngram) %>% mutate(model="5-gram") %>% mutate(Term=str_replace(Term,"ngram","surp"))

summ <- a %>% union(b) %>% union(c) %>% pivot_wider(names_from="model", values_from=c(`E`,`CI`,`P`)) %>% 
  select(Term, `E_5-gram`, `CI_5-gram`, `P_5-gram`, `E_GRNN`, CI_GRNN, P_GRNN, E_TXL, CI_TXL, P_TXL)


```
## BRM models

We include a by-subject effect for everything, and a by_word random intercept (full mixed effects). 

Priors:

- normal(1000,1000) for intercept -- we think RTs are about 1 second usually
- normal(0,500) for beta and sd -- we don't really know what effects are
- lkj(1) for correlations -- we don't have reason to think correlations might go any particular way 

```{r}
  priors <- c(
      set_prior("normal(1000, 1000)", class="Intercept"),
      set_prior("normal(0, 500)", class="b"),
      set_prior("normal(0, 500)", class="sd"),
      set_prior("lkj(1)",       class="cor"))
  

```

### On pre-error data only 


```{r}

show_summary <- function(model){
  intervals <- gather_draws(model, `b_.*`, regex=T) %>% mean_qi()
  
  stats <- gather_draws(model, `b_.*`, regex=T) %>% 
    mutate(above_0=ifelse(.value>0, 1,0)) %>% 
    group_by(.variable) %>% 
    summarize(pct_above_0=mean(above_0)) %>% 
    mutate(`P` = signif(2*pmin(pct_above_0,1-pct_above_0), digits=2)) %>% 
    left_join(intervals, by=".variable") %>% 
    mutate(lower=round(.lower, digits=1),
           upper=round(.upper, digits=1),
           E=round(.value, digits=1),
           `CI`=str_c("[",lower,", ", upper,"]"),
           Term=str_sub(.variable, 3, -1),
           ) %>% 
    select(Term, `E`, `CI`,`P`)
  
  stats
}
brm_txl_interact <- read_rds(paste0(here(),"/Analysis/brm_txl_interact.rds"))
brm_grnn_interact <- read_rds(paste0(here(),"/Analysis/brm_grnn_interact.rds"))
brm_ngram_interact <- read_rds(paste0(here(),"/Analysis/brm_ngram_interact.rds"))
a <- show_summary(brm_txl_interact) %>% mutate(model="TXL") %>% mutate(Term=str_replace(Term,"txl","surp"))
b <- show_summary(brm_grnn_interact) %>% mutate(model="GRNN") %>% mutate(Term=str_replace(Term,"grnn","surp"))
c <- show_summary(brm_ngram_interact) %>% mutate(model="5-gram") %>% mutate(Term=str_replace(Term,"ngram","surp"))

summ <- a %>% union(b) %>% union(c) %>% pivot_wider(names_from="model", values_from=c(`E`,`CI`,`P`)) %>% 
  select(Term, `E_5-gram`, `CI_5-gram`, `P_5-gram`, `E_GRNN`, CI_GRNN, P_GRNN, E_TXL, CI_TXL, P_TXL)

```

### On post-error as well


```{r}
show_summary_2 <- function(model){
  intervals <- gather_draws(model, `b_.*`, regex=T) %>% mean_qi()
  
  stats <- gather_draws(model, `b_.*`, regex=T) %>% 
    mutate(above_0=ifelse(.value>0, 1,0)) %>% 
    group_by(.variable) %>% 
    summarize(pct_above_0=mean(above_0)) %>% 
    mutate(`P-value equivalent` = signif(2*pmin(pct_above_0,1-pct_above_0), digits=2)) %>% 
    left_join(intervals, by=".variable") %>% 
    mutate(lower=round(.lower, digits=2),
           upper=round(.upper, digits=2),
           `Credible Interval`=str_c("[",lower,", ", upper,"]"),
           Term=str_sub(.variable, 3, -1),
           Estimate=round(.value, digits=2)) %>% 
    select(Term, Estimate, `Credible Interval`, `P-value equivalent`)
  
  stats
}
```


## Comparision with SPR

We want to compare how good our participants were on the comprehension questions to how good people were on the SPR experiment reported in Futrell et al (2017). They had each story read about 100 times across 181 participants (participants generally read 5 stories). This means they have about 10 times as much data as we do. However, reading times in a story were excluded based on the comprehension of that story, so it makes sense to compare per-story accuracies with our per-story accuracies.

## Error patterns
When participants do make errors, they correct them quickly, generally within one second, which means it's probably not disrupting their reading much. 


While we are planning on analysing data after mistakes, it might be interesting to know how much of the data is pre-mistake, and how many of the sentences are completed error-free. If we only exclude the errors and the first words of sentences, there are `r nrow(data_non_error)` words. If we also exclude all words after mistakes, there are `r nrow(data_before)` words. We will exclude only a couple of words after each mistake, but we could also opt to analyse only the pre-error sections. Of the `r nrow(data_sentence)` sentences that good participants completed, `r filter(data_sentence, sent.correct==1) %>% nrow()` were completed entirely correctly. 
# Discussion
We've shown another way to adapt the Maze task to make it suitable for a wider range of experiments. While we think that error-correction Maze is generally a good idea, especially when using A-maze, this is an orthagonal adjustment to the task. By using the Natural Stories corpus, we establish that Maze works for longer naturalistic tasks, with participants able to do the task and understand what they are reading. Additionally, their RTs reflect expected patterns in terms of length and surprisal. While there are some differences, especially in scale, to that found with other incremental processing paradigms, we think this is a reason to explore more. Could be a statistical thing (???) or due to task differences. Comparisons between methods could be very useful for identifying how task demands influence processing. 

All the code is available, and we encourage researchers to consider trying out Maze if they think it's appropriate to their experiments. 

This is just one other data point on A-maze, so we still don't know how widely the method holds up; however the fact that it works on such disparate materials gives us hope that it will be useful for a variety of paradigms. 

With new methods, its useful to try to establish how widely they work. We hope to test A-maze on an experiment that is very different from the syntactic disambiguation, in order to learn whether it works for a different type of psycholinguistic material. 
While Maze has traditionally been limited to single sentence items; we innovate on how the materials are presented and show that it is possible for participants to read naturalistic passages that are presented in the Maze paradigm. We confirm that participants can understand what they are reading, and that Maze shows linear surprisal effects. This opens up another area of research to being amenable to the new A-maze paradigm. 

This is just one other data point on A-maze, so we still don't know how widely the method holds up; however the fact that it works on such disparate materials gives us hope that it will be useful for a variety of paradigms. 

The Maze task might be espeically good for comparing with NNs because of the forced incrementality. While eventually we do want to figure out models of more natural reading, this might be a useful complement b/c we don't have to deal with nasty spillover. 
Might be extra good to do with pauses for mistakes to fix the incentives. 
This simple tweak in principle solves several problems. Firstly, it allows for running longer materials, potentially expanding the range of questions Maze is suitable for. Now, when participants make the occassional mistake or misclick, they can continue with the story, seeing every word of it. Thus, they provide reading times in later sentences and see all the context. Seeing the error message and resolving it is quick and does not disrupt the (already slow)  reading process too much.

Secondly, this error-correction compensates for some of the shortcomings of A-Maze identified in @boyceMazeMadeEasy2020; distractors might still cause participants to make unavoidable mistakes, but at least the still see the rest of the sentence and we get their data. We believe (although don't have evidence) that being able to correct mistakes may reduce frustration related to unavoidable mistakes from impossible choices and accidental misclicks. Whether this also solves the data loss problem from the researcher perspective within a sentence depends on whether post-mistake data are high-quality and trustworthy; this is a hard-to-assess question of potential interest. 

Even if researchers exclude all post-mistake data from analysis, this process can still address the question of whether errors are due to inattentive participants or bad distractors (as discussed in @boyceMazeMadeEasy2020, section XX). When we have a participants selection for every word, we can easy calculate a per-word error rate for each participant, without having to address the censoring present in error-terminating Maze (where we can't know if a participant would have made more mistakes after the first; with error correction, we know how may more they did make). If participants are guessing, we'll see high per-word error rates, if the early distractors are bad, we'll see low per-word error rates (with errors concentrated at the start of the sentences).  This per-word error rate metric measures participants task accuracy, and allows us to exclude data from inattentive participants. This provides a convenient and clear-cut way of controlling data quality after the fact. 

It also starts to reduce the perverse incentives. With error-terminates Maze, the fastest way to get through the task is to select randomly, and it's quite quick because mistakes skip you to the next sentence. With error-correction Maze, randomly jamming buttons takes more effort, but is still an effective strategy. [Footnote: In discussing this work, we recieved the suggestion that one way to disincentivize random clicking is to add a pause when a participant makes a mistake, forcing them to wait some short period of time (ex 500ms or 1 sec) before being able to correct their mistake. This seems like a promising improvement that could be worth implementing and testing.]
\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
