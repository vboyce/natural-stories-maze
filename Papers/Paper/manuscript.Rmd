---
title             : "A-maze of Natural Stories: Texts are comprehensible during the Maze task"
shorttitle        : "A-maze of Natural Stories"

author: 
  - name          : "Veronica Boyce"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "TODO"
    email         : "vboyce@stanford.edu"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Formal Analysis
      - Investigation
      - Software
      - Writing - Original Draft Preparation
      - Writing - Review & Editing
  - name          : "Roger Levy"
    affiliation   : "2"
    role:
      - Conceptualization
      - Formal Analysis
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Stanford University"
  - id            : "2"
    institution   : "Massachusetts Institute of Technology"
header-includes:
 - \usepackage{setspace}\singlespacing

authornote: |
  TODO
  
abstract: |
  TODO Easy to use, reliable methods are important to science. In studying how people understand language in real time, we use incremental processing methods to measure word-by-word reading time. However, neither of the common methods (eye-tracking and self-paced reading) can run over the web and produce localized effects. Another method, called the Maze task, seems to be able to do this, especially since a technique called A-maze makes the task much faster to construct. Due to task limitations, Maze has not been used on long, naturalistic passages, only short targeted materials. Here we present an adaptation of the Maze method suitable for long materials; we test this method on the Natural Stories corpus and find that participants can comprehend what they read while doing the task and that the reading time patterns are broadly similar to those found with other methods. We find support for the localization of reading time effects during the Maze task, as well as extending the range of materials Maze is suitable for. 
  
keywords          : "TODO"
wordcount         : "X"

bibliography      : ["r-references.bib","refs.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
 library("papaja")
 library("citr")
 library(here)
 library(tidyverse)
library(patchwork)
 library(brms)
 library(lme4)
 library(tidybayes)
 library(mgcv)
 library(cowplot)
 library(gridExtra)
r_refs("r-references.bib", append=F)
theme_set(theme_bw())
options(knitr.table.format = "pdf")
```
# Intro
We process language incrementally, building a model of the sentence as we hear or read word.  When a word is unexpected and does not fit the model of the sentence so far, our understanding of the sentence may need to be revised. Incremental processing methods measure how long each word takes to read as a proxy for how hard it is to incoroporate into the mental model of the sentence. Fluent language uses process language rapidly and flexibly adapt to many unexpected words, which makes it hard to pinpoint slow downs related to difficult-to-process words. 

TODO motivate why we care about localization

The two most commonly used methods are eye-tracking and self-paced reading. Both these methods are known to have spillover effects, which makes localization harder. CITE some things TODO might be worth bringing up RT/surprisal/spillover findings hereOur next question is what the relationship between a word's predictability and it's reading time is for Maze. It's well established for eye-tracking and SPR that RTs are roughly linear in terms of a word's surprisal (negative log probability). Due to spillover effects, on SPR and eye-tracking, there is also a positive linear relationship between the surprisal of a previous word and the RT on the current word -- this is an indication of lack of localization. In addition to suprisal predicting RT, word length and word's overall frequency are also often found to be predictive. TODO many CITES

An alternative method that seems to have superior localization is the Maze task, which adopts an unnatural way of reading to force incremental processing [@forsterMazeTaskMeasuring2009]. In the Maze task, participants see two words at a time, a correct word that continues the sentence, and a distractor which does not. Participants must choose the correct word, and their reaction time (RT) is the dependent measure. If participants make a mistake, the sentence discontinues. Theoretically, participants must fully integrate each word into the sentence in order to confidently select it. This idea is supported by studies finding strongly localized effects [@witzel12]. 

The downside of Maze is that materials are effort-intensive to construct because of the need to select infelicitious words as distractors for each spot of each sentence; this may explain why the Maze task was not widely adopted. 
@boyceMazeMadeEasy2020 demonstrate a way to automatically generate Maze distractors by using NLP language models to find words that are high-surprisal in the context of the target sentence. The quality of these A-Maze distractors is not up to that of hand-generated distractors, but @boyceMazeMadeEasy2020 found that materials with A-maze distractors had similar results to the hand-generated distractors from @witzel12. @sloggettAmazeAnyOther also found that A-maze and G-maze distractors yielded similar results on a disambiguation paradigm.

While new, A-maze seems like a potentially powerful addition to the psycholinguists toolkit. However, like the other Maze tasks it has been limited in its application to single-sentence items probing minimal comparisons in constructed sentences. While some psycholinguistic phenomena are well-explored within individual targeted sentences, there's also a need for reading time data on longer passages, for instance in order to compare to language models or study discourse effects. Therefore, it's important to test whether the Maze task works on these types of materials. 

While the issue of needing to generate distractors for long passages is solved with A-maze, another problem with Maze remains. In particular, because Maze tasks discontinue after participants make mistakes, the farther into an item a word is, the fewer participants see it. This makes it hard to run long materials using Maze, and prevents Maze from being used on long text passages where SPR or eye-tracking could be used. 

We address this problem by creating a Maze task interface where participants correct their mistakes and continue with the sentence rather than terminating on mistakes. This allows for multi-sentence items to be run using Maze, which we verify by running Maze on the Natural Stories corpus. With this data, we are able to confirm that participants can comprehend what they read using Maze and investigate the effects of surprisal on RT in Maze. 



# Error-correction maze
(ref:diagram-cap) Schematic of error-correction Maze. A participant reads a sentence word by word, choosing the correct word at each time point (selections marked in blue). When they make a mistake, an error message is displayed, they try again, and continue with the sentence. 

```{r diagram, out.height="25%", fig.width=8, fig.height=3, fig.pos='t', fig.cap="(ref:diagram-cap)"}
knitr::include_graphics(paste0(here(),"/Papers/maze_diagram_cropped.pdf"))
```
One advantage of the Maze task is that it forces incremental processing and automatically excludes inattentive participants by terminating a sentence when a participant makes an error. Thus, only participants who have processed the sentence and are paying attention contribute data at critical regions later in the sentence. However, this means we don't have data after a participant makes a mistake in an item. In traditional G-maze tasks, with hand-crafted distractors and attentive participants, this is a small issue. However, this data loss is much worse with A-maze materials and crowd-sourced participants [@boyceMazeMadeEasy2020]. The high errors are likely from some combination of participants guessing randomly and from auto-generated distractors that in fact fit the sentence; as @boyceMazeMadeEasy2020 noted, some distractors, especially early in the sentence, were problematic and caused considerable data loss. 

We could try to improve the situation by auto-generating better distractors or hand-replacing problematic ones. However, the inherent data losses poses a more fundamental problem: even with attentive participants and good distractors, the Maze paradigm will eventually run into problems as error rates compound over long materials where few participants will make it to the end. For instance, with a 1% error rate, `r round(.99**15*100)`% would complete each 15-word sentence, but only `r round(.99**50*100)`% of participants would complete a 50 word vignette, and `r round(.99**200*100)`% a 200 word passages.  In order to run longer materials, we need something to do when participants make a mistake, other than terminate the entire item.

To resolve this, we introduce an *error-correction* variant of Maze, where we let participants continue on when they make a mistake. We present them with an error message and wait until they select the correct option, before continuing the sentence as normal. This *error-correction* variant is shown in Figure \@ref(fig:diagram). We make this "error-correction" Maze available as an option in a modification of the Ibex Maze implementation introduced in @boyceMazeMadeEasy2020 (TODO repo link). The code records both the RT to the first click and also the total RT until the correct answer is selected as separate values.

Our goal is that this error-correction will minimize the problems of having occasional poor auto-generated distractors by letting participants continue on and still read the whole item. It will also enable us to run long materials on Maze, thus expanding the range of questions that Maze can be used on. 


# Natural Stories corpus

We test this error-correction Maze on the Natural Stories corpus. The Natural Stories corpus [@futrellNaturalStoriesCorpus2020] consists of 10 passages each roughly 1000 words long which are designed to read fluently to native speakers. At the same time, the passages contain copious punctuation (including quoted speech), many proper nouns, and low frequency grammatical constructions. Taken together, these properties make this a severe test of our process. The materials are on the difficult side for our participants and for the language model we use to generate distractors. If participants can succeed at the Maze task on this set of materials, we think they are likely to succeed on basically any naturalistic text. 

The corpus also comes with binary-choice comprehension questions, 6 per story, which we use to assess comprehension. Using this corpus on the A-maze task allows us to address, first whether participants will read and understand long passages using A-maze with error correction, and second, whether the resulting RTs profiles will show expected patterns such as a linear relationship with surprisal. 

# Methods

```{r participants}

data <- read_rds(paste0(here(),"/Data/cleaned.rds"))

data_filt <- data %>% 
  filter(native %in% c("ENG", "English", "ENGLISH", "english")) #I peeked at what people put that semantically maps to english

data_stories <- data_filt %>% 
  select(type, subject) %>% 
  unique() %>% 
  group_by(type) %>% 
  tally() %>% 
  filter(type!="practice")

data_error_summ <- data_filt %>% 
  mutate(correct.num=ifelse(correct=="yes", 1,0)) %>% 
  group_by(subject, num_correct) %>%
  filter(type!="practice") %>% 
  filter(rt<5000) %>% 
  summarize(mean_rt=mean(rt),
            pct_correct=mean(correct.num)) %>% 
  mutate(good_comp=ifelse(num_correct>4,"5 or 6 correct","4 or fewer correct")) %>% 
    mutate(accurate=ifelse(pct_correct>.8,">80% correct", "<80% correct"),
           is.attentive=ifelse(pct_correct>.8,1,0))

some <- data_filt %>% select(subject, num_correct) %>% 
  unique() %>% 
  left_join(data_error_summ, by=c("subject", "num_correct")) %>% 
  mutate(accurate=ifelse(pct_correct>.8,">80% correct", "<80% correct")) %>% 
  group_by(num_correct,accurate)

data_good <- data_filt %>% 
  left_join(data_error_summ, by=c("subject", "num_correct")) %>% 
  filter(type!="practice") %>% 
  filter(pct_correct>.8) %>% 
  mutate(is.correct=ifelse(correct=="yes",1,0))

data_non_error <- data_good %>% 
  filter(is.correct==1) %>% 
  filter(word_num>0)

data_before <- data_good %>% 
  mutate(word_num_mistake=ifelse(correct=="no", word_num,NA)) %>% 
  group_by(sentence, subject) %>% 
  fill(word_num_mistake) %>% ungroup() %>% 
  mutate(after_mistake=word_num-word_num_mistake,
         after_mistake=ifelse(is.na(after_mistake),0,after_mistake)) %>% 
  filter(after_mistake==0) %>% 
  filter(is.correct==1) %>% 
  filter(word_num>0)

data_sentence <- data_before <- data_good %>% 
  mutate(word_num_mistake=ifelse(correct=="no", word_num,NA)) %>% 
  group_by(sentence, subject) %>% 
  fill(word_num_mistake) %>% ungroup() %>% 
  mutate(after_mistake=word_num-word_num_mistake,
         after_mistake=ifelse(is.na(after_mistake),0,after_mistake)) %>% 
  group_by(sentence, subject) %>% 
  summarize(total_mistake=sum(after_mistake)) %>% 
  mutate(sent.correct=ifelse(total_mistake==0,1,0))
```

We constructed A-maze distractors for the Natural stories corpus [@futrellNaturalStoriesCorpus2020] and recruited 100 MTurkers to each read a story in the Maze paradigm. 

## Materials
We took the texts of the Natural Stories corpus [@futrellNaturalStoriesCorpus2020] and split it into sentences because the A-Maze process works on the sentence level. We used the original comprehension questions provided in the Natural Stories corpus. To familiarize participants with the task, we wrote a short practice passage and corresponding comprehension questions.
We then ran the sentences of the items through the A-maze generation process. We updated the code of the generation process to fix some issues identified in that paper; the code used is available at REPO. The underlying method did not change, but features such as appropriately handling more types of punctuation (needed for the Natural Stories corpus) were added. We ran the materials using FOOBAR parameters. We used the auto-generated distractors as they were, without checking them for quality. Materials are available at REPO.
TODO other notable changes to the codebase??

## Participants
We recruited 100 participants from Amazon Mechanical Turk, and paid each participant 3.50 dollars. We excluded data from those who did not report English as their native language, leaving `r data_filt %>% select(subject) %>% unique() %>% nrow()` participants. 

## Procedure
Participants first gave their informed consent and saw task instructions. Then they read a short practice story in the Maze paradigm and answered 2 binary-choice practice comprehension questions, before reading the main story in the A-maze task. After the story, they answered the 6 main comprehension questions, commented on their experience, answered optional demographic questions, saw an debriefing and were given a code to enter into Mturk for payment. The experiment was implemented in Ibex (CITE) and the experimental code is availabe at REPO.  

## Data analysis
Analyses were done in R (CITE), using the following CURATE packages. We used `r cite_r("r-references.bib")` for all our analyses.

# Results

## Reading stories in the Maze task 

(ref:error-cap) A. Correlation between a participant's accuracy on the Maze task (fraction of words selected correctly) and their average reaction time (in ms). Many participants (marked in green) chose the correct word >80% of the time; others (in red) appear to be randomly guessing and were excluded from further analysis. B. Performance on the comprehension questions. Participants who had >80% task accuracy tended to do well on comprehension questions; those who were at chance on the task were also at chance on comprehension questions. 

```{r errors, out.height="25%", fig.width=8, fig.height=3, fig.pos='t', fig.cap="(ref:error-cap)"}
error_plot <- ggplot(data_error_summ, aes(x=pct_correct, y=mean_rt, color=accurate))+
  geom_point(size=1)+
  labs(x="Fraction words selected correctly",
       y="Mean Reaction Time (ms)")+
  coord_cartesian(xlim=c(0.4,1), ylim=c(0,1600), expand=F)+
    scale_color_manual(values=c(">80% correct"="darkgreen","<80% correct"="darkred"))+
  guides(color=FALSE)

comp_plot <- ggplot(some, aes(x=num_correct,fill=accurate))+
  geom_bar(position="dodge")+
  facet_grid(.~accurate)+
  labs(x="Comprehension questions correct (out of 6)", y="Participants")+
  scale_fill_manual(values=c(">80% correct"="darkgreen","<80% correct"="darkred"))+
guides(fill=FALSE)

error_plot+comp_plot+plot_annotation(tag_levels="A")

```

Our first concern was whether this error-correction paradigm on long passages would work. It did; some participants were able to complete the Maze task with high accuracy and understand the story while doing so. 

Participant accuracy reflects how well participants can navigate the task and whether the auto-generated distractors are of acceptable quality. We calculated the per-word error rate for each participant and graphed it against their average reaction time. (To avoid biasing the average if a participant took a pause before returning to the task, RTs greater than 5 seconds were excluded.) As seen in Figure \@ref(fig:errors)A, there is a cluster of participants who make relatively few errors (marked in green), with some reaching 99% accuracy. This indicates that the distractors were generally appropriate and that some participants can maintain focus on the task for the whole story. These careful participants tend to take around 1 second for each word selection, which is much slower than other paradigms CITE SPR something. We also have a cluster of participants (in red) who we infer sped through the task clicking randomly. This distribution is likely due to the mix of workers on Mturk; as we did not use qualification cutoffs. 

Another check is whether participants comprehended the story. We counted how many of the binary-choice comprehension questions each participant got right (out of 6), and group them by their performance on the Maze task. As seen in Figure \@ref(fig:errors)B, most participants were were accurate on the task also did well on comprehension questions, while participants who were at chance on the Maze task were also at chance on the comprehension questions. Participants usually answered quickly (within 10 seconds), so we do not believe they were looking up the answers on the internet. These are not especially hard questions, and we can't rule out that some participants may have been able to guess the answers without reading the story. Nonetheless, this provides preliminary evidence that people can understand and remember details of stories they read during the Maze task. 

Given that Maze task accuracy and comprehension question accuracy are correlated, but task accuracy is finer-grained, we use task performance as our exclusion metric. This easy to calculate speed and accuracy comparision makes it easy to see who was paying attention to the task; we exclude the random guessers and only analyse data from participants with at least 80% accuracy (in the gap between high-performers and low-performers). 


 
## RT and surprisal

```{r}
subject_att <- data_error_summ %>% 
  select(subject, is.attentive)

data_low_error <- data_filt %>% 
  left_join(subject_att, by="subject") %>% 
  filter(is.attentive==1) %>% 
  filter(type!="practice")

  data_error_free <- data_low_error %>% 
    mutate(word_num_mistake=ifelse(correct=="no", word_num,NA)) %>% 
    group_by(sentence, subject) %>% fill(word_num_mistake) %>% ungroup() %>% 
    mutate(after_mistake=word_num-word_num_mistake,
           after_mistake=ifelse(is.na(after_mistake),0,after_mistake)) %>% 
    filter(correct=="yes") %>% 
    filter(!after_mistake %in% c(1,2))
  
data_no_first <- data_error_free %>% filter(word_num!=0)

data_ready <- data_no_first %>% filter(rt>100 & rt<5000) %>% 
  select(subject, word_num, word, rt, sentence, type)

data_post_only <- data_no_first %>% filter(rt>100 & rt<5000) %>% 
  filter(!after_mistake==0) %>% 
    select(subject, word_num, word, rt, sentence, type)

data_pre_error <- data_no_first %>% filter(rt>100 & rt<5000) %>% 
  filter(after_mistake==0) %>% 
  select(subject, word_num, word, rt, sentence, type)

data_stories <- data_ready %>% select(type, subject) %>% 
  unique() %>% 
  group_by(type) %>% 
  tally()

data_anything_goes <- data_filt %>% 
  filter(type!="practice") %>% 
  filter(correct=="yes") %>% 
    select(subject, word_num, word, rt, sentence, type)

```



### GAMs

We wanted to see if Maze also showed a linear relationship between surprisal and RT, and whether this was localized at the current word or whether previous word surprisals were also influential. To do this, we created a set of predictors of frequency, word length, and surprisals from several different langauge models, and fit generalized additive models to check for a linear relationship.
 For length, we use the length of characters (excluding end punctuation). For a measure of unigram frequency, we tokenize the training data from @gulordava18 (this tokenizes off punctuation, but preserves case) and tally up instances. For the models, we use the log2 frequency of the expected occurances in 1 billion words (thus frequency is log, but higher values indicate higher frequency words). For surprisals, we get per-word surprisals for each of 3 different language models: a Kneser-Ney smoothed 5-gram, GRNN [@gulordava], and Transformer-XL [@daiTransformerXLAttentiveLanguage2019a]. We centered but did not rescale the length and frequency predictors; for interpretability, we kept surprisal uncentered. For all of these predictors, we consider both the predictor at the current word as well as lagged predictors from the previous word. CITE some papers that did the same thing . We used smooths for the surprisal terms and tensor effects for the frequency by length effects and interactions. 

We only include words that were in the vocabularies of all three models as a single token and for which we have frequency information. This has the effect of excluding words that had punctuation as well as some uncommon or proper nouns. We also exclude the first word of every sentence (which had a dummy distractor). We do not analyse RTs for words where the RT was <100 or >5000 ms (<100 is likely a recording error, >5000 is likely the participant getting distracted). We also omit words where the participant made a mistake and had made a mistake earlier in the sentence (Although see supplement for results including post-mistake TODO). 

(ref:gam-cap) GAM predictions of reaction time (RT) as a function of either current word surprisal (top) or previous word surprisal (bottom). Density of data is shown along the x-axis. For each of the 3 language models used, there is a linear relationship between current word surprisal and RT (at least when there is enough data). There is no relationship between previous word surprisal and RT. 

```{r gam, out.height="40%", fig.width=8, fig.height=3, fig.pos='t', fig.cap="(ref:gam-cap)"}
labelled_pre_error <- read_rds(paste0(here(),"/Analysis/pre_error.rds"))

ngram_data <- labelled_pre_error %>% select(rt, surprisal=ngram_surp, prev_surp=past_ngram, 
                                            freq=freq_center, len=length_center,
                                            prev_freq=past_c_freq, prev_len=past_c_length
                                            ) %>% mutate(model="5-gram")

grnn_data <- labelled_pre_error %>% select(rt, surprisal=grnn_surp, prev_surp=past_grnn, 
                                            freq=freq_center, len=length_center,
                                            prev_freq=past_c_freq, prev_len=past_c_length
                                            )%>% mutate(model="GRNN")

txl_data <- labelled_pre_error %>% select(rt, surprisal=txl_surp, prev_surp=past_txl, 
                                            freq=freq_center, len=length_center,
                                            prev_freq=past_c_freq, prev_len=past_c_length
                                            ) %>% mutate(model="TXL")

all_data <- ngram_data %>% union(grnn_data) %>% union(txl_data) %>% 
  select(surprisal, prev_surp,model) %>% 
  pivot_longer(cols=`surprisal`:`prev_surp`) %>% 
  mutate(s=ifelse(name=="surprisal", "Current","Previous"))

all <-read_rds(paste0(here(),"/Analysis/gam_predictions.rds"))
gam1 <-  ggplot(all, aes(x=surprisal, y=rt, ymin=CI_lower, ymax=CI_upper))+
  geom_line()+
  geom_ribbon(alpha=.3)+
   #geom_density(data=all_data, aes(x=value), fill="gray",)+
  facet_grid(s~model)+
  coord_cartesian(ylim=c(700,1300), xlim=c(0,28))+
  labs(x="Surprisal (bits)", y="Reaction Time (ms)")+theme(axis.ticks.x=element_blank(), axis.title.x=element_blank(), axis.text.x=element_blank(), plot.margin=margin(t=0,r=0,b=0,l=0,unit="pt"))


dens2 <-   ggplot(all_data, aes(x=value))+
  geom_density(fill="gray",)+
  facet_grid(.~model)+
  labs(x="Surprisal (bits)", y="")+
  theme(strip.text.x=element_blank(),axis.text.y=element_blank(), panel.grid=element_blank(), axis.ticks.y=element_blank(), plot.margin = unit(c(0, 0, 0, 0), "cm"))


p2 = align_plots(gam1, dens2, align = "v", axis="lr")
plot_grid(p2[[1]], p2[[2]], nrow=2, rel_heights = c(1, .3))
ggsave("sample_plot.pdf")


```

The smooths for the current and previous words surprisals are shown in Figure \@ref(fig:gam). Note that for each of the models, high-surprisal words at relatively rare, with much of the data for words between 0 and 15 bits of surprisal. All of the models show a roughly linear relationship between current word surprisal and RT, especially in the region with more data. All of the models show a flat relationship between previous word surprisal and RT. This is a sign of localization as the previous word's surprisal is not affecting RT, only the word's own suprisal is. The linear relationship matches that found with other methodologies. 

Given the linear relationship, we next fit linear models to look at coefficients and model fit. We fit BRMS models. 

### BRM models

We built models using BRMS, with surprisal, frequency, length and surprisal x length and frequency x length effects. Predictors were as described above, except all were centered. We used full mixed effects, including by-subject effects of everything and a per-word random intercept. We used weak priors (normal(1000,1000) for intercept, normal(0,500) for beta and sd, and lkj(1) for correlations). TODO CITE also maybe this is too details-y methods. 


```{r}

show_summary <- function(model){
  intervals <- gather_draws(model, `b_.*`, regex=T) %>% mean_qi()
  
  stats <- gather_draws(model, `b_.*`, regex=T) %>% 
    mutate(above_0=ifelse(.value>0, 1,0)) %>% 
    group_by(.variable) %>% 
    summarize(pct_above_0=mean(above_0)) %>% 
    mutate(`P` = signif(2*pmin(pct_above_0,1-pct_above_0), digits=2)) %>% 
    left_join(intervals, by=".variable") %>% 
    mutate(lower=round(.lower, digits=1),
           upper=round(.upper, digits=1),
           E=round(.value, digits=1),
           `Estimate`=str_c(E," [",lower,", ", upper,"]"),
           Term=str_sub(.variable, 3, -1),
           ) %>% 
    select(Term, `Estimate`)
  
  stats
}
```

```{r pre-error}
brm_txl_interact <- read_rds(paste0(here(),"/Analysis/brm_txl_interact.rds"))
brm_grnn_interact <- read_rds(paste0(here(),"/Analysis/brm_grnn_interact.rds"))
brm_ngram_interact <- read_rds(paste0(here(),"/Analysis/brm_ngram_interact.rds"))
a <- show_summary(brm_txl_interact) %>% mutate(model="TXL") %>% mutate(Term=str_replace(Term,"txl","surp"))
b <- show_summary(brm_grnn_interact) %>% mutate(model="GRNN") %>% mutate(Term=str_replace(Term,"grnn","surp"))
c <- show_summary(brm_ngram_interact) %>% mutate(model="5-gram") %>% mutate(Term=str_replace(Term,"ngram","surp"))

summ <- a %>% union(b) %>% union(c) %>% pivot_wider(names_from="model", values_from=c(`Estimate`)) %>% 
  mutate(Term=factor(Term, 
                        levels=c("Intercept", 
                                 "surp_center",
                                 "length_center",
                                 "freq_center",
                                 "surp_center:length_center",
                                 "length_center:freq_center",
                                 "past_c_surp",
                                 "past_c_length",
                                 "past_c_freq",
                                 "past_c_surp:past_c_length",
                                 "past_c_length:past_c_freq")
                                 )) %>% 
  arrange(Term) %>% 
  mutate(Term=c("Intercept", "Surprisal", "Length", "Frequency",
                  "Surp x Length", "Freq x Length", "Past Surprisal",
                  "Past Length", "Past Freq", "Past Surp x Length", "Past Freq x Length")) %>% 
  select(Term, `5-gram`, `GRNN`, TXL) 

knitr::kable(summ, format="latex", position="t",caption="Predictions from fitted Bayesian regression models. All terms were centered, but not rescaled. Units are in ms. Surprisal is per bit, length per character, and frequency per $log_2$ occurance per billion words.")
```



As we can see in Table \@ref(tab:pre-error), we find large effects of surprisal and length, but minimal effects of frequency. These effects are larger than what is usually reported in other methods CITE, but this could be due to the overall slowness of the method; the intercepts are also much bigger. The lack of frequency effects is somewhat surprising, and some CITE argue that they aren't real anyway. Notably the coefficients for the lagged terms are smaller than surprisal and length of the current word.  

### Model comparison

We also ran a nested model comparison with models fit in lmer. For this, we fit models with only frequency and length as predictors, as well as models that also had one or more sources of surprisal. For all effects, we included both current and lagged versions of it. The model with all 3 surprisal predictors is better than any model with only one. Any one surprisal predictor is better than no surprisals predictors. However, adding ngram predictors to a model that already has txl & grnn does not help. In other cases, adding the 3rd surprisal source to the other two does help. Ngram+Grnn is not better than Grnn only. Otherwise, pairs are better than singletons. This suggests that Ngram's info is a subset of GRNN, but not a subset of TXL. 

```{r}
d_lm <- labelled_pre_error %>% group_by(word, txl_center, ngram_center, grnn_center, freq_center, length_center,
                                        past_c_txl, past_c_ngram, past_c_grnn, past_c_freq, past_c_length, Word_ID) %>% 
  summarize(mean_rt=mean(rt))

no_surp <- lmer(mean_rt ~ freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

ngram_only <- lmer(mean_rt ~ ngram_center + past_c_ngram + freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

grnn_only <- lmer(mean_rt ~ grnn_center + past_c_grnn + freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

txl_only <- lmer(mean_rt ~ txl_center + past_c_txl + freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

all_surp <- lmer(mean_rt ~ ngram_center + past_c_ngram + 
                   grnn_center + past_c_grnn +
                   txl_center + past_c_txl +
                   freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)


ngram_grnn <-  lmer(mean_rt ~ ngram_center + past_c_ngram + 
                   grnn_center + past_c_grnn +
                   freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

ngram_txl <-  lmer(mean_rt ~ ngram_center + past_c_ngram + 
                   txl_center + past_c_txl +
                   freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

grnn_txl <-  lmer(mean_rt ~ grnn_center + past_c_grnn +
                   txl_center + past_c_txl +
                   freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

no_lag <-  lmer(mean_rt ~ ngram_center +
                   grnn_center +
                   txl_center + 
                   freq_center*length_center+(1|word), data=d_lm)

no_surp_lag <-  lmer(mean_rt ~ ngram_center +  
                   grnn_center + 
                   txl_center +                    freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

  only_surp_lag <-  lmer(mean_rt ~ ngram_center + past_c_ngram + 
                   grnn_center + past_c_grnn +
                   txl_center + past_c_txl +
                   freq_center*length_center+(1|word), data=d_lm)
```

```{r, include=F}

anova(all_surp, txl_only)
anova(all_surp, grnn_only)
anova(all_surp, ngram_only)

anova(txl_only, no_surp)
anova(grnn_only, no_surp)
anova(ngram_only, no_surp)

anova(all_surp, ngram_grnn)
anova(all_surp, ngram_txl)
anova(all_surp, grnn_txl)

anova(ngram_grnn, ngram_only)
anova(ngram_grnn, grnn_only)
anova(ngram_txl, ngram_only)
anova(ngram_txl, txl_only)
anova(grnn_txl, grnn_only)
anova(grnn_txl, txl_only)

anova(all_surp, no_lag)

anova(all_surp, no_surp_lag)

anova(no_surp_lag, no_lag) 

anova(all_surp, only_surp_lag)

anova(only_surp_lag, no_lag)
```



<!-- ## Comparision with SPR
We want to compare how good our participants were on the comprehension questions to how good people were on the SPR experiment reported in Futrell et al (2017). They had each story read about 100 times across 181 participants (participants generally read 5 stories). This means they have about 10 times as much data as we do. However, reading times in a story were excluded based on the comprehension of that story, so it makes sense to compare per-story accuracies with our per-story accuracies. -->

<!-- ## Error patterns
When participants do make errors, they correct them quickly, generally within one second, which means it's probably not disrupting their reading much. 


While we are planning on analysing data after mistakes, it might be interesting to know how much of the data is pre-mistake, and how many of the sentences are completed error-free. If we only exclude the errors and the first words of sentences, there are `r nrow(data_non_error)` words. If we also exclude all words after mistakes, there are `r nrow(data_before)` words. We will exclude only a couple of words after each mistake, but we could also opt to analyse only the pre-error sections. Of the `r nrow(data_sentence)` sentences that good participants completed, `r filter(data_sentence, sent.correct==1) %>% nrow()` were completed entirely correctly. -->

# Discussion
[summarize results] 
- error-correction paradigm
- test on natural stories
- it works

We've shown another way to adapt the Maze task to make it suitable for a wider range of experiments. While we think that error-correction Maze is generally a good idea, especially when using A-maze, this is an orthagonal adjustment to the task. By using the Natural Stories corpus, we establish that Maze works for longer naturalistic tasks, with participants able to do the task and understand what they are reading. Additionally, their RTs reflect expected patterns in terms of length and surprisal. While there are some differences, especially in scale, to that found with other incremental processing paradigms, we think this is a reason to explore more. Could be a statistical thing (???) or due to task differences. Comparisons between methods could be very useful for identifying how task demands influence processing. 

All the code is available, and we encourage researchers to consider trying out Maze if they think it's appropriate to their experiments. 



While Maze has traditionally been limited to single sentence items; we innovate on how the materials are presented and show that it is possible for participants to read naturalistic passages that are presented in the Maze paradigm. We confirm that participants can understand what they are reading, and that Maze shows linear surprisal effects. This opens up another area of research to being amenable to the new A-maze paradigm. 


The Maze task might be espeically good for comparing with NNs because of the forced incrementality. While eventually we do want to figure out models of more natural reading, this might be a useful complement b/c we don't have to deal with nasty spillover. 
Might be extra good to do with pauses for mistakes to fix the incentives. 

Secondly, this error-correction compensates for some of the shortcomings of A-Maze identified in @boyceMazeMadeEasy2020; distractors might still cause participants to make unavoidable mistakes, but at least the still see the rest of the sentence and we get their data.  Whether this also solves the data loss problem from the researcher perspective within a sentence depends on whether post-mistake data are high-quality and trustworthy; this is a hard-to-assess question of potential interest. 

Even if researchers exclude all post-mistake data from analysis, this process can still address the question of whether errors are due to inattentive participants or bad distractors (as discussed in @boyceMazeMadeEasy2020, section XX). When we have a participants selection for every word, we can easy calculate a per-word error rate for each participant, without having to address the censoring present in error-terminating Maze (where we can't know if a participant would have made more mistakes after the first; with error correction, we know how may more they did make). If participants are guessing, we'll see high per-word error rates, if the early distractors are bad, we'll see low per-word error rates (with errors concentrated at the start of the sentences).  This per-word error rate metric measures participants task accuracy, and allows us to exclude data from inattentive participants. This provides a convenient and clear-cut way of controlling data quality after the fact. 

It also starts to reduce the perverse incentives. With error-terminates Maze, the fastest way to get through the task is to select randomly, and it's quite quick because mistakes skip you to the next sentence. With error-correction Maze, randomly jamming buttons takes more effort, but is still an effective strategy. [Footnote: In discussing this work, we recieved the suggestion that one way to disincentivize random clicking is to add a pause when a participant makes a mistake, forcing them to wait some short period of time (ex 500ms or 1 sec) before being able to correct their mistake. This seems like a promising improvement that could be worth implementing and testing.]
\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
