---
title             : "A-maze of Natural Stories: Comprehension and surprisal in the Maze task TODO title"
shorttitle        : "A-maze of Natural Stories"

author: 
  - name          : "Veronica Boyce"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Jane Stanford Way, Building 420, Stanford University, Stanford, CA 94305"
    email         : "vboyce@stanford.edu"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Formal Analysis
      - Investigation
      - Software
      - Writing - Original Draft Preparation
      - Writing - Review & Editing
  - name          : "Roger Levy"
    affiliation   : "2"
    role:
      - Conceptualization
      - Formal Analysis
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Stanford University"
  - id            : "2"
    institution   : "Massachusetts Institute of Technology"
header-includes:
 - \usepackage{setspace}\singlespacing
 - \renewcommand{\textfraction}{0.05}
 - \renewcommand{\topfraction}{0.8}
 - \renewcommand{\bottomfraction}{0.8}
 - \renewcommand{\floatpagefraction}{1}
 - \setcounter{topnumber}{3}
 - \setcounter{bottomnumber}{3}
 - \setcounter{totalnumber}{4}

authornote: |
  TODO
  
abstract: |
   TODO needs work
   A-maze is a new method for measuring incremental sentence processing that can localize slowdowns related to syntactic ambiguities. We adapt A-maze for use on longer passages and test it on the Natural Stories corpus. We find that people can comprehend these longer text passages during the Maze task. Moreover, the  task yields useable reaction time data with word predictability effects that are linear in the surprisal of the current word, with little spillover effect from the surprisal of the previous word. This expands the types of effects that can be studied with A-maze, showing it to be a a versatile alternative to eye-tracking and self-paced reading.We find support for the localization of reading time effects during the Maze task, as well as extending the range of materials Maze is suitable for.  How long it takes to read a word in a sentence is reflective of how hard it is to identify and integrate the word in the surrounding context. Techniques that slow down the reading process and localize the processing time for each word are useful to understanding the time course of language processing.

keywords: TODO
wordcount: TODO

bibliography      : ["r-references.bib","refs.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(dev = "png", dev.args = list(type = "cairo-png"))
 library("papaja")
 library(here)
 library(tidyverse)
library(patchwork)
 library(brms)
 library(lme4)
 library(tidybayes)
 library(mgcv)
 library(cowplot)
 library(gridExtra)
library(broom.mixed)
library(tidymv)

r_refs("r-references.bib", append=F)
theme_set(theme_bw())
options(knitr.table.format = "pdf")
```
# Introduction

TODO overall: go through Roger's comments!
TODO fix tense!

Sometimes we stumble in reading, where we expected a sentence to go in one direction and it did not, and then are at a parsing dead-end where we have to read a sentence again. The words that cause stumbles are the outliers on a distribution of words difficulty to read, within fluent reading, there is variation in how easy words are to read and process in their context.  We're able to read many long or surprising words without noticing a problem, but these less expected words do take longer to process as we rebuild our burgeoning mental model of the sentence. Fortunately for fluent readers and unfortunately for studying language, this process of adjustment is very quick, with millisecond scale differences in reading time between words.

We don't have a thorough understanding of how the mind processes language, but we can learn something about what is going on from measuring how long it takes in different circumstances. This relies on a linking hypothesis that processing time reflects something about how much processing is going on, even if we don't know whether this is character recognition, memory retrieval, or parsing. This is enough to compare theories that predict the relative difficulty, and thus relative processing time, of two words in different contexts. 

For instance, different theories about what makes object relative clauses harder to understand than subject relative clauses make different predictions about which words are the loci of the overall difficulty and slower reading times associated with object relatives [@grodnerConsequencesSerialNature2005; @staubEyeMovementsProcessing2010; @traxlerProcessingSubjectObject2002]. Measures of reading time can also inform theories about the time course of processing (i.e. which steps are parallel versus serial, @bartekSearchOnlineLocality2011) or the functional form of relationships between word characteristics and processing time [@smithEffectWordPredictability2013].

Some of these theories rely on being able to attribute processing slowdowns to a particular word. Determining that object relatives are overall slower that subject relatives is easy, even an imprecise measure of reading time will determine that the same set of words in a different order took longer to read on a sentence level. However, many language processing theories make specific (and contrasting) theories about which words in a sentence should be harder to process. To adjudicate these theories, we want methods that are *localized*, where it is easy to determine which word is responsible for an observed slow-down in reading time. Ideally, a longer reading time on a word would be an indication of that word's increased difficulty, and not the lingering signal of a prior word's increased difficulty. When the signal isn't localized, advanced analysis techniques may be required to disentangle the slow-downs [@shainDeconvolutionalTimeSeries2018]. 

## Incremental processing methods

Behavioral methods that measure how long it takes to read each word in a sentence are referred to as incremental processing methods, and their dependent measures are called reading times or reaction times, both abbreviated RT. The two most commonly used methods are eye-tracking and self-paced reading, but both of these suffer from a lack of localization. In eye-tracking, participants read a text on a screen naturally, while their eye-movements are recorded [@raynerEyeMovementsReading1998]. The downside of this natural reading is that people often skip short words and look ahead or look back as they read, the dynamics of which make it hard to isolate effects [@raynerEffectsFrequencyPredictability2004; @levyEyeMovementEvidence2009; @frazierMakingCorrectingErrors1982]. Self-paced reading (SPR) is a more controlled process where a participant sees one word on screen at a time and presses a button to see the next word instead. Even in this method, readers may maintain ambiguities about what a word was or means until it is later resolved by context, so it may take multiple words for slowdowns to catch up with them [@macdonaldInteractionLexicalSyntactic1993; @koornneefUseVerbbasedImplicit2006]. Both these methods are prone to spillover, as the time to plan motor movements like saccades or button presses is substantial compared to the time it takes to recognize a word, these movements are often initiated before a word is fully processed, so by the time any late processing difficulties occur, the reader may already be several words along. 

### the Maze task
An alternative method that is designed to increase localization at the expense of naturalness is the Maze task [@freedman85; @forsterMazeTaskMeasuring2009]. In the Maze task, participants see two words at a time, a correct word that continues the sentence, and a distractor which does not. Participants must choose the correct word, and their time to selection is treated as the reaction time (RT). @forsterMazeTaskMeasuring2009 introduces two versions of the Maze task: lexical L-maze where the distractors are nonce words and grammatical G-maze where the distractors are real words that don't fit with the context of the sentence so far. Theoretically, participants must fully integrate each word into the sentence in order to confidently select it; this may require mentally reparsing previous material in order to allow the integration and selection of a disambiguating word. @forsterMazeTaskMeasuring2009 call this forced incremental processing to distinguish from other incremental processing methods where words can be passively read before later committing to a parse. This idea of strong localization is supported by studies finding strongly localized effects for G-maze which is more sensitive than L-maze [@witzelComparisonsOnlineReading2012a]. 

The downside of G-maze is that materials are effort-intensive to construct because of the need to select infelicitious words as distractors for each spot of each sentence; this may explain why the Maze task has not been widely adopted. @boyceMazeMadeEasy2020 demonstrate a way to automatically generate Maze distractors by using NLP language models to find words that are high-surprisal in the context of the target sentence, and thus likely to be judged infelicitous by human readers. @boyceMazeMadeEasy2020 found that materials with A-maze distractors had similar results to the hand-generated distractors from @witzelComparisonsOnlineReading2012a. A-maze outperformed L-maze and an SPR control in detecting and localizing expected slowdown effects. @sloggettAmazeAnyOther2020 also found that A-maze and G-maze distractors yielded similar results on a disambiguation paradigm.

Another recent variant of the Maze task is interpolated or I-maze, which uses a mix of real word distractors (generated via the A-maze process) and nonce word distractors [@vaniUsingInterpolatedMaze2021]. The presence of real word distractors encourages close attention to the sentential context, while nonce words can be used as distractors where the word in the sentence is itself ungrammatical or highly unexpected. @vaniUsingInterpolatedMaze2021 used I-maze to compare object- and subject- relative clauses and @wilcoxTargetedAssessmentIncremental2021 used it to compare differences in RT for pairs of grammatical and ungrammatical sentences NLP language models predictions.


## Frequency, length, and surprisal effects
One way of assessing how much a method localized RT effects is to look at how strongly properties of a word correlate with the RT of that word compared with RTs of downstream words later in the sentence. A couple of properties known to influence reading time are a word's length and overall frequency in a language, as longer and less common words take longer to process [@klieglLengthFrequencyPredictability2004]. Another measure is predictability, or how expected a word is given the context, with more predictable words being read faster [@raynerEffectsFrequencyPredictability2004]. A word can have low predictability for a number of reasons: it could be low frequency, semantically unexpected, the start of a low-frequency syntactic construction, or a word that disambiguates prior words to the less common parse. Many targeted effects of interest are essentially looking at specific features that contribute to how predictable or unpredictable a word is. This means that how good a method is at detecting and localizing effects of predictability is a key aspect of how useful it is as measure of incremental processing.
With al these factors potentially contributing to predictability, how is predictability as a whole measured? The typical method is to use language models that are trained on large corpora of language to predict what word comes next in a sentence. A variety of pre-trained models exist, that vary in their internals, but all of them will generate assessments of how predictable words are. Predictability is often represented in terms of bits of surprisal, which is the negative log probability of a word (1 bit of surprisal means a word is expected to occur half the time, 2 bits is 1/4 of the time etc). 
The functional form of the relationship between RTs from eye-tracking and SPR corpora and the predictability of the words is linear in terms of surprisal [@smithEffectWordPredictability2013; @wilcoxPredictivePowerNeural2020; @goodkindPredictivePowerWord2018; @lukeLimitsLexicalPrediction2016]. Due to spillover effects, this linear relationship also holds between the surprisal of a previous word and the RT on the current word. 

## Current experiment
The Maze task has thus far only been used on constructed sentences focusing on targeted effects and not on the sorts of long naturalistic passages used to assess the relationship between RT and surprisal. We want to test whether participants can read and understand stories while doing the demanding Maze task and also whether the RT profiles from the Maze task are similar to those from other methods.

The Natural Stories corpus [@futrellNaturalStoriesCorpus2020] consists of 10 passages each roughly 1000 words long which are designed to read fluently to native speakers. At the same time, the passages contain copious punctuation, quoted speech, many proper nouns, and low frequency grammatical constructions. Taken together, these properties are a severe test of A-maze, as these features may make it harder to auto-generate appropriate distractors and require focus from participants. If participants can succeed at the Maze task on Natural Stories, we think they are likely to succeed on a wide variety of naturalistic texts. The corpus is accompanied by binary-choice comprehension questions, 6 per story, which we use to assess comprehension.

We tweak the A-maze task to accomodate these longer passages and then have participants read the passages in the Maze. We compare our A-maze results with SPR data collected on this corpus by @futrellNaturalStoriesCorpus2020. We find that participants were able to read and understand these long passages using A-maze, and their RT profiles show a similar pattern to SPR but with less noise and spillover 

# Error-correction maze
<!-- GOOD -->
(ref:diagram-cap) Schematic of error-correction Maze. A participant reads a sentence word by word, choosing the correct word at each time point (selections marked in blue ovals). When they make a mistake, an error message is displayed, so they try again and continue with the sentence. 

```{r diagram, out.height="25%", fig.width=8, fig.height=3, fig.pos="ht", fig.cap="(ref:diagram-cap)"}
knitr::include_graphics(here("Papers/maze_diagram_cropped.pdf"))
```

One of the benefits of the Maze task is that it forces incremental processing by having participants make an active choice about what the next word is. But what if they choose incorrectly? In the traditional Maze paradigm, there isn't a way to handle this; a mistake indicates they're not understanding the sentence properly, so the sentence ends, and the participant moves on to the next item [@forsterMazeTaskMeasuring2009]. An advantage of this is that participants who contribute RT data are very likely to have understand the sentence up to that point (it's hard to choose correctly in G-maze otherwise). This contrasts with other methods, where determining whether participants are paying attention usually requires separate comprehension check questions, that are usually not used for Maze. 

However, terminating sentences on errors means that we don't have data after a participant makes a mistake in an item. In traditional G-maze tasks, with hand-crafted distractors and attentive participants, this is a small issue. However, this data loss is much worse with A-maze materials and crowd-sourced participants [@boyceMazeMadeEasy2020]. The high errors are likely from some combination of participants guessing randomly and from auto-generated distractors that in fact fit the sentence; as @boyceMazeMadeEasy2020 noted, some distractors, especially early in the sentence, were problematic and caused considerable data loss. 

This situation could be improved by auto-generating better distractors or hand-replacing problematic ones, but that does not solve the fundamental problem with long items. Well-chosen distractors and attentive participants will reduce the error rate, but the error rate will still compound over long materials. For instance, with a 1% error rate, `r round(.99**15*100)`%  of participants would complete each 15-word sentence, but only `r round(.99**50*100)`% would complete a 50 word vignette, and `r round(.99**200*100)`% would complete a 200 word passage.  In order to run longer materials, we need something to do when participants make a mistake, other than terminate the entire item.

To resolve this, we introduce an *error-correction* variant of Maze shown in Figure \@ref(fig:diagram). When a participant makes an error, we present them with an error message and wait until they select the correct option, before continuing the sentence as normal. We make this "error-correction" Maze available as an option in a modification of the Ibex Maze implementation introduced in @boyceMazeMadeEasy2020 (https://github.com/vboyce/Ibex-with-Maze). The code records both the RT to the first click and also the total RT until the correct answer is selected as separate values.

This variant of Maze expands the types of materials that can be used with Maze to include arbitrarily long passages and cushions the impact of occasional problematic distractors. Error-correction Maze is a change in experimental procedure, and is independent of what types of distractors are used. This error-correction presentation is used here with A-maze, but would also work with G-maze or I-maze. 

# Methods


```{r participants}

data <- read_rds(here("Data/cleaned.rds"))

data_filt <- data %>% 
  filter(native %in% c("ENG", "English", "ENGLISH", "english")) #I peeked at what people put that semantically maps to english

data_stories <- data_filt %>% 
  select(type, subject) %>% 
  unique() %>% 
  group_by(type) %>% 
  tally() %>% 
  filter(type!="practice")

data_error_summ <- data_filt %>% 
  mutate(correct.num=ifelse(correct=="yes", 1,0)) %>% 
  group_by(subject, num_correct) %>%
  filter(type!="practice") %>% 
  filter(rt<5000) %>% 
  summarize(mean_rt=mean(rt),
            pct_correct=mean(correct.num)) %>% 
  mutate(good_comp=ifelse(num_correct>4,"5 or 6 correct","4 or fewer correct")) %>% 
    mutate(accurate=ifelse(pct_correct>.8,">80% correct", "<80% correct"),
           is.attentive=ifelse(pct_correct>.8,1,0))

some <- data_filt %>% select(subject, num_correct) %>% 
  unique() %>% 
  left_join(data_error_summ, by=c("subject", "num_correct")) %>% 
  mutate(accurate=ifelse(pct_correct>.8,">80% correct", "<80% correct")) %>% 
  group_by(num_correct,accurate)

data_good <- data_filt %>% 
  left_join(data_error_summ, by=c("subject", "num_correct")) %>% 
  filter(type!="practice") %>% 
  filter(pct_correct>.8) %>% 
  mutate(is.correct=ifelse(correct=="yes",1,0))

data_non_error <- data_good %>% 
  filter(is.correct==1) %>% 
  filter(word_num>0)

data_before <- data_good %>% 
  mutate(word_num_mistake=ifelse(correct=="no", word_num,NA)) %>% 
  group_by(sentence, subject) %>% 
  fill(word_num_mistake) %>% ungroup() %>% 
  mutate(after_mistake=word_num-word_num_mistake,
         after_mistake=ifelse(is.na(after_mistake),0,after_mistake)) %>% 
  filter(after_mistake==0) %>% 
  filter(is.correct==1) %>% 
  filter(word_num>0)

data_sentence <- data_before <- data_good %>% 
  mutate(word_num_mistake=ifelse(correct=="no", word_num,NA)) %>% 
  group_by(sentence, subject) %>% 
  fill(word_num_mistake) %>% ungroup() %>% 
  mutate(after_mistake=word_num-word_num_mistake,
         after_mistake=ifelse(is.na(after_mistake),0,after_mistake)) %>% 
  group_by(sentence, subject) %>% 
  summarize(total_mistake=sum(after_mistake)) %>% 
  mutate(sent.correct=ifelse(total_mistake==0,1,0))
```

We constructed A-maze distractors for the Natural Stories corpus [@futrellNaturalStoriesCorpus2020] and recruited 100 crowd-sourced participants to each read a story in the Maze paradigm. 

## Materials
We used the texts from the Natural Stories corpus [@futrellNaturalStoriesCorpus2020] and generated A-maze distractors for them. We used the original comprehension questions provided in the Natural Stories corpus. To familiarize participants with the task, we wrote a short practice passage and corresponding comprehension questions. See the Appendix for an except of one of the stories and its corresponding comprehension questions. All materials are available at TODO NEW REPO. 

To generate distractors, we first split the corpora up into sentences, and then ran the sentences through the A-maze generation process. We used an updated version of the codebase from @boyceMazeMadeEasy2020. This newer version had the capability to match the greater variety of punctuation present in this corpus (updated auto-generation code at https://github.com/vboyce/Maze).  We took the auto-generated distractors as they were, without checking them for quality. 


## Participants
We recruited 100 participants from Amazon Mechanical Turk in April 2020, and paid each participant $3.50 for roughly 20 minutes of work. We excluded data from those who did not report English as their native language, leaving `r data_filt %>% select(subject) %>% unique() %>% nrow()` participants. 

## Procedure
Participants first gave their informed consent and saw task instructions. Then they read a short practice story in the Maze paradigm and answered 2 binary-choice practice comprehension questions, before reading the main story in the A-maze task. After the story, they answered the 6 main comprehension questions, commented on their experience, answered optional demographic questions, and were debriefed, before getting a code to enter for payment. The experiment was implemented in Ibex (https://github.com/addrummond/ibex) and the experimental code is available at REPO.  

## Models

```{r}
package_list <- c("tidyverse","brms","rstan","papaja",
                  "lme4", "mgcv", "tidybayes")
```

TODO top level description 

In order to measure the relationship between a word's properties and it's RT, we fit models using surprisal, frequency, and length as predictors of RT. We considered these predictors from both the current and past word to look for the possibility of spill over effects. We fit generalized additive models (GAMs) to look at the functional form of the surprisal-RT relationship and linear models (LMs) to look at the size of the effects. As described in the Results section, we base all models only on data from participants who choose correctly on at least 80% of the words in the Maze task. 


 TODO FIX ME We conducted data processing and analyses using `r cite_r("r-references.bib", withhold=F, pkgs=package_list)`.


```{r}
surps <- read_rds(here("Prep_code/natural_stories_surprisals.rds")) 

not_first <- surps %>% filter(Word_In_Sentence_Num!=1)

#nrow(not_first)
  

not_na <- not_first %>% filter(across(ends_with("surp"),~!is.na(.x)),
                               !is.na(freq))


not_ngram <- not_na %>%   filter(ngram_token_count==1)


#nrow(not_ngram)

not_other <- not_ngram %>% 
filter(txl_token_count==1)%>% 
  filter(grnn_token_count==1)
#nrow(not_other)

not_gpt <- not_other %>% filter(gpt_token_count==1)

#nrow(not_gpt)

```
### Predictors 
We created a set of predictor variables of frequency, word length, and surprisals from 4 language models.  For length, we used the length in characters excluding end punctuation. For unigram frequency, we tokenized the training data from @gulordava18 and tallied up instances. We then rescaled it to be the log2 frequency of expected occurrences in 1 billion words as the model predictor, so higher values indicate higher log frequencies. We got per-word surprisals for each of 4 different language models: a Kneser-Ney smoothed 5-gram, GRNN [@gulordava18], Transformer-XL [@daiTransformerXLAttentiveLanguage2019], and GPT-2 [@radfordLanguageModelsAre], using lm-zoo [@gauthierSyntaxGymOnlinePlatform2020]. These models cover a range of common architectures. We can infer robustness if models provide similar results, but also compare between models to look at how well each model fits human data. For all of these predictors, we consider both the predictor at the current word as well as lagged predictors from the previous word.

### Exclusions
We exclude the first word of every sentence because it had an x-x-x distractor, which leaves `r nrow(not_first)` words. We exclude words for which we don't have surprisal or frequency information, leaving `r nrow(not_na)` words. We additionally exclude words that any model treated as being composed of multiple tokens. While surprisals should be additive, the summing the surprisals over these tokens gives some unreasonable responses. For instance, in one story the word king!\' is given a surprisal of 64 by GRNN (context: The other birds gave out one by one and when the eagle saw this he thought, \'What is the use of flying any higher? This victory is in the bag and I am king!\'). To avoid these outliers we exclude all words that any model treated as multi-token, leaving `r nrow(not_gpt)` words. This primarily excludes words with punctuation. (While GPT-2 using byte-pair encoding that can split up words into multiple parts, excluding words it split up only excludes 30 words that were not already excluded by other models.)

We excluded outlier RTs that were <100 or >5000 ms (<100 is likely a recording error, >5000 is likely the participant getting distracted). We exclude words where mistakes occurred or which occurred after a mistake in the same sentence. We only analysed words where we had values for all predictors, which means that if the previous word was unknown to a model, this word will be excluded because we're missing values for a lagged predictor. 

### Model specification
For generalized additive models, we centered but did not rescale the length and frequency predictors, but left surprisal uncentered for interpretability. We used smooths for the surprisal terms and tensor effects for the frequency by length effects and interactions. To minimize the effect of repeated measures on confidence about inferred curve shapes, we collapse the reading times across subjects by modelling the mean RT for each word. TODO edit for bootstrapping

For linear models, we centered all predictors. We used full mixed effects, including by-subject slopes and a per-word-token random intercept [@barrRandomEffectsStructure2013]. We used weak priors (normal(1000,1000) for intercept, normal(0,500) for beta and sd, and lkj(1) for correlations). Models were run in brm [@burknerAdvancedBayesianMultilevel2018]. 

For model comparison, we took by-item averaged data to aid in fast model fitting.  We included frequency, length, and their interaction to all models. Then we fit models with either 1 or 2 sources of surprisal using lm and assessed the effect of adding the second surprisal source with an anova. We used predictors for the current and past word and centered all effects. TODO cite LM

## Self-paced reading comparison

In addition to the texts, @futrellNaturalStoriesCorpus2020 released reading time data from a SPR study they ran in 2011. They recruited 181 participants from Amazon Mechanical Turk, most of whom read 5 of the stories. After reading each story, each participant answered 6 binary-choice comprehension questions. As a comparison to our A-maze models, we run similar models on the SPR corpus on Natural Stories [@futrellNaturalStoriesCorpus2020]. 

For comparability, we analyse only the first story each participant read, and, in line with @futrellNaturalStoriesCorpus2020, exclude participants who got less than 5/6 of the comprehension questions correct. To account for spill over effects known to exist in SPR, we analyse predictors at the current word as well as the past 3 words for all models. 
For linear models, we centered all predictors. We were unable to fit the full mixed effects model. The best model we could fit had by-subject random intercept, uncorrelated by-subject random slopes for surprisal, length and frequency, and  a per-word-token random intercept, fit with lme4, as this structure did not fit reliably in brms.

## SPR-Maze correlation
We additionally compare how correlated the Maze and SPR results are to each other, in comparison to within-Maze and within-SPR correlations. For Maze, within each story, we randomly split subjects into two halves. Within each half, we calculate a per-word average for each word and then a per-sentence average RT across word averages. We calculate a within-Maze correlation between these two halves. To avoid differences due to dataset size, we downsample the SPR data choosing a number of participants equal to the number we have for Maze. We then use the same procedure on this subset as for Maze to get a within-SPR correlation. For between Maze-SPR correlation, we take the average correlation across each of the 4 pairs of Maze half and SPR half. 

# Results

## Reading stories in the Maze task 

(ref:error-cap) A. Correlation between a participant's accuracy on the Maze task (fraction of words selected correctly) and their average reaction time (in ms). Many participants (marked in green) chose the correct word >80% of the time; others (in red) appear to be randomly guessing and were excluded from further analysis. B. Performance on the comprehension questions. Participants with low accuracy also performed poorly on comprehension questions; Participants with >80% task accuracy tended to do well; their performance was roughly comparable to the performance of SPR participants from  @futrellNaturalStoriesCorpus2020 on their first stories. 

```{r errors, out.height="25%", fig.width=8, fig.height=3, fig.pos="ht", fig.cap="(ref:error-cap)"}
error_plot <- ggplot(data_error_summ, aes(x=pct_correct, y=mean_rt, color=accurate))+
  geom_point(size=1)+
  labs(x="Fraction words selected correctly",
       y="Mean Reaction Time (ms)")+
  coord_cartesian(xlim=c(0.4,1), ylim=c(0,1600), expand=F)+
    scale_color_manual(values=c(">80% correct"="darkgreen","<80% correct"="darkred"))+
  guides(color="none")


comp_comp <- read_rds(here("Analysis/comp.rds")) %>% 
  filter(source %in% c(">80% correct","<80% correct", "SPR 1st story")) 

comp_tally <- read_rds(here("Analysis/comp.rds")) %>% group_by(source) %>% filter(correct>4) %>% 
  summarize(good=round(sum(pct)*100))
comp_plot <- ggplot(comp_comp, aes(x=correct,y=pct,fill=source))+
  geom_col(position="dodge")+
  facet_grid(.~source)+
  labs(x="Comprehension questions correct (out of 6)", y="Percent participants")+
  scale_fill_manual(values=c(">80% correct"="darkgreen","<80% correct"="darkred",  "SPR 1st story"="grey30"))+
guides(fill="none")

error_plot+comp_plot+plot_annotation(tag_levels="A")

```

Many participants completed the Maze task with a high degree of accuracy and also answered the comprehension questions correctly. 

```{r}

data_long <- data_filt %>% filter(type!="practice") %>% filter(rt>5000)
data_no_prac <- data_filt %>% filter(type!="practice")
```

Participant accuracy reflects both how well participants can navigate the task and what quality the auto-generated distractors are. We calculated the per-word error rate for each participant and graphed it against their average reaction time. (To avoid biasing the average if a participant took a pause before returning to the task, RTs greater than 5 seconds were excluded; this excluded `r nrow(data_long)` words, or `r nrow(data_long)/nrow(data_no_prac)*100 %>% round(2)`% of trials.) As seen in Figure \@ref(fig:errors)A, one cluster of participants (marked in green) made relatively few errors, with some reaching 99% accuracy. This confirms that the distractors were generally appropriate and shows that some participants maintained focus on the task for the whole story. These careful participants took around 1 second for each word selection, which is much slower than in eye-tracking or SPR. Another cluster of participants (in red) sped through the task, seemingly clicking randomly. This bimodal distribution is likely due to the mix of workers on Mechanical Turk, as we did not use qualification cutoffs. 

Another check is whether participants comprehended the story. We counted how many of the binary-choice comprehension questions each participant got right (out of 6). As seen in Figure \@ref(fig:errors)B, most participants who were accurate on the task also did well on comprehension questions, while participants who were at chance on the Maze task were also at chance on the comprehension questions. Participants usually answered quickly (within 10 seconds), so we do not believe they were looking up the answers on the internet. We can't rule out that some participants may have been able to guess the answers without understanding the story. Nonetheless, this provides preliminary evidence that people can understand and remember details of stories they read during the Maze task. The comprehension question performance of accurate Maze participants is broadly similar to the performance of SPR participants from @futrellNaturalStoriesCorpus2020 on the first story read. Overall, `r comp_tally %>% filter(source=="all Maze") %>% pull(good)`% of Maze participants got 5 or 6 questions right (`r comp_tally %>% filter(source=="<80% correct") %>% pull(good)`% of low-accuracy participants and `r comp_tally %>% filter(source==">80% correct") %>% pull(good)`% of high-accuracy participants) compared to `r comp_tally %>% filter(source=="SPR") %>% pull(good)`% of all SPR reads and `r comp_tally %>% filter(source=="SPR 1st story") %>% pull(good)`% of 1st SPR reads. Note that these differences cannot be directly attributed to methods, as the participant populations differed. While both studies were conducted on Mturk, the quality of Mturk data has decreased from 2011 when the SPR was collected to 2020 when the A-maze was collected. 

We use task performance as our exclusion metric for A-maze because it is more fine-grained and only analyze data from participants with at least 80% accuracy (in the gap between high-performers and low-performers). For the SPR comparison, we follow @futrellNaturalStoriesCorpus2020's criteria and exclude participants who got less than 5 of the questions correct. 


 
## RT and surprisal

```{r}
subject_att <- data_error_summ %>% 
  select(subject, is.attentive)

data_low_error <- data_filt %>% 
  left_join(subject_att, by="subject") %>% 
  filter(is.attentive==1) %>% 
  filter(type!="practice")

  data_error_free <- data_low_error %>% 
    mutate(word_num_mistake=ifelse(correct=="no", word_num,NA)) %>% 
    group_by(sentence, subject) %>% fill(word_num_mistake) %>% ungroup() %>% 
    mutate(after_mistake=word_num-word_num_mistake,
           after_mistake=ifelse(is.na(after_mistake),0,after_mistake)) %>% 
    filter(correct=="yes") %>% 
    filter(!after_mistake %in% c(1,2))
  
data_no_first <- data_error_free %>% filter(word_num!=0)

data_ready <- data_no_first %>% filter(rt>100 & rt<5000) %>% 
  select(subject, word_num, word, rt, sentence, type)

data_post_only <- data_no_first %>% filter(rt>100 & rt<5000) %>% 
  filter(!after_mistake==0) %>% 
    select(subject, word_num, word, rt, sentence, type)

data_pre_error <- data_no_first %>% filter(rt>100 & rt<5000) %>% 
  filter(after_mistake==0) %>% 
  select(subject, word_num, word, rt, sentence, type)

data_stories <- data_ready %>% select(type, subject) %>% 
  unique() %>% 
  group_by(type) %>% 
  tally()

data_anything_goes <- data_filt %>% 
  filter(type!="practice") %>% 
  filter(correct=="yes") %>% 
    select(subject, word_num, word, rt, sentence, type)

```

TODO remind why we care so much about surprisal

We fitted generalized additive models to test whether the RTs from the Maze experiment showed a linear relationship with surprisal and whether the effect was limited the current word or had spilled over from the prior word. For these models, we only included data that occurred before any mistakes in the sentence; due to limits of model vocabulary, words with punctuation and some uncommon or proper nouns were excluded. (See Methods for details on exclusions.)

(ref:gam-cap) GAM predictions of reaction time (RT) as a function of either current word surprisal (top) or previous word surprisal (bottom). Density of data is shown along the x-axis. For each of the 4 language models used, there is a linear relationship between current word surprisal and RT (at least when there is enough data). The relationship between previous word surprisal and RT is much flatter. 

```{r gam, out.height="40%", fig.width=8, fig.height=3, fig.pos="ht", fig.cap="(ref:gam-cap)"}

gam1 <- read_rds(here("Analysis/models/gam_predictions.rds")) %>%  ggplot( aes(x=surprisal, y=rt, ymin=CI_lower, ymax=CI_upper))+
  geom_line()+
  geom_ribbon(alpha=.3)+
  #facet_grid(s~model, scales="free_y")+
  facet_grid(s~model)+
  coord_cartesian(xlim=c(0,28))+
  labs(x="Surprisal (bits)", y="Reaction Time (ms)")+theme(axis.ticks.x=element_blank(), axis.title.x=element_blank(), axis.text.x=element_blank(), plot.margin=margin(t=0,r=0,b=0,l=0,unit="pt"))

all_mean_data <-  read_rds(here("Analysis/models/meaned_maze.rds"))

dens2 <-   ggplot(all_mean_data, aes(x=value))+
  geom_density(fill="gray",)+
  facet_grid(.~model)+
  labs(x="Surprisal (bits)", y="")+
    coord_cartesian(xlim=c(0,28))+
  theme(axis.text.y = element_blank(), strip.text=element_blank(), axis.ticks.y =element_blank(), axis.title.y=element_blank(), panel.grid=element_blank(), plot.margin = unit(c(0, 0, 0, 0), "cm"))

p2 = cowplot::align_plots(gam1, dens2, align = "v", axis="lr")
plot_grid(p2[[1]], p2[[2]], nrow=2, rel_heights = c(1, .3))


```

The smooths for the current and previous words surprisals are shown in Figure \@ref(fig:gam). Note that for each of the models, high-surprisal words are rare, with much of the data for words between 0 and 15 bits of surprisal. All of the models show a roughly linear relationship between current word surprisal and RT, especially in the region with more data. All of the models show a flatter relationship between previous word surprisal and RT. This is a sign of localization as the previous word's surprisal is not affecting RT much. The linear relationship matches that found with other methodologies. 



Given that the GAM models show a roughly linear relationship, we fit mixed linear models to quantify the influence of surprisal, frequency and word length. 
We built linear models with surprisal, frequency, length and surprisal x length and frequency x length effects from the current and previous words as predictors. We centered, but did not rescale, the predictors. 




```{r pre-error}
summ <- read_rds(here("Analysis/models/brms_maze_summ.rds")) %>% pivot_wider(names_from="model", values_from=c(`Estimate`)) %>% 
  mutate(Term=factor(Term, 
                        levels=c("Intercept", 
                                 "surprisal",
                                 "len",
                                 "freq",
                                 "surprisal:len",
                                 "len:freq",
                                 "prev_surp",
                                 "prev_len",
                                 "prev_freq",
                                 "prev_surp:prev_len",
                                 "prev_len:prev_freq")
                                 )) %>% 
  arrange(Term) %>% 
  mutate(Term=c("Intercept", "Surprisal", "Length", "Frequency",
                  "Surp x Length", "Freq x Length", "Past Surprisal",
                  "Past Length", "Past Freq", "Past Surp x Length", "Past Freq x Length")) 

knitr::kable(summ, format="latex", position="ht",caption="Predictions from fitted Bayesian regression models. All terms were centered, but not rescaled. Units are in ms. Surprisal is per bit, length per character, and frequency per $log_2$ occurance per billion words. Interval is 2.5th quantile to 97.5th quantile of model draws.")
```



As we can see in Table \@ref(tab:pre-error), we find large effects of surprisal and length, but minimal effects of frequency. The lack of frequency effects is somewhat surprising, but consistent with @shainLargescaleStudyEffects2019. Notably the coefficients for the lagged terms are small relative to the effects of surprisal and length of the current word.  

As a last analysis, we checked which of our surprisal models had the best fit using a nested model comparison shown in Table \@ref(tab:maze-compare). We assess the benefits of adding each model's predictions as a second surprisal source to see which models pick up on information not contained in another model.  GPT-2 provides a lot of additional predictive value over each other model, GRNN provides a lot over 5-gram and TXL and a little complemetary information over GPT-2. TXL provides a lot over 5-gram, and 5-gram provides little over any model. Log likelihood, which is a measure of fit to data, shows  a similar hierarchy, as GPT-2 is better than GRNN is better than TXL is better than 5-gram.
```{r maze-compare}

read_rds(here("Analysis/maze_model_compare.rds")) %>% knitr::kable(format="latex", position="ht",caption="Results of model comparisons on Maze data. Each row shows the additional predictive value gained from adding that model to another model. F values and p values from anova tests between 1-surprisal-source and 2-source models are reported. We also report log likelihoods of models with only one surprisal source.")


```


## Comparison with SPR






(ref:spr-gam-cap) GAM predictions of reaction time (RT) for SPR data from @futrellNaturalStoriesCorpus2020 as a function of current word surprisal (top) or the surprisal of an earlier word, up to 3 words back. Density of data is shown along the x-axis.

```{r spr-gam, out.height="40%", fig.width=8, fig.height=4, fig.pos="ht", fig.cap="(ref:spr-gam-cap)"}
gam1 <- read_rds(here("Analysis/models/spr_gam_predictions.rds")) %>%  ggplot( aes(x=surprisal, y=rt, ymin=CI_lower, ymax=CI_upper))+
  geom_line()+
  geom_ribbon(alpha=.3)+
  facet_grid(s~model)+
  coord_cartesian(xlim=c(0,28))+
  labs(x="Surprisal (bits)", y="Reaction Time (ms)")+theme(axis.ticks.x=element_blank(), axis.title.x=element_blank(), axis.text.x=element_blank(), plot.margin=margin(t=0,r=0,b=0,l=0,unit="pt"))

 all_mean_spr <-  read_rds(here("Analysis/models/meaned_spr.rds"))


dens2 <-   ggplot(all_mean_spr, aes(x=value))+
  geom_density(fill="gray",)+
  facet_grid(.~model)+
  labs(x="Surprisal (bits)", y="")+
    coord_cartesian(xlim=c(0,28))+
  theme(axis.text.y = element_blank(), strip.text=element_blank(), axis.ticks.y =element_blank(), axis.title.y=element_blank(), panel.grid=element_blank(), plot.margin = unit(c(0, 0, 0, 0), "cm"))

p2 = cowplot::align_plots(gam1, dens2, align = "v", axis="lr")
plot_grid(p2[[1]], p2[[2]], nrow=2, rel_heights = c(1, .2))

```


```{r spr-table}

summ_spr <- read_rds(here("Analysis/models/lmer_spr_summ.rds")) %>% 
  pivot_wider(names_from="model", values_from=c(`Estimate`)) %>% 
  mutate(Term=factor(Term, 
                        levels=c("(Intercept)", 
                                 "surp_center",
                                 "length_center",
                                 "freq_center",
                                 "surp_center:length_center",
                                 "length_center:freq_center",
                                 "past_surp_center",
                                 "past_length_center",
                                 "past_freq_center",
                                 "past_surp_center:past_length_center",
                                 "past_length_center:past_freq_center",
                                 "past2_surp_center",
                                 "past2_length_center",
                                 "past2_freq_center",
                                 "past2_surp_center:past2_length_center",
                                 "past2_length_center:past2_freq_center",
                                 "past3_surp_center",
                                 "past3_length_center",
                                 "past3_freq_center",
                                 "past3_surp_center:past3_length_center",
                                 "past3_length_center:past3_freq_center"
                                 )
                                 )) %>% 
  arrange(Term) %>% 
  mutate(Term=c("Intercept", "Surprisal", "Length", "Frequency","Surp x Length", "Freq x Length",
                "Past Surprisal", "Past Length", "Past Freq", "Past Surp x Length", "Past Freq x Length",
                "2Past Surprisal", "2Past Length", "2Past Freq", "2Past Surp x Length", "2Past Freq x Length",
                "3Past Surprisal", "3Past Length", "3Past Freq", "3Past Surp x Length", "3Past Freq x Length")) 

knitr::kable(summ_spr, format="latex", position="ht",caption="Predictions from fitted regression models for SPR data. All terms were centered, but not rescaled. Units are in ms. Surprisal is per bit, length per character, and frequency per $log_2$ occurance per billion words. Uncertainty interval is +/- 1.97 standard error.")

```

As a comparison, we ran the same types of models on the Self-Paced Reading data collected by @futrellNaturalStoriesCorpus2020. As shown in Figure \@ref(fig:spr-gam), there is a roughly linear but fairly flat relationship between RT and surprisal. Note that the y-axis is fairly narrow, and so the predicted effect of changes in surprisal is fairly small. This is confirmed by linear models (see Table \@ref(tab:spr-table)). Surprisal and length effects are evident for the current word, and most models shows surprisal effects from the past word, but we do not see frequency effects. The effects are much smaller than for A-maze, even though this model accounts for spillover effects from more previous words. 

Similarly to the model comparison for Maze, we also conducted a model comparison on the SPR data shown in Table \@ref(tab:spr-compare).  Model comparisons show that GPT-2 and 5-gram models contain some value over each other model, which is less clear for TXL and GRNN. In terms of log likelihoods, we find that GPT-2 is better than 5-gram is better than GRNN is better than TXL, although differences are small. The relatively good fit of 5-gram models to SPR data compared with neural models matches results from @huSystematicAssessmentSyntactic2020 and @wilcoxPredictivePowerNeural2020.  This contrasts with the Maze results, where the 5-gram model has the worst fit and does not provide additional predictive value to the other models. 

```{r}
maze <- read_rds(here("Analysis/maze_model_compare.rds"))
spr <- read_rds(here("Analysis/spr_model_compare.rds"))
```

As an overall measure of fit to data, we calculate multiple R-squared for the single surprisal source models for both A-maze and SPR. The models predict A-maze better with R-squared values ranging from `r maze %>% filter(Model=="5-gram") %>% pull(r_squared)` for the 5-gram model to `r maze %>% filter(Model=="GPT-2") %>% pull(r_squared)` for GPT-2. Whereas for SPR, the R-squared values range from from `r spr %>% filter(Model=="5-gram") %>% pull(r_squared)` to `r spr %>% filter(Model=="GPT-2") %>% pull(r_squared)`. This suggests that the effect size differences are not due merely to the larger overall reading time for A-maze, but that instead A-maze is more sensitive to surprisal and length effects. 


```{r spr-compare}

read_rds(here("Analysis/spr_model_compare.rds")) %>% knitr::kable(format="latex", position="ht",caption="Results of model comparisons on SPR data. Each row shows the additional predictive value gained from adding that model to another model. F values and p values from anova tests are reported. We also report log likelihoods of models with only one surprisal source.")

```


```{r}
tokenization <- here("Data/SPR/all_stories.tok.txt") %>% 
  read_delim(delim="\t") %>% 
  mutate(Word_In_Story_Num=zone,
         Story_Num=item)

labs <- read_rds(here("Prep_code/natural_stories_surprisals.rds")) %>% 
  left_join(read_delim(here("Materials/natural_stories_sentences.tsv"), delim="\t")) %>% 
  select(word_num=Word_In_Sentence_Num, word=Word, sentence=Sentence, everything())

spr <- read_rds(here("Data/SPR/first.rds")) %>% left_join(tokenization) %>% left_join(labs) %>% select(WorkerId,rt,Story_Num,Sentence_Num, word_num)

maze <- read_rds(here("Data/maze_pre_error.rds")) %>% left_join(labs) %>% select(subject, rt, Story_Num, Sentence_Num, word_num)
```

```{r spr-split-half}
set.seed(42)
spr_subs <- spr %>% filter(!is.na(Story_Num)) %>% select(WorkerId, Story_Num) %>% unique() %>% sample_n(63) %>%  group_by(Story_Num) %>%
  mutate(half=ifelse(row_number() <= .5*n(),"spr1", "spr2"))
#180 participants


spr_split <- spr %>%
  inner_join(spr_subs) %>% 
  group_by(half,Story_Num, Sentence_Num, word_num) %>% summarize(spr=mean(rt)) %>% 
  pivot_wider(names_from=half, values_from=spr)
  
spr_sentence_avg <- spr_split%>% group_by(Story_Num, Sentence_Num) %>% 
  summarize(spr1=mean(spr1),
            spr2=mean(spr2))

spr_spr <- cor(spr_sentence_avg$spr1,spr_sentence_avg$spr2, use="complete.obs")
```

```{r maze-split-half}
set.seed(42)
maze_subs <- maze %>% select(subject, Story_Num) %>% unique() %>% sample_n(63)%>% group_by(Story_Num) %>%
  mutate(half=ifelse(row_number() <= .5*n(),"maze1", "maze2"))

maze_split <- maze %>% 
  left_join(maze_subs) %>% 
  group_by(half,Story_Num, Sentence_Num, word_num) %>% summarize(maze=mean(rt)) %>% 
  pivot_wider(names_from=half, values_from=maze)
  
maze_sentence_avg <- maze_split%>% group_by(Story_Num, Sentence_Num) %>% 
  summarize(maze1=mean(maze1),
            maze2=mean(maze2))

maze_maze <- cor(maze_sentence_avg$maze1,maze_sentence_avg$maze2, use="complete.obs")
```

```{r maze-spr}
summ_spr <- spr %>% inner_join(spr_subs)%>% group_by(Story_Num, Sentence_Num, word_num,half) %>% summarize(spr=mean(rt, na.rm=T)) %>% rename(spr_half=half) %>% filter(!is.na(spr))
  
summ <- maze %>% inner_join(maze_subs) %>% group_by(Story_Num, Sentence_Num, word_num, half) %>% summarize(maze=mean(rt, na.rm=T)) %>% rename(maze_half=half) %>% filter(!is.na(maze))%>% inner_join(summ_spr)

sentence_avg <- summ %>% group_by(Story_Num, Sentence_Num, maze_half, spr_half) %>% 
  summarize(maze=mean(maze),
            spr=mean(spr))

sentence_avg11 <- sentence_avg %>% filter(maze_half=="maze1" & spr_half=="spr1")
sentence_avg12 <- sentence_avg %>% filter(maze_half=="maze1" & spr_half=="spr2")
sentence_avg21 <- sentence_avg %>% filter(maze_half=="maze2" & spr_half=="spr1")
sentence_avg22 <- sentence_avg %>% filter(maze_half=="maze2" & spr_half=="spr2")

spr_maze <-   mean(c(cor(sentence_avg11$spr,sentence_avg11$maze, use="complete.obs"),
  cor(sentence_avg12$spr,sentence_avg12$maze, use="complete.obs"),
  cor(sentence_avg21$spr,sentence_avg21$maze, use="complete.obs"),
  cor(sentence_avg22$spr,sentence_avg22$maze, use="complete.obs")))
```

(ref:scatter) Correlation between SPR and Maze data. RTs were averaged across participants per word and then averaged together within each sentence. RTs in ms. 

```{r scatter, out.height="20%", fig.width=6, fig.height=2, fig.pos="ht", fig.cap="(ref:scatter)"}
spr_collapse <- spr %>% 
  group_by(Story_Num, Sentence_Num,word_num) %>% 
  summarize(spr=mean(rt, na.rm=T)) %>% 
  group_by(Story_Num, Sentence_Num) %>%
  summarize(spr=mean(spr))

both <- maze %>% 
  group_by(Story_Num, Sentence_Num, word_num) %>% 
  summarize(maze=mean(rt, na.rm=T)) %>% 
  group_by(Story_Num, Sentence_Num) %>% 
  summarize(maze=mean(maze)) %>% 
  inner_join(spr_collapse)
ggplot(both, aes(x=maze, y=spr))+geom_point(alpha=.5, size=1)+labs(x="A-Maze", y="SPR")+coord_fixed(ratio=1)
```

An additional comparison between SPR and Maze is how correlated their reading times are; that is, do the methods pick up on the same effects. If so, we would expect that the sentences that take longer to read in one method also take longer in the other. We calculate the average RT at the sentence level (see Methods for details). The correlation between Maze and SPR is `r round(spr_maze,2)`, compared to `r round(spr_spr,2)` within SPR and `r round(maze_maze,2)` within Maze. See Figure \@ref(fig:scatter) for a visual comparison of overall Maze versus SPR RTs. 



# Discussion


## maze works

## strong effects of suprisal, mostly localized

## better than SPR

## correlated w SPR, probably the same effects, just stronger & less noisy

## Why you care - broad applications that might want this method

## shill for error correction
We introduced a tweak to the paradigm for displaying Maze tasks that makes it useable for multi-sentence materials. In this error-correction Maze paradigm, participants read all the words because they can correct their mistakes and move on. We tested this method on the Natural Stories corpus, showing that, despite the oddness of the task, participants can read and understand a 1000 word story in this method. We additionally showed that the RTs generated in the Maze task show a linear relationship between the RT of a word and it's surprisal, but no relationship with the surprisal of the previous word. This provides additional evidence for the argument that the Maze task forces very incremental processing [@forsterMazeTaskMeasuring2009]. 

Actually the potential applicability and value is much broader, for any question where nailing down the locus on incremental processing difficulty is important, little spillover is very valuable. 

This extreme incrementality makes the Maze task a good target for any question that requires precisely determining the locus of incremental processing difficulty and thus benefits from the lack of spillover. 
It shows broadly the same effects as other methods; however the differences in scale of suprisal effects and the lack of frequency effects detected here are a reason to explore more. Comparisons between different processing on the same materials could be useful for identifying how task demands influence language processing [ex. @bartekSearchOnlineLocality2011]. 

While the error-correction paradigm is crucial to running long materials, it also provides some benefits even on shorter materials when using A-maze with variable participant populations. The error-correction compensates for some of the shortcomings of A-maze identified in @boyceMazeMadeEasy2020; poor distractors might still cause participants to make unavoidable mistakes, but they will still see the rest of the sentence, which may reduce frustration. One questions that remains is whether this post-mistake data is useable, which depends on whether RTs from a few words after a mistake show any differences from RTs before any mistakes. Whether post-mistake data is high-quality and trustworthy enough to be included in analyses is a  hard-to-assess question of potential interest. 

Even if post-mistake data is not analysed, it can be used to distinguish between errors due to inattentive participants or merely specific bad distractors early in the sentence. Researchers can calculate a per-word error rate for each participant; high per-word error rates are consistent with guessing, low error rates with errors clustered early the sentence are consistent with poor distractors. This per-word error rate metric measures participants task accuracy and provides a convenient and clear-cut way of controlling data quality after the fact. 

Error-correction Maze also starts to reduce perverse incentives from the desire to complete the task quickly. With traditional Maze, clicking randomly will likely lead to a mistake, which will cause a participant to skip ahead to the next sentence. With error-correction Maze, randomly jamming buttons takes more effort, but is still faster than doing the task. In discussing this work, we received the suggestion that one way to disincentivize random clicking is to add a pause when a participant makes a mistake, forcing them to wait some short period of time (ex 500ms or 1 sec) before being able to correct their mistake. This seems like a promising improvement that could be worth implementing and testing.

Between the distractor auto-generation process introduced in @boyceMazeMadeEasy2020 and the error-correction paradigm introduced here, the Maze paradigm is now easy to use on a wide range of materials. The ease of use, large effects, and forced incrementality make Maze a good complement to existing incremental processing methods. We encourage researchers to consider Maze as an option for doing incremental processing work. 
\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

# Appendix 
The beginning of one of the stories. This is the first 200 words of a 1000 word story.  

Tulip mania was a period in the Dutch Golden Age during which contract prices for bulbs of the recently introduced tulip reached extraordinarily high levels and then suddenly collapsed. At the peak of tulip mania in February sixteen thirty-seven, tulip contracts sold for more than ten times the annual income of a skilled craftsman. It is generally considered the first recorded economic bubble. The tulip, introduced to Europe in the mid sixteenth century from the Ottoman Empire, became very popular in the United Provinces, which we now know as the Netherlands. Tulip cultivation in the United Provinces is generally thought to have started in earnest around fifteen ninety-three, after the Flemish botanist Charles de l'Ecluse had taken up a post at the University of Leiden and established a botanical garden, which is famous as one of the oldest in the world. There, he planted his collection of tulip bulbs that the Emperor's ambassador sent to him from Turkey, which were able to tolerate the harsher conditions of the northern climate. It was shortly thereafter that the tulips began to grow in popularity. The flower rapidly became a coveted luxury item and a status symbol, and a profusion of varieties followed.

The first 2 out of the 6 comprehension questions.

When did tulip mania reach its peak? 1630's, 1730's

From which country did tulips come to Europe? Turkey, Egypt
