---
title             : "A-maze of Natural Stories: Texts are comprehensible during the Maze task"
shorttitle        : "A-maze of Natural Stories"

author: 
  - name          : "Veronica Boyce"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Jane Stanford Way, Building 420, Stanford University, Stanford, CA 94305"
    email         : "vboyce@stanford.edu"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Formal Analysis
      - Investigation
      - Software
      - Writing - Original Draft Preparation
      - Writing - Review & Editing
  - name          : "Roger Levy"
    affiliation   : "2"
    role:
      - Conceptualization
      - Formal Analysis
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Stanford University"
  - id            : "2"
    institution   : "Massachusetts Institute of Technology"
header-includes:
 - \usepackage{setspace}\singlespacing

authornote: |
  TODO
  
abstract: |
  We find support for the localization of reading time effects during the Maze task, as well as extending the range of materials Maze is suitable for.  How long it takes to read a word in a sentence is reflective of how hard it is to identify and integrate the word in the surrounding context. Techniques that slow down the reading process and localize the processing time for each word are useful to understanding the time course of language processing. A-maze is a new method for measuring incremental sentence processing that can localize slowdowns related to syntactic ambiguities. We adapt A-maze for use on longer passages and test it on the Natural Stories corpus. We find that people can comprehend what they read during the Maze task. Moreover, the  task yields useable reaction time data with word predictability effects that are linear in the surprisal of the current word, with little spillover effect from the surprisal of the previous word. This expands the types of effects that can be studied with A-maze, showing it to be a a versatile alternative to eye-tracking and self-paced reading.

keywords: TODO
wordcount: TODO

bibliography      : ["r-references.bib","refs.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(dev = "png", dev.args = list(type = "cairo-png"))
 library("papaja")
 library(here)
 library(tidyverse)
library(patchwork)
 library(brms)
 library(lme4)
 library(tidybayes)
 library(mgcv)
 library(cowplot)
 library(gridExtra)
library(broom.mixed)
library(tidymv)

r_refs("r-references.bib", append=F)
theme_set(theme_bw())
options(knitr.table.format = "pdf")
```
# Intro
It's remarkable how flexible we are when reading; while we do occasionally stumble when we read something unexpected, we often are able to read slightly unexpected things without a problem. However, these expectations shape how fast we read, even if we don't notice a stumble, unexpected words take longer to process as they force us to rebuild our burgeoning mental model of the sentence. Fortunately for fluent readers and unfortunately for studying language, this process of adjustment is very quick, which makes measures of reading time messy. 

Measures of online reading are one way to understand language and how the mind processes language. Many theories of language structure and language processing ground out in predictions about the difficulty of processing words [@bartekSearchOnlineLocality2011]. For instance, the subject v object relative debate includes theories that make fine-grained predictions about which word is how slow -- this needs localized methods to adjudicate it [@grodnerConsequencesSerialNature2005; @staubEyeMovementsProcessing2010; @traxlerProcessingSubjectObject2002]. Other theories such as noisy channel processing also need support from localized word-by-word results [@levyIntegratingSurprisalUncertaininput2011]. 

Incremental processing methods such as self-paced reading or eye-tracking measure how long someone spends looking at one word before moving on and use that as a proxy for how difficult word was in context. How unexpected a word is correlates with how long it takes to read it and move on [@raynerEffectsFrequencyPredictability2004; @klieglLengthFrequencyPredictability2004]. However, the two major methods of measuring incremental processing both suffer from imprecise localization. In eye-tracking, people read naturally which involves skipping words, jumping ahead and looking back, the dynamics of which make it hard to isolate effects [@raynerEffectsFrequencyPredictability2004; @levyEyeMovementEvidence2009; @frazierMakingCorrectingErrors1982; @raynerEyeMovementsReading1998]. Even when reading order is controlled in self-paced reading, readers may maintain ambiguities about what a word was or means until it is later resolved by context, so it may take multiple words for slowdowns to catch up with them [@macdonaldInteractionLexicalSyntactic1993; @koornneefUseVerbbasedImplicit2006]. 

One effect of this lack of localization is that reading time for a word is dependent not only on how unexpected a word is, but also how unexpected the previous word is. This is an indication of spillover from the previous word. It's well established for eye-tracking and SPR that RTs are roughly linear in terms of a word's surprisal (negative log probability) [@smithEffectWordPredictability2013; @wilcoxPredictivePowerNeural]. Due to spillover effects, on SPR and eye-tracking, there is also a positive linear relationship between the surprisal of a previous word and the RT on the current word -- this is an indication of lack of localization. In addition to suprisal predicting RT, word length and word's overall frequency are also often found to be predictive [@klieglLengthFrequencyPredictability2004].

An alternative method that seems to have superior localization is the Maze task, which adopts an unnatural way of reading to force incremental processing [@freedman85; @forsterMazeTaskMeasuring2009]. In the Maze task, participants see two words at a time, a correct word that continues the sentence, and a distractor which does not. Participants must choose the correct word, and their reaction time (RT) is the dependent measure. If participants make a mistake, the sentence discontinues. Theoretically, participants must fully integrate each word into the sentence in order to confidently select it. This idea is supported by studies finding strongly localized effects [@witzelComparisonsOnlineReading2012a]. 

The downside of Maze is that materials are effort-intensive to construct because of the need to select infelicitious words as distractors for each spot of each sentence; this may explain why the Maze task was not widely adopted. @boyceMazeMadeEasy2020 demonstrate a way to automatically generate Maze distractors by using NLP language models to find words that are high-surprisal in the context of the target sentence. The quality of these A-Maze distractors is not up to that of hand-generated distractors, but @boyceMazeMadeEasy2020 found that materials with A-maze distractors had similar results to the hand-generated distractors from @witzelComparisonsOnlineReading2012a. A-maze out performed a SPR control in detecting and localizing expected slowdown effects. @sloggettAmazeAnyOther2020 also found that A-maze and G-maze distractors yielded similar results on a disambiguation paradigm.

A-maze is a potentially powerful addition to the psycholinguists toolkit. However, like the other Maze tasks it has been limited in its application to single-sentence items probing minimal comparisons in constructed sentences. This limits its useful, as some important questions such as comparing human data to language models or studying discourse effects require processing times over multi-sentence passages. Therefore, it's important to expand the Maze task to these types of materials and verify that it finds comparable patterns to other methods. 

While the issue of needing to generate distractors for long passages is solved with A-maze, another problem with Maze remains. In particular, because Maze tasks discontinue after participants make mistakes, the farther into an item a word is, the fewer participants see it. This makes it hard to run long materials using Maze, and prevents Maze from being used on long text passages where SPR or eye-tracking could be used. We resolve this issue by introducing a new paradigm for running Maze tasks where participants correct their errors and continue reading rather than discontinuing after a mistake.

Using this tweak, we test A-maze on the passages from the Natural Stories corpus. The Natural Stories corpus [@futrellNaturalStoriesCorpus2020] consists of 10 passages each roughly 1000 words long which are designed to read fluently to native speakers. At the same time, the passages contain copious punctuation, quoted speech, many proper nouns, and low frequency grammatical constructions. Taken together, these properties make this a severe test of our process, as these features make it harder for the language model to choose good distractors and require focus from participants. If participants can succeed at the Maze task on this set of materials, we think they are likely to succeed on basically any naturalistic text. The corpus is accompanied by binary-choice comprehension questions, 6 per story, which we use to assess comprehension. @futrellNaturalStoriesCorpus2020 collected self-paced reading time data on this corpus, which we compare the A-maze results to. Participants were able to read and understand these long passages using A-maze with the error correction paradigm, and their RT profiles showed a linear relationship to the surprisal of the words. 

# Error-correction maze
<!-- GOOD -->
(ref:diagram-cap) Schematic of error-correction Maze. A participant reads a sentence word by word, choosing the correct word at each time point (selections marked in blue ovals). When they make a mistake, an error message is displayed, so they try again and continue with the sentence. 

```{r diagram, out.height="25%", fig.width=8, fig.height=3, fig.pos='t', fig.cap="(ref:diagram-cap)"}
knitr::include_graphics(paste0(here(),"/Papers/maze_diagram_cropped.pdf"))
```
One advantage of the Maze task is that it forces incremental processing and automatically excludes inattentive participants by terminating a sentence when a participant makes an error. Thus, only participants who have processed the sentence and are paying attention contribute data at critical regions later in the sentence. However, this means we don't have data after a participant makes a mistake in an item. In traditional G-maze tasks, with hand-crafted distractors and attentive participants, this is a small issue. However, this data loss is much worse with A-maze materials and crowd-sourced participants [@boyceMazeMadeEasy2020]. The high errors are likely from some combination of participants guessing randomly and from auto-generated distractors that in fact fit the sentence; as @boyceMazeMadeEasy2020 noted, some distractors, especially early in the sentence, were problematic and caused considerable data loss. 

This situation could be improved by auto-generating better distractors or hand-replacing problematic ones, but that does not solve the fundamental problem. Well-chosen distractors and attentive participants will reduce the error rate, but the error rate will still compound over long materials. For instance, with a 1% error rate, `r round(.99**15*100)`% would complete each 15-word sentence, but only `r round(.99**50*100)`% of participants would complete a 50 word vignette, and `r round(.99**200*100)`% a 200 word passages.  In order to run longer materials, we need something to do when participants make a mistake, other than terminate the entire item.

To resolve this, we introduce an *error-correction* variant of Maze shown in Figure \@ref(fig:diagram). When a participant makes an error, we present them with an error message and wait until they select the correct option, before continuing the sentence as normal. We make this "error-correction" Maze available as an option in a modification of the Ibex Maze implementation introduced in @boyceMazeMadeEasy2020 (https://github.com/vboyce/Ibex-with-Maze). The code records both the RT to the first click and also the total RT until the correct answer is selected as separate values.

This variant of Maze expands the types of materials that can be used with maze to include arbitrarily long passages and cushions the impact of occasional problematic distractors. 

# Methods


```{r participants}

data <- read_rds(paste0(here(),"/Data/cleaned.rds"))

data_filt <- data %>% 
  filter(native %in% c("ENG", "English", "ENGLISH", "english")) #I peeked at what people put that semantically maps to english

data_stories <- data_filt %>% 
  select(type, subject) %>% 
  unique() %>% 
  group_by(type) %>% 
  tally() %>% 
  filter(type!="practice")

data_error_summ <- data_filt %>% 
  mutate(correct.num=ifelse(correct=="yes", 1,0)) %>% 
  group_by(subject, num_correct) %>%
  filter(type!="practice") %>% 
  filter(rt<5000) %>% 
  summarize(mean_rt=mean(rt),
            pct_correct=mean(correct.num)) %>% 
  mutate(good_comp=ifelse(num_correct>4,"5 or 6 correct","4 or fewer correct")) %>% 
    mutate(accurate=ifelse(pct_correct>.8,">80% correct", "<80% correct"),
           is.attentive=ifelse(pct_correct>.8,1,0))

some <- data_filt %>% select(subject, num_correct) %>% 
  unique() %>% 
  left_join(data_error_summ, by=c("subject", "num_correct")) %>% 
  mutate(accurate=ifelse(pct_correct>.8,">80% correct", "<80% correct")) %>% 
  group_by(num_correct,accurate)

data_good <- data_filt %>% 
  left_join(data_error_summ, by=c("subject", "num_correct")) %>% 
  filter(type!="practice") %>% 
  filter(pct_correct>.8) %>% 
  mutate(is.correct=ifelse(correct=="yes",1,0))

data_non_error <- data_good %>% 
  filter(is.correct==1) %>% 
  filter(word_num>0)

data_before <- data_good %>% 
  mutate(word_num_mistake=ifelse(correct=="no", word_num,NA)) %>% 
  group_by(sentence, subject) %>% 
  fill(word_num_mistake) %>% ungroup() %>% 
  mutate(after_mistake=word_num-word_num_mistake,
         after_mistake=ifelse(is.na(after_mistake),0,after_mistake)) %>% 
  filter(after_mistake==0) %>% 
  filter(is.correct==1) %>% 
  filter(word_num>0)

data_sentence <- data_before <- data_good %>% 
  mutate(word_num_mistake=ifelse(correct=="no", word_num,NA)) %>% 
  group_by(sentence, subject) %>% 
  fill(word_num_mistake) %>% ungroup() %>% 
  mutate(after_mistake=word_num-word_num_mistake,
         after_mistake=ifelse(is.na(after_mistake),0,after_mistake)) %>% 
  group_by(sentence, subject) %>% 
  summarize(total_mistake=sum(after_mistake)) %>% 
  mutate(sent.correct=ifelse(total_mistake==0,1,0))
```

We constructed A-maze distractors for the Natural Stories corpus [@futrellNaturalStoriesCorpus2020] and recruited 100 crowd-sourced participants to each read a story in the Maze paradigm. 

## Materials
We took the texts of the Natural Stories corpus [@futrellNaturalStoriesCorpus2020], split them into sentences, and ran the sentences through the A-maze generation process.
We follow the A-maze generation process outlined in @boyceMazeMadeEasy2020, although we use an updated version of the codebase with fixes some issues identified in that paper (code at https://github.com/vboyce/Maze). Additionally, the capability to appropriately handle a wider variety of punctuation (needed for this corpus) was added. We took the auto-generated distractors as they were, without checking them for quality. 
We used the original comprehension questions provided in the Natural Stories corpus. To familiarize participants with the task, we wrote a short practice passage and corresponding comprehension questions. All materials are available at TODO NEW REPO. 

## Participants
We recruited 100 participants from Amazon Mechanical Turk, and paid each participant $3.50 for roughly 20 minutes of work. We excluded data from those who did not report English as their native language, leaving `r data_filt %>% select(subject) %>% unique() %>% nrow()` participants. 

## Procedure
Participants first gave their informed consent and saw task instructions. Then they read a short practice story in the Maze paradigm and answered 2 binary-choice practice comprehension questions, before reading the main story in the A-maze task. After the story, they answered the 6 main comprehension questions, commented on their experience, answered optional demographic questions, saw an debriefing and were given a code to enter for payment. The experiment was implemented in Ibex (https://github.com/addrummond/ibex) and the experimental code is available at REPO.  

## Models

```{r}
package_list <- c("tidyverse","brms","rstan","papaja",
                  "lme4", "mgcv", "tidybayes")
```


We conducted data processing and analyses using `r cite_r("r-references.bib", withhold=F, pkgs=package_list)`.

To model the relationship between RT and word surprisal, we created a set of predictor variables of frequency, word length, and surprisals from three language models.  For length, we used the length in characters excluding end punctuation. For unigram frequency, we tokenized the training data from @gulordava18 and tallied up instances. We then used the log2 frequency of the expected occurances in 1 billion words as the model predictor, so higher values indicate higher log frequencies. We got per-word surprisals for each of 3 different language models: a Kneser-Ney smoothed 5-gram, GRNN [@gulordava18], and Transformer-XL [@daiTransformerXLAttentiveLanguage2019].  For all of these predictors, we consider both the predictor at the current word as well as lagged predictors from the previous word.

We only included words that were a single token in each of the model vocabularies and for which we had frequency information. In practice, this excluded words with punctuation as well as uncommon or proper nouns. We also excluded the first word of every sentence (which had a dummy distractor). We excluded outlier RTs that were <100 or >5000 ms (<100 is likely a recording error, >5000 is likely the participant getting distracted). We exclude words where mistakes occurred or which occurred after a mistake in the same sentence.

For generalized additive models, we centered but did not rescale the length and frequency predictors, but left surprisal uncentered for interpretability. We used smooths for the surprisal terms and tensor effects for the frequency by length effects and interactions. 

For linear models, we centered all predictors. We used full mixed effects, including by-subject slopes and a per-word-token random intercept [@barrRandomEffectsStructure2013]. We used weak priors (normal(1000,1000) for intercept, normal(0,500) for beta and sd, and lkj(1) for correlations). Models were run in brm [@burknerAdvancedBayesianMultilevel2018]. 

For model comparison, we fit models with only frequency and length as predictors, as well as models that also had one or more sources of surprisal. We centered all effects. We included a random intercept for word identity, but no further random effects so the models would fit in lme4 [@batesFittingLinearMixedeffects2015]. 

## Self-paced reading comparison

In addition to the texts, @futrellNaturalStoriesCorpus2020 released reading time data from a SPR study they ran. They recruited 181 participants, most of whom read 5 of the stories for a total of 988 story reads, or close to a million RT measures. After reading each story, each participant answered 6 binary-choice comprehension questions. As a comparison to our A-maze models, we run similar models on the SPR corpus on Natural Stories [@futrellNaturalStoriesCorpus2020]. Data exclusions are the same as for A-maze; we do not make any exclusions based on participants comprehension accuracy. Generalized additive models are run with the same structure and predictors as for A-maze. Due to the size of the data, we were unable to run the full Bayesian mixed model; instead we estimate effect sizes uses a model with only random intercept for word identity fit in lmer, like that used for model comparison. We also run model comparison models similar to those described above. 







# Results

## Reading stories in the Maze task 

(ref:error-cap) A. Correlation between a participant's accuracy on the Maze task (fraction of words selected correctly) and their average reaction time (in ms). Many participants (marked in green) chose the correct word >80% of the time; others (in red) appear to be randomly guessing and were excluded from further analysis. B. Performance on the comprehension questions. Participants who had >80% task accuracy tended to do well; those who were at chance on the task were also at chance on the questions. 

```{r errors, out.height="25%", fig.width=8, fig.height=3, fig.pos='t', fig.cap="(ref:error-cap)"}
error_plot <- ggplot(data_error_summ, aes(x=pct_correct, y=mean_rt, color=accurate))+
  geom_point(size=1)+
  labs(x="Fraction words selected correctly",
       y="Mean Reaction Time (ms)")+
  coord_cartesian(xlim=c(0.4,1), ylim=c(0,1600), expand=F)+
    scale_color_manual(values=c(">80% correct"="darkgreen","<80% correct"="darkred"))+
  guides(color="none")

comp_plot <- ggplot(some, aes(x=num_correct,fill=accurate))+
  geom_bar(position="dodge")+
  facet_grid(.~accurate)+
  labs(x="Comprehension questions correct (out of 6)", y="Participants")+
  scale_fill_manual(values=c(">80% correct"="darkgreen","<80% correct"="darkred"))+
guides(fill="none")

error_plot+comp_plot+plot_annotation(tag_levels="A")

```

Many participants completed the Maze task with a high degree of accuracy and also answers the comprehension questions correctly. 

Participant accuracy reflects both how well participants can navigate the task and what quality the auto-generated distractors are. We calculated the per-word error rate for each participant and graphed it against their average reaction time. (To avoid biasing the average if a participant took a pause before returning to the task, RTs greater than 5 seconds were excluded.) As seen in Figure \@ref(fig:errors)A, one cluster of participants (marked in green) make relatively few errors, with some reaching 99% accuracy. This confirms that the distractors were generally appropriate and shows that some participants maintained focus on the task for the whole story. These careful participants took around 1 second for each word selection, which is much slower than other paradigms. Another cluster of participants (in red) sped through the task, seemingly clicking randomly. This bimodal distribution is likely due to the mix of workers on Mechanical Turk, as we did not use qualification cutoffs. 

Another check is whether participants comprehended the story. We counted how many of the binary-choice comprehension questions each participant got right (out of 6). As seen in Figure \@ref(fig:errors)B, most participants were were accurate on the task also did well on comprehension questions, while participants who were at chance on the Maze task were also at chance on the comprehension questions. Participants usually answered quickly (within 10 seconds), so we do not believe they were looking up the answers on the internet. We can't rule out that some participants may have been able to guess the answers without understanding the story. Nonetheless, this provides preliminary evidence that people can understand and remember details of stories they read during the Maze task. 

We use task performance as our exclusion metric and only analyze data from participants with at least 80% accuracy (in the gap between high-performers and low-performers). 


 
## RT and surprisal

```{r}
subject_att <- data_error_summ %>% 
  select(subject, is.attentive)

data_low_error <- data_filt %>% 
  left_join(subject_att, by="subject") %>% 
  filter(is.attentive==1) %>% 
  filter(type!="practice")

  data_error_free <- data_low_error %>% 
    mutate(word_num_mistake=ifelse(correct=="no", word_num,NA)) %>% 
    group_by(sentence, subject) %>% fill(word_num_mistake) %>% ungroup() %>% 
    mutate(after_mistake=word_num-word_num_mistake,
           after_mistake=ifelse(is.na(after_mistake),0,after_mistake)) %>% 
    filter(correct=="yes") %>% 
    filter(!after_mistake %in% c(1,2))
  
data_no_first <- data_error_free %>% filter(word_num!=0)

data_ready <- data_no_first %>% filter(rt>100 & rt<5000) %>% 
  select(subject, word_num, word, rt, sentence, type)

data_post_only <- data_no_first %>% filter(rt>100 & rt<5000) %>% 
  filter(!after_mistake==0) %>% 
    select(subject, word_num, word, rt, sentence, type)

data_pre_error <- data_no_first %>% filter(rt>100 & rt<5000) %>% 
  filter(after_mistake==0) %>% 
  select(subject, word_num, word, rt, sentence, type)

data_stories <- data_ready %>% select(type, subject) %>% 
  unique() %>% 
  group_by(type) %>% 
  tally()

data_anything_goes <- data_filt %>% 
  filter(type!="practice") %>% 
  filter(correct=="yes") %>% 
    select(subject, word_num, word, rt, sentence, type)

```


We fitted generalized additive models to test whether the RTs from the Maze experiment showed a linear relationship with surprisal and whether the effect was limited the current word or had spilled over from the prior word. For these models, we only included data that occurred before any mistakes in the sentence; due to limits of model vocabulary, words with punctuation and some uncommon or proper nouns were excluded. 

(ref:gam-cap) GAM predictions of reaction time (RT) as a function of either current word surprisal (top) or previous word surprisal (bottom). Density of data is shown along the x-axis. For each of the 3 language models used, there is a linear relationship between current word surprisal and RT (at least when there is enough data). There is no relationship between previous word surprisal and RT. 

```{r gam, out.height="40%", fig.width=8, fig.height=3, fig.pos='t', fig.cap="(ref:gam-cap)"}
labelled_pre_error <- read_rds(paste0(here(),"/Analysis/pre_error.rds"))

ngram_data <- labelled_pre_error %>% select(rt, surprisal=ngram_surp, prev_surp=past_ngram, 
                                            freq=freq_center, len=length_center,
                                            prev_freq=past_c_freq, prev_len=past_c_length
                                            ) %>% mutate(model="5-gram")

grnn_data <- labelled_pre_error %>% select(rt, surprisal=grnn_surp, prev_surp=past_grnn, 
                                            freq=freq_center, len=length_center,
                                            prev_freq=past_c_freq, prev_len=past_c_length
                                            )%>% mutate(model="GRNN")

txl_data <- labelled_pre_error %>% select(rt, surprisal=txl_surp, prev_surp=past_txl, 
                                            freq=freq_center, len=length_center,
                                            prev_freq=past_c_freq, prev_len=past_c_length
                                            ) %>% mutate(model="TXL")

all_data <- ngram_data %>% union(grnn_data) %>% union(txl_data) %>% 
  select(surprisal, prev_surp,model) %>% 
  pivot_longer(cols=`surprisal`:`prev_surp`) %>% 
  mutate(s=ifelse(name=="surprisal", "Current","Previous"))

all <-read_rds(paste0(here(),"/Analysis/gam_predictions.rds"))
gam1 <-  ggplot(all, aes(x=surprisal, y=rt, ymin=CI_lower, ymax=CI_upper))+
  geom_line()+
  geom_ribbon(alpha=.3)+
   #geom_density(data=all_data, aes(x=value), fill="gray",)+
  facet_grid(s~model)+
  coord_cartesian(ylim=c(700,1300), xlim=c(0,28))+
  labs(x="Surprisal (bits)", y="Reaction Time (ms)")+theme(axis.ticks.x=element_blank(), axis.title.x=element_blank(), axis.text.x=element_blank(), plot.margin=margin(t=0,r=0,b=0,l=0,unit="pt"))


dens2 <-   ggplot(all_data, aes(x=value))+
  geom_density(fill="gray",)+
  facet_grid(.~model)+
  labs(x="Surprisal (bits)", y="")+
  theme(strip.text.x=element_blank(),axis.text.y=element_blank(), panel.grid=element_blank(), axis.ticks.y=element_blank(), plot.margin = unit(c(0, 0, 0, 0), "cm"))


p2 = align_plots(gam1, dens2, align = "v", axis="lr")
plot_grid(p2[[1]], p2[[2]], nrow=2, rel_heights = c(1, .3))
ggsave("sample_plot.pdf")


```

The smooths for the current and previous words surprisals are shown in Figure \@ref(fig:gam). Note that for each of the models, high-surprisal words are rare, with much of the data for words between 0 and 15 bits of surprisal. All of the models show a roughly linear relationship between current word surprisal and RT, especially in the region with more data. All of the models show a flat relationship between previous word surprisal and RT. This is a sign of localization as the previous word's surprisal is not affecting RT, only the word's own suprisal is. The linear relationship matches that found with other methodologies. 



Given that the GAM models show a roughly linear relationship, we fit mixed linear models to quantify the influence of surprisal, frequency and word length. 
We built linear models with surprisal, frequency, length and surprisal x length and frequency x length effects from the current and previous words as predictors. 


```{r}

show_summary <- function(model){
  intervals <- gather_draws(model, `b_.*`, regex=T) %>% mean_qi()
  
  stats <- gather_draws(model, `b_.*`, regex=T) %>% 
    mutate(above_0=ifelse(.value>0, 1,0)) %>% 
    group_by(.variable) %>% 
    summarize(pct_above_0=mean(above_0)) %>% 
    mutate(`P` = signif(2*pmin(pct_above_0,1-pct_above_0), digits=2)) %>% 
    left_join(intervals, by=".variable") %>% 
    mutate(lower=round(.lower, digits=1),
           upper=round(.upper, digits=1),
           E=round(.value, digits=1),
           `Estimate`=str_c(E," [",lower,", ", upper,"]"),
           Term=str_sub(.variable, 3, -1),
           ) %>% 
    select(Term, `Estimate`)
  
  stats
}
```

```{r pre-error-prep, eval=F}
brm_txl_interact <- read_rds(paste0(here(),"/Analysis/brm_txl_interact.rds"))
brm_grnn_interact <- read_rds(paste0(here(),"/Analysis/brm_grnn_interact.rds"))
brm_ngram_interact <- read_rds(paste0(here(),"/Analysis/brm_ngram_interact.rds"))
a <- show_summary(brm_txl_interact) %>% mutate(model="TXL") %>% mutate(Term=str_replace(Term,"txl","surp"))
b <- show_summary(brm_grnn_interact) %>% mutate(model="GRNN") %>% mutate(Term=str_replace(Term,"grnn","surp"))
c <- show_summary(brm_ngram_interact) %>% mutate(model="5-gram") %>% mutate(Term=str_replace(Term,"ngram","surp"))

summ <- a %>% union(b) %>% union(c) %>% write_rds(here("Analysis/brms_summary.rds"))

```

```{r pre-error}
summ <- read_rds(here("Analysis/brms_summary.rds")) %>% pivot_wider(names_from="model", values_from=c(`Estimate`)) %>% 
  mutate(Term=factor(Term, 
                        levels=c("Intercept", 
                                 "surp_center",
                                 "length_center",
                                 "freq_center",
                                 "surp_center:length_center",
                                 "length_center:freq_center",
                                 "past_c_surp",
                                 "past_c_length",
                                 "past_c_freq",
                                 "past_c_surp:past_c_length",
                                 "past_c_length:past_c_freq")
                                 )) %>% 
  arrange(Term) %>% 
  mutate(Term=c("Intercept", "Surprisal", "Length", "Frequency",
                  "Surp x Length", "Freq x Length", "Past Surprisal",
                  "Past Length", "Past Freq", "Past Surp x Length", "Past Freq x Length")) %>% 
  select(Term, `5-gram`, `GRNN`, TXL) 

knitr::kable(summ, format="latex", position="t",caption="Predictions from fitted Bayesian regression models. All terms were centered, but not rescaled. Units are in ms. Surprisal is per bit, length per character, and frequency per $log_2$ occurance per billion words.")
```



As we can see in Table \@ref(tab:pre-error), we find large effects of surprisal and length, but minimal effects of frequency. These effects are larger than what is usually reported in other methods CITE, but this could be due to the overall slowness of the method. The lack of frequency effects is somewhat surprising, but consistent with @shainLargescaleStudyEffects2019. Notably the coefficients for the lagged terms are small relative to the effects of surprisal and length of the current word.  

As a last analysis, we checked which of our surprisal models had the best fit using a nested model comparison. 
We found that all surprisal sources provide predictive value over none, but that the information provided by the Ngram model does not provide additional value to a model that already has GRNN in it. TXL and GRNN appear to contain some complementary predictive value. 

```{r,include=F, eval=F}
d_lm <- labelled_pre_error %>% group_by(word, txl_center, ngram_center, grnn_center, freq_center, length_center,
                                        past_c_txl, past_c_ngram, past_c_grnn, past_c_freq, past_c_length, Word_ID) %>% 
  summarize(mean_rt=mean(rt))

no_surp <- lmer(mean_rt ~ freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

ngram_only <- lmer(mean_rt ~ ngram_center + past_c_ngram + freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

grnn_only <- lmer(mean_rt ~ grnn_center + past_c_grnn + freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

txl_only <- lmer(mean_rt ~ txl_center + past_c_txl + freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

all_surp <- lmer(mean_rt ~ ngram_center + past_c_ngram + 
                   grnn_center + past_c_grnn +
                   txl_center + past_c_txl +
                   freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)


ngram_grnn <-  lmer(mean_rt ~ ngram_center + past_c_ngram + 
                   grnn_center + past_c_grnn +
                   freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

ngram_txl <-  lmer(mean_rt ~ ngram_center + past_c_ngram + 
                   txl_center + past_c_txl +
                   freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

grnn_txl <-  lmer(mean_rt ~ grnn_center + past_c_grnn +
                   txl_center + past_c_txl +
                   freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

no_lag <-  lmer(mean_rt ~ ngram_center +
                   grnn_center +
                   txl_center + 
                   freq_center*length_center+(1|word), data=d_lm)

no_surp_lag <-  lmer(mean_rt ~ ngram_center +  
                   grnn_center + 
                   txl_center +                    freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d_lm)

  only_surp_lag <-  lmer(mean_rt ~ ngram_center + past_c_ngram + 
                   grnn_center + past_c_grnn +
                   txl_center + past_c_txl +
                   freq_center*length_center+(1|word), data=d_lm)
```

```{r, include=F, eval=F}
anova(all_surp, txl_only)
anova(all_surp, grnn_only)
anova(all_surp, ngram_only)

anova(txl_only, no_surp)
anova(grnn_only, no_surp)
anova(ngram_only, no_surp)

anova(all_surp, ngram_grnn)
anova(all_surp, ngram_txl)
anova(all_surp, grnn_txl)

anova(ngram_grnn, ngram_only)
anova(ngram_grnn, grnn_only)
anova(ngram_txl, ngram_only)
anova(ngram_txl, txl_only)
anova(grnn_txl, grnn_only)
anova(grnn_txl, txl_only)

anova(all_surp, no_lag)

anova(all_surp, no_surp_lag)

anova(no_surp_lag, no_lag) 

anova(all_surp, only_surp_lag)

anova(only_surp_lag, no_lag)
```

## Comparison with SPR

```{r read_data, eval=F}
d.raw.a <- here("Data/SPR/batch1_pro.csv") %>% 
  read_csv() 

d.raw.b <- here("Data/SPR/batch2_pro.csv") %>% 
  read_csv()



offset=230 # following note in repo, we have to correct alignment on item 3 b/c an empty token was displayed

d.raw <- d.raw.a %>% union(d.raw.b) %>% 
  mutate(zone=if_else(item == 3 & zone > offset, zone - 3, zone - 2))

# spot check that this matches number wise with their processed words

```

```{r surprisals, eval=F}

tokenization <- here("Data/SPR/all_stories.tok.txt") %>% 
  read_delim(delim="\t") %>% 
  mutate(Word_In_Story_Num=zone,
         Story_Num=item) %>%
  rename(Word=word)

#something is going on is story 2, sentence 4 His brother had blatantly peeked..." 
#they have it as "peaked", not sure what was displayed, but we can inner join
surprisals <- here("Data/SPR/natural_stories_surprisals2.rds") %>% 
  read_rds() %>% 
  inner_join(tokenization, by=c("Word_In_Story_Num", "Story_Num", "Word")) %>% 
  filter(ngram_token_count==1 & txl_token_count==1 & grnn_token_count==1) %>% 
  filter(!is.na(txl_surp) & !is.na(ngram_surp) & !is.na(grnn_surp) & !is.na(freq) & !is.na(length)) %>% 
    mutate(txl_center=txl_surp-mean(txl_surp, na.rm=T),
         ngram_center=ngram_surp-mean(ngram_surp, na.rm=T),
         grnn_center=grnn_surp-mean(grnn_surp, na.rm=T),
         freq_center=freq-mean(freq, na.rm=T),
         length_center=length-mean(length, na.rm=T))

past_word <- surprisals %>% 
  mutate(Word_In_Sentence_Num=Word_In_Sentence_Num+1) %>% 
  rename(past_txl=txl_surp, past_ngram=ngram_surp, past_grnn=grnn_surp, past_freq=freq, past_length=length,past_c_txl=txl_center, past_c_ngram=ngram_center, past_c_grnn=grnn_center, past_c_freq=freq_center, past_c_length=length_center) %>% 
  select(Story_Num,Sentence_Num, Word_In_Sentence_Num, starts_with("past")) %>% 
  inner_join(surprisals, by=c("Story_Num","Sentence_Num", "Word_In_Sentence_Num"))

```

```{r filter, eval=F}

d.filter <- d.raw %>% 
  inner_join(past_word, by=c("zone", "item")) %>% 
  filter(RT>100 & RT<5000) %>% 
  filter(Word_In_Sentence_Num!=0)


#subset for testing
d.subset <- d.filter #%>% filter(Story_Num==1)
```


```{r pre_gam, eval=F}
ngram_data <- d.subset %>% select(rt=RT, surprisal=ngram_surp, prev_surp=past_ngram, 
                                            freq=freq_center, len=length_center,
                                            prev_freq=past_c_freq, prev_len=past_c_length
                                            ) %>% mutate(model="5-gram")

grnn_data <- d.subset %>% select(rt=RT, surprisal=grnn_surp, prev_surp=past_grnn, 
                                            freq=freq_center, len=length_center,
                                            prev_freq=past_c_freq, prev_len=past_c_length
                                            )%>% mutate(model="GRNN")

txl_data <- d.subset %>% select(rt=RT, surprisal=txl_surp, prev_surp=past_txl, 
                                            freq=freq_center, len=length_center,
                                            prev_freq=past_c_freq, prev_len=past_c_length
                                            ) %>% mutate(model="TXL")

all_data <- ngram_data %>% union(grnn_data) %>% union(txl_data) %>% 
  select(surprisal, prev_surp,model) %>% 
  pivot_longer(cols=`surprisal`:`prev_surp`) %>% 
  mutate(s=ifelse(name=="surprisal", "Current","Previous"))

```

```{r gams, eval=F}
gam_ngram_1 <- gam(rt ~ s(surprisal, bs="cr", k=20)+ te(freq, len, bs="cr")+s(prev_surp, bs="cr", k=20)+te(prev_freq, prev_len, bs="cr"), data=ngram_data, method="REML")

gam_grnn_1 <- gam(rt ~ s(surprisal, bs="cr", k=20)+ te(freq, len, bs="cr")+s(prev_surp, bs="cr", k=20)+te(prev_freq, prev_len, bs="cr"), data=grnn_data, method="REML")

gam_txl_1 <- gam(rt ~ s(surprisal, bs="cr", k=20)+ te(freq, len, bs="cr")+s(prev_surp, bs="cr", k=20)+te(prev_freq, prev_len, bs="cr"), data=txl_data, method="REML")

```

```{r predict-gam, eval=F}


a <- get_gam_predictions(model=gam_ngram_1, series=surprisal, series_length=100) %>% select(surprisal, rt, CI_upper, CI_lower) %>% unique() %>% mutate(model="5-gram", s="Current")
b <- get_gam_predictions(model=gam_ngram_1, series=prev_surp, series_length=100) %>% select(surprisal=prev_surp, rt, CI_upper, CI_lower) %>% unique() %>% mutate(model="5-gram", s="Previous")
c <- get_gam_predictions(model=gam_grnn_1, series=surprisal, series_length=100) %>% select(surprisal, rt, CI_upper, CI_lower) %>% unique() %>% mutate(model="GRNN", s="Current")
d <- get_gam_predictions(model=gam_grnn_1, series=prev_surp, series_length=100) %>% select(surprisal=prev_surp, rt, CI_upper, CI_lower) %>% unique() %>% mutate(model="GRNN", s="Previous")
e <- get_gam_predictions(model=gam_txl_1, series=surprisal, series_length=100) %>% select(surprisal, rt, CI_upper, CI_lower) %>% unique() %>% mutate(model="TXL", s="Current")
f <- get_gam_predictions(model=gam_txl_1, series=prev_surp, series_length=100) %>% select(surprisal=prev_surp, rt, CI_upper, CI_lower) %>% unique() %>% mutate(model="TXL", s="Previous")
all <- a %>% union(b) %>% union(c) %>% union(d) %>% union(e) %>% union(f) %>% write_rds("SPR_gam_predictions.rds")

```

```{r prep-brms, eval=F}
d.for.lm <- d.subset %>%
  mutate(Word_ID=as_factor(str_c(Story_Num, Word_In_Story_Num, sep="_"))) %>% 
  mutate(subject=factor(WorkerId),
         rt=RT) 
```

```{r, eval=F}
#mean by subject for each word
d.byword <- d.for.lm %>% rename(word=Word) %>% 
  group_by(word, txl_center, ngram_center, grnn_center, freq_center, length_center,
                                        past_c_txl, past_c_ngram, past_c_grnn, past_c_freq, past_c_length, Word_ID) %>% 
  summarize(mean_rt=mean(rt))
```

```{r, eval=F}

ngram_only <- lmer(mean_rt ~ ngram_center + past_c_ngram + freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d.byword)

grnn_only <- lmer(mean_rt ~ grnn_center + past_c_grnn + freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d.byword)

txl_only <- lmer(mean_rt ~ txl_center + past_c_txl + freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d.byword)

```




(ref:spr-gam-cap) GAM predictions of reaction time (RT) for SPR data from @futrellNaturalStoriesCorpus2020 as a function of either current word surprisal (top) or previous word surprisal (bottom). Density of data is shown along the x-axis. 

```{r spr-gam, out.height="40%", fig.width=8, fig.height=3, fig.pos='t', fig.cap="(ref:spr-gam-cap)"}
all <- read_rds(here("Analysis/SPR_gam_predictions.rds"))

gam1 <-  ggplot(all, aes(x=surprisal, y=rt, ymin=CI_lower, ymax=CI_upper))+
  geom_line()+
  geom_ribbon(alpha=.3)+
  facet_grid(s~model)+
  coord_cartesian(ylim=c(300,400), xlim=c(0,28))+
  labs(x="Surprisal (bits)", y="Reaction Time (ms)")+
  theme(axis.ticks.x=element_blank(), 
        axis.title.x=element_blank(), 
        axis.text.x=element_blank(), 
        plot.margin=margin(t=0,r=0,b=0,l=0,unit="pt"))


dens2 <-   ggplot(all_data, aes(x=value))+
  geom_density(fill="gray",)+
  facet_grid(.~model)+
  labs(x="Surprisal (bits)", y="")+
  theme(strip.text.x=element_blank(),
        axis.text.y=element_blank(),
        panel.grid=element_blank(),
        axis.ticks.y=element_blank(), 
        plot.margin = unit(c(0, 0, 0, 0), "cm"))


p2 = align_plots(gam1, dens2, align = "v", axis="lr")
plot_grid(p2[[1]], p2[[2]], nrow=2, rel_heights = c(1, .3))

```
```{r spr-prep, eval=F}
ngram_only_tidy <- tidy(ngram_only, effects="fixed") %>% mutate(type="5-gram") %>% 
  mutate(term=str_replace(term, "ngram", "surp"))

txl_only_tidy <- tidy(txl_only, effects="fixed") %>% mutate(type="TXL") %>% 
   mutate(term=str_replace(term, "txl", "surp"))

grnn_only_tidy <- tidy(grnn_only, effects="fixed") %>% mutate(type="GRNN") %>% 
   mutate(term=str_replace(term, "grnn", "surp"))

all_tidy <- ngram_only_tidy %>% union(txl_only_tidy) %>% union(grnn_only_tidy) %>% write_rds(here("Analysis/SPR_lmer_summ.rds"))
```
```{r spr-table}

all_tidy <- read_rds(here("Analysis/SPR_lmer_summ.rds")) %>% 
  select(Term=term, estimate, std.error, type) %>% 
  mutate(Ci_high=estimate+1.96*std.error,
         Ci_low=estimate-1.96*std.error,
         Range=str_c(round(estimate,1)," [",round(Ci_low, 1),", ",round(Ci_high,1),"]")) %>% 
  select(Term, type, Range) %>% 
  pivot_wider(id_cols=Term, names_from=type, values_from=Range) %>% 
    mutate(Term=factor(Term, 
                        levels=c("(Intercept)", 
                                 "surp_center",
                                 "length_center",
                                 "freq_center",
                                 "freq_center:length_center",
                                 "past_c_surp",
                                 "past_c_length",
                                 "past_c_freq",
                                 "past_c_freq:past_c_length")
                                 )) %>% 
  arrange(Term) %>% 
  mutate(Term=c("Intercept", "Surprisal", "Length", "Frequency",
                   "Freq x Length", "Past Surprisal",
                  "Past Length", "Past Freq", "Past Freq x Length"))

knitr::kable(all_tidy, format="latex", position="t",caption="Predictions from fitted linear regression models. All terms were centered, but not rescaled. Units are in ms. Surprisal is per bit, length per character, and frequency per $log_2$ occurance per billion words.")
```

As a comparison, we ran the same types of models on the Self-Paced Reading data collected by @futrellNaturalStoriesCorpus2020. As shown in Figure \@ref(fig:spr-gam), there is a roughly linear but fairly flat relationship between RT and surprisal. Note that the y-axis is fairly narrow, and so the predicted effect of changes in surprisal is fairly small. This is confirmed by linear models (see Table \@ref(tab:spr-table)); note that due to the size of the data, these models were fit differently than the A-maze models due to computational constraints. Surprisal and length effects are evident for both the current and past word, but we do not see frequency effects. The effects are much smaller than for A-maze, although some of this may be due to noise from spillover from prior words or differences in model structure. 

Lastly, we are interested to know which of the models is better at predicting human behavior data, and whether they are complementary. I fit a series of models including the frequency and length predictors, and some number of past or present surprisal sources. This allows us to tell whether different models of surprisal are "picking up" on different things that affect reading time. 

We ran nested model comparison on the SPR data as well, and found that all sources of surprisal have predictive value, but the GRNN model provides the most benefit. Adding other models to GRNN is marginally beneficial, but TXL and N-gram together are more predictive than either alone. 

```{r, eval=F}

no_surp <- lmer(mean_rt ~ freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d.byword)

ngram_only <- lmer(mean_rt ~ ngram_center + past_c_ngram + freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d.byword)

grnn_only <- lmer(mean_rt ~ grnn_center + past_c_grnn + freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d.byword)

txl_only <- lmer(mean_rt ~ txl_center + past_c_txl + freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d.byword)

all_surp <- lmer(mean_rt ~ ngram_center + past_c_ngram + 
                   grnn_center + past_c_grnn +
                   txl_center + past_c_txl +
                   freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d.byword)


ngram_grnn <-  lmer(mean_rt ~ ngram_center + past_c_ngram + 
                   grnn_center + past_c_grnn +
                   freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d.byword)

ngram_txl <-  lmer(mean_rt ~ ngram_center + past_c_ngram + 
                   txl_center + past_c_txl +
                   freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d.byword)

grnn_txl <-  lmer(mean_rt ~ grnn_center + past_c_grnn +
                   txl_center + past_c_txl +
                   freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d.byword)

no_lag <-  lmer(mean_rt ~ ngram_center +
                   grnn_center +
                   txl_center + 
                   freq_center*length_center+(1|word), data=d.byword)

no_surp_lag <-  lmer(mean_rt ~ ngram_center +  
                   grnn_center + 
                   txl_center +                    freq_center*length_center+past_c_freq*past_c_length+(1|word), data=d.byword)

  only_surp_lag <-  lmer(mean_rt ~ ngram_center + past_c_ngram + 
                   grnn_center + past_c_grnn +
                   txl_center + past_c_txl +
                   freq_center*length_center+(1|word), data=d.byword)
```

```{r, eval=F}
anova(txl_only, no_surp)
anova(grnn_only, no_surp)
anova(ngram_only, no_surp)

anova(all_surp, ngram_grnn)
anova(all_surp, ngram_txl)
anova(all_surp, grnn_txl)

anova(ngram_grnn, ngram_only)
anova(ngram_grnn, grnn_only)
anova(ngram_txl, ngram_only)
anova(ngram_txl, txl_only)
anova(grnn_txl, grnn_only)
anova(grnn_txl, txl_only)
```

# Discussion

We introduced a tweak to the paradigm for displaying Maze tasks that makes it useable for multi-sentence materials. In this error-correction Maze paradigm, participants read all the words because they can correct their mistakes and move on. We tested this method on the Natural Stories corpus, showing that, despite the oddness of the task, participants can read and understand a 1000 word story in this method. We additionally showed that the RTs generated in the Maze task show a linear relationship between the RT of a word and it's surprisal, but no relationship with the surprisal of the previous word. This provides additional evidence for the argument that the Maze task forces very incremental processing [@forsterMazeTaskMeasuring2009]. 

This extreme incrementality may make the Maze task a good target for comparisons between human RTs and predictions of surprisal from neural language models because spillover effects would not need to be considered as much [@wilcoxPredictivePowerNeural]. 
It shows broadly the same effects as other methods; however the differences in scale of suprisal effects and the lack of frequency effects detected here are a reason to explore more. Comparisons between different processing on the same materials could be useful for identifying how task demands influence language processing [ex. @bartekSearchOnlineLocality2011]. 

While the error-correction paradigm is crucial to running long materials, it also provides some benefits even on shorter materials when using A-maze with variable participant populations. The error-correction compensates for some of the shortcomings of A-Maze identified in @boyceMazeMadeEasy2020; poor distractors might still cause participants to make unavoidable mistakes, but they will still see the rest of the sentence, which may reduce frustration. Whether this also solves the data loss problem from the researcher perspective within a sentence depends on whether post-mistake data are high-quality and trustworthy; this is a hard-to-assess question of potential interest. 

Even if post-mistake data is not analysed, it can be used to distinguish between errors due to inattentive participants or merely specific bad distractors early in the sentence. Researchers can calculate a per-word error rate for each participant; high per-word error rates are consistent with guessing, low error rates with errors clustered early the sentence are consistent with poor distractors. This per-word error rate metric measures participants task accuracy and provides a convenient and clear-cut way of controlling data quality after the fact. 

Error-correction Maze also starts to reduce perverse incentives from the desire to complete the task quickly. With traditional Maze, clicking randomly will likely lead to a mistake, which will cause a participant to skip ahead to the next sentence. With error-correction Maze, randomly jamming buttons takes more effort, but is still faster than doing the task. In discussing this work, we received the suggestion that one way to disincentivize random clicking is to add a pause when a participant makes a mistake, forcing them to wait some short period of time (ex 500ms or 1 sec) before being able to correct their mistake. This seems like a promising improvement that could be worth implementing and testing.

Between the distractor auto-generation process introduced in @boyceMazeMadeEasy2020 and the error-correction paradigm introduced here, the Maze paradigm is now easy to use on a wide range of materials. The ease of use, large effects, and forced incrementality make Maze a good complement to existing incremental processing methods. We encourage researchers to consider Maze as an option for doing incremental processing work. 
\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
