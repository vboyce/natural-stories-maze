---
title             : "A-maze of Natural Stories: Texts are comprehensible during the Maze task"
shorttitle        : "A-maze of Natural Stories"

author: 
  - name          : "Veronica Boyce"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "TODO"
    email         : "vboyce@stanford.edu"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Formal Analysis
      - Investigation
      - Software
      - Writing - Original Draft Preparation
      - Writing - Review & Editing
  - name          : "Roger Levy"
    affiliation   : "2"
    role:
      - Conceptualization
      - Formal Analysis
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Stanford University"
  - id            : "2"
    institution   : "Massachusetts Institute of Technology"

authornote: |
  TODO
  
abstract: |
  Easy to use, reliable methods are important to science. In studying how people understand language in real time, we use incremental processing methods to measure word-by-word reading time. However, neither of the common methods (eye-tracking and self-paced reading) can run over the web and produce localized effects. Another method, called the Maze task, seems to be able to do this, especially since a technique called A-maze makes the task much faster to construct. Due to task limitations, Maze has not been used on long, naturalistic passages, only short targeted materials. Here we present an adaptation of the Maze method suitable for long materials; we test this method on the Natural Stories corpus and find that participants can comprehend what they read while doing the task and that the reading time patterns are broadly similar to those found with other methods. We find support for the localization of reading time effects during the Maze task, as well as extending the range of materials Maze is suitable for. 
  
keywords          : "TODO"
wordcount         : "X"

bibliography      : ["r-references.bib","refs.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```

```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```

A diversity of robust methods provides for converging evidence from different methods (either within or between types), and choices allow for a good fit to the scope and goals of research. But we also want these methods to be reliable. Some of that is theoretical, but we also want empirical evidence that what the method captures is relevant to what we want to study, and that this connection is true across as broad a range of materials as we hope to use it for. Robustness is important to establishing how good a new method is. What range of tasks can this new tool accomplish and how well? (Could really extend on a toolbox analogy.)

When we hear or read language, we incorporate new words into our model of the unfolding context as we go. To understand this real-time process and how it copes with unexpected words, psycholinguists use incremental processing methods. This set of methods involves measuring how long participants choose to spend reading and integrating a word before moving on, although the tasks and measures of reading/reaction time differ.

The two most commonly used methods are eye-tracking and self-paced reading. In eye-tracking, participants read text freely from a screen, while the eye movements and fixations are tracked with an infrared camera CITE, check for truth. Eye-tracking is naturalistic and yields fine-grained data about how long participants spent on each word when, but it's expensive to run requiring both specialized equipment and researcher time to run experiments. 

Self-paced reading (commonly abbreviated SPR) in contrast can be run online with no specialzed equipment or direct researcher supervision, and so it's possible to collect data quickly and cheaply. In SPR, participants see one word of the text on screen at a time and press a button to contine to the next word instead; the time between button presses when a word was on screen is used as the reading time. The problem with SPR, especially when run online, is that effects from one word tend to show up as longer reading times spread out over several later words, in what is known as spillover effects. Both of these methods suffer somewhat from analytic flexibilty as processing difficulty may manifest in several ways in eye-tracking and is hard to localize in SPR.

For some experimental paradigms, it would be idea to have a readily useable method that localizes processing effects in a straightforward to analyse manner. This may seem like too much to hope for, but the Maze task appears to fill this gap. 

In the Maze task, participants see two words at a time, one of which continues the sentence, and one of which does not, and they have to choose which word is the correct continuation. The time to choose the correct word is taken as the dependent measure and interpreted as representing the difficulty of integrating the word into the sentence so far. If participants make a mistake, the sentence discontinues. Because of the required choices, the Maze task is claimed to  force extremely incremental processing, where participants must fully integrate each word into the sentence in order to confidently select it and move on. Experiments using  The actually prohibitive fine print on the Maze task is the need for distractors; while cheap and easy to run, traditional Maze materials are effort-intensive to construct, which may have kept this method from widespread adoption. 

However, a solution to the issue of generating Maze materials has been proposed. An alternative version of the Maze task, dubbed A-maze, was recently introduced that gets around this problem by automatically generating distrators using a program and neural nets (Boyce et al, 2020). While the quality of the automatically generated distractors is not quite up that of hand-generated (as judged by their being occassional distractors that seem completely acceptable continuations), the results from A-maze match those of G-maze. Boyce et al provide an interface for running the Maze task in a browser and a way of generating distractors that are usually, although not always, sufficiently bad. 

In a comparison between these A-maze materials and traditional hand-crafted distractors (from Witzel 2012), they find that A-maze yields comparable results to G-maze (either lab or web), and in one condition recovers effects that G-maze did not. A-maze also outperforms self-paced reading on this syntactic disambiguation paradigm. Sloggett (cite) also directly compared A-maze and G-maze on a disambiguaton paradigm and similarly found that A-maze and G-maze yielded similar results. 

These demonstrations make A-maze seem quite promising, as it has the advantages of the traditional Maze task, without it's drawback. This method seems worth exploring further. 

One evident shortcoming of the Maze task is that it has been used on only a limited set of materials. Some of this is due to the fact that it isn't used as widely as other incremental processing methods, and some is due to inherent limitations of the paradigm. In particular, because Maze tasks discontinue after participants make mistakes, the farther into an item a word is, the fewer participants see it. This makes it hard to run long materials using Maze, and prevents Maze from being used on long text passages where SPR or eye-tracking could be used. 

We find a way around this problem by creating a version of the Maze task interface where participants correct their mistakes and continue with the sentence rather than being discontinued. This allows for multi-sentence items to be run using Maze, which we verify by running Maze on the Natural Stories corpus. With this data, we are able to confirm that participants can comprehend what they read using Maze and investigate the effects of surprisal on RT in Maze. 


# Error-correction maze

The Maze tasked as historically done involves presenting in-lab participants (often college students) with materials that consist of individual, unrelated sentences, some of which exemplify the relevant properties (often with a Latin Square design). While this is a common psycholinguistic paradigm, it isn't the only one, and Maze would be more versatile (and thus more appealing as an alternative to eye-tracking and SPR) if it also worked on other types of materials, such as longer texts used in research in discourse or surprisal. There's also interest in comparing neural net performance to that of human behavioral data on naturalistic corpora, which necessitates behavioral corpora. 

One major concern with Maze is data loss -- whenever a participant makes a mistake, they don't see the rest of the sentence and thus they do not contribute RT data for any words in the rest of the sentence. TODO cite how much this happened in old paper. While this hasn't been a problem with in lab G-maze (error rates were fairly low and items not too long), it's more of a problem with A-maze on crowd-sourced participants, due to both compontents. In particular, Boyce et al 2020 found that some of the distractors, especially early in the sentence are problematic, and therefore data-loss is high. 

However, even with G-maze quality materials, and high quality subjects, the Maze paradigm with eventually run into trouble as materials get longer. Even assuming a very low per word error rate , the errors will build up as materials get longer, and few participants will make it to the end. For instance, with a 1% error rate, `r round(.99**15*100)`% would complete each 15-word sentence, but only `r round(.99**50*100)`% of participants would complete each 50 word vignette, and `r round(.99**200*100)`% of 200 word passages.  One potential option would be to have participants skip to the next sentence (rather than next item) when they make a mistake. However, having missed some context from the last sentence, this may be jarring and and confusing for participants. Their RTs may also be unreliable  due to having missed context and discourse cues. 

To solve this conundrum, we decide to get rid of the source of the problem: the terminating on errors. Instead, we let participants continue after they make a mistake. To do this, we present them with an error message, and wait until they select the correct option, before continuing the sentence as normal. This was easily implemented as a new option built on the Ibex Maze code used in Boyce et al 2020. We record both the RT to the first click and also the total RT until they select the correct answer as separate values. 

This in principle solves several problems. Firstly, it allows for running longer materials, potentially expanding the range of questions Maze is suitable for. Secondly, it compensates for some of the shortcomings of Maze identified in Boyce et al 2020 -- distractors might still cause participants to make unavoidable mistakes, but at least they still see the rest of the sentence and we get their data. We believe (although don't have evidence) that being able to correct mistakes may reduce frustration related to unavoidable mistakes from impossible choices and accidental misclicks. Whether this also solves the data loss problem from the researcher perspective within a sentence depends on whether post-mistake data are high-quality and trustworthy; this is a hard-to-assess question of potential interest. 

Even if all post-mistake data are discarded before analysis, they can still help address the "distractors or participants" question around errors mentioned in Boyce. When participants continue with the sentence, it is easy to calculate the per-word error rate for participants, without having to account for the censoring present in traditional Maze (if you already made a mistake, we can't know if you would have made another one; with error-correction, we do know if you did). This allows us to exclude participants on the basis of their task accuracy, ensuring RT data comes from participants who were paying attention and doing the task. This provides a convenient and clear-cut way of controlling data quality after the fact. 

It also starts to reduce the perverse incentives. With error-terminates Maze, the fastest way to get through the task is to select randomly, and it's quite quick because mistakes skip you to the next sentence. With error-correction Maze, randomly jamming buttons takes more effort, but is still an effective strategy. [Footnote: In discussing this work, we recieved the suggestion that one way to disincentivize random clicking is to add a pause when a participant makes a mistake, forcing them to wait some short period of time (ex 500ms or 1 sec) before being able to correct their mistake. This seems like a promising improvement that could be worth implementing and testing.]

# Natural Stories corpus

We test this method using the Natural Stories corpus, which consists of 10 passages, some fiction and some non-fiction. We think this is a fairly severe test: the passages are long (about 1000 words each), they contain a lot of punctuation (for instance quoted speech), a number of proper nouns (which are likely out of vocabulary for the autogeneration model), and a number of low frequency grammatical constructions. However, they are intended to read naturally to native English speakers. We choose this because if participants can succeed at doing the Maze task and understanding it on this set of materials, we think they are likely to on basically any naturalistic text (many of which might be shorter or easier to follow). The corpus also comes with binary-choice comprehension questions, 6 per story. 

THis corpus has been used for SPR, so there's the possiblity of comparison. 

In addition to testing that Maze works at all on this type of material, we also wanted to check for two other desired characteristics. That the materials can be comprehended even when read with Maze, and that commonly found psycholinguistic effects of length and surprisal show.  whether surprisal and other coarse-level processing effects would appear (as a sanity check) -- eye-tracking and SPR show these effects robustly and in relatively well established patterns, and for Maze to be trusted as an alternative, it would need to as well. Maze typically doesn't use comprehension questions because comprehending the local sentence structure is required to do the task. Thus, there isn't much information on how well participants comprehend and remember what they read. We wanted to know if participants understood and internalized the gist of what they were reading with Maze. 

To satisfy these various desires, we decided to run a Maze task on the Natural Stories corpus (Futrell et al 2020). This corpus consists of 10 passages, each about 1000 words long. They are naturalistic, although they were designed to contain a fair number of low frequency constructions such as BLAH and BLAH. (We do not specifically analyse this.) They also come with comprehension questions, and SPR data from the corpus is available. THis seemed like an appropriate severe test, and the corpus was available. SEE FOOBAR for examples of the materials and questions. 


# Methods
We report how we determined our sample size, all data exclusions (if any), all manipulations, and all measures in the study. <!-- 21-word solution (Simmons, Nelson & Simonsohn, 2012; retrieved from http://ssrn.com/abstract=2160588) -->

## Participants
We recruited XX participants from Mechanical Turk, and paid each participant YY dollars. 

## Material

We took the corpus and split it up into sentences (this involved some judgement calls due to quoted speech) because the Maze material generation works at the sentence level. We ran the sentences through A-maze generation using TODO foo, bar parameters. Materials are available in TODO: anon repo. We set up the materials in Ibex, using Maze to display things.

We also wrote a short passage to use as practice, with 2 comprehension questions (double check!!). 

## Procedure

Participants gave their informed consent, saw instructions, saw the practice item and practice comprehension questions, and then completed the story using the Maze task, then the 6 main comprehension questions. At the end of the experiment, we asked how their experience was, gathered demographic questions (which ones), and debriefed participants. 

## Data analysis
We used `r cite_r("r-references.bib")` for all our analyses.
We excluded participants who did not indicate that their native language was English. This left us with YY participants. 

# Results

## Qualitative stuff

Our first points of interest were how well participants would do on the Maze task and on the comprehension questions. Figure blah shows the distributions and relationship between participants accuracy on the Maze task (i.e. fraction of the time they selected the correct continuation) and the time to took for them to make selections (mean RT; RT's above ZZ were assumed to be indicative of a break and excluded). Participant dots are color coded based on whether they were at least 80% accurate; this is the threshold we use for including data in future analysis. A number of participants did quite well at the task (accuracies reached (double check!) 99%). This shows that it's possible for participants to focus on the task for the whole time. (While this is not overall much longer than previous studies such as Witzel 2012 and Boyce et al 2020, it had to be continuous, and so we couldn't use a blocking structure to create pauses.) It also shows that A-maze distractors are high-enough quality that participants are able to get them right most of the time. Given the clump of participants at around 50% accuracy, which is consistent with randomly clicking, we assume they are not taking the task seriously. In terms of RT, we see that attentive participants take on average %FOOBAR seconds, while participants who are clicking randomly can do so much faster. 

In terms of comprehension questions, we see relatively good performance by participants who were attentive to the task. These answers were also provided quickly, which mostly rules out the (define quickly) possibility that they were googling for the answers. There's a chance that some knew the answers already, so this isn't super conclusive. Because the stories take a long time to read (~15 minutes+), participants are having to remember the story/answers to questions for quite a while. This performance is less goood than that for SPR; but it's hard to know comparing between methods with different task demands and subject pools. Participants who didn't do well also didn't do well here. 

## Quantitative stuff

There's commonly observed effects of surprisal, frequency, and length on stuff, so we looked at that. TODO: how we got estimates of surprisal, frequency, and length. TODO: word exclusions

- GAMs
- LMER/BRMS 

# Discussion
We've shown another way to adapt the Maze task to make it suitable for a wider range of experiments. While we think that error-correction Maze is generally a good idea, especially when using A-maze, this is an orthagonal adjustment to the task. By using the Natural Stories corpus, we establish that Maze works for longer naturalistic tasks, with participants able to do the task and understand what they are reading. Additionally, their RTs reflect expected patterns in terms of length and surprisal. While there are some differences, especially in scale, to that found with other incremental processing paradigms, we think this is a reason to explore more. Could be a statistical thing (???) or due to task differences. Comparisons between methods could be very useful for identifying how task demands influence processing. 

All the code is available, and we encourage researchers to consider trying out Maze if they think it's appropriate to their experiments. 

This is just one other data point on A-maze, so we still don't know how widely the method holds up; however the fact that it works on such disparate materials gives us hope that it will be useful for a variety of paradigms. 

With new methods, its useful to try to establish how widely they work. We hope to test A-maze on an experiment that is very different from the syntactic disambiguation, in order to learn whether it works for a different type of psycholinguistic material. 
While Maze has traditionally been limited to single sentence items; we innovate on how the materials are presented and show that it is possible for participants to read naturalistic passages that are presented in the Maze paradigm. We confirm that participants can understand what they are reading, and that Maze shows linear surprisal effects. This opens up another area of research to being amenable to the new A-maze paradigm. 

This is just one other data point on A-maze, so we still don't know how widely the method holds up; however the fact that it works on such disparate materials gives us hope that it will be useful for a variety of paradigms. 

The Maze task might be espeically good for comparing with NNs because of the forced incrementality. While eventually we do want to figure out models of more natural reading, this might be a useful complement b/c we don't have to deal with nasty spillover. 

\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup
