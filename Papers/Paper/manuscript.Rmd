---
title             : "A-maze of Natural Stories: Comprehension and surprisal in the Maze task"
shorttitle        : "A-maze of Natural Stories"

author: 
  - name          : "Veronica Boyce"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "450 Jane Stanford Way, Building 420, Stanford University, Stanford, CA 94305"
    email         : "vboyce@stanford.edu"
    role:         # Contributorship roles (e.g., CRediT, https://casrai.org/credit/)
      - Conceptualization
      - Formal Analysis
      - Investigation
      - Software
      - Writing - Original Draft Preparation
      - Writing - Review & Editing
  - name          : "Roger Levy"
    affiliation   : "2"
    role:
      - Conceptualization
      - Formal Analysis
      - Writing - Review & Editing

affiliation:
  - id            : "1"
    institution   : "Stanford University"
  - id            : "2"
    institution   : "Massachusetts Institute of Technology"
header-includes:
 - \usepackage{setspace}\singlespacing
 - \renewcommand{\textfraction}{0.05}
 - \renewcommand{\topfraction}{0.8}
 - \renewcommand{\bottomfraction}{0.8}
 - \renewcommand{\floatpagefraction}{1}
 - \setcounter{topnumber}{3}
 - \setcounter{bottomnumber}{3}
 - \setcounter{totalnumber}{4}

authornote: |
   We thank the AMLAP 2020 audience, the Computational Psycholinguistics Lab at MIT, the Language and Cognition Lab at Stanford, the QuantLang Lab at UC Irvine, and Mike Frank for feedback on this work.
  
abstract: |
   Behavioral measures of word-by-word reading time provide experimental evidence to test theories of language processing. A-maze is a recent method for measuring incremental sentence processing that can localize slowdowns related to syntactic ambiguities in individual sentences. We adapted A-maze for use on longer passages and tested it on the Natural Stories corpus. Participants were able to comprehend these longer text passages that they read via the Maze task.  Moreover, the Maze task yielded useable reaction time data with word predictability effects that were linearly related to surprisal, the same pattern found with other incremental methods. Crucially, Maze reaction times show a tight relationship with properties of the current word, but little spillover of effects from previous words. This superior localization is an advantage of Maze compared with other methods. Overall, we expanded the scope of experimental materials, and thus theoretical questions, that can be studied with the Maze task. 


bibliography      : ["r-references.bib","refs.bib"]

floatsintext      : yes
figurelist        : no
tablelist         : no
footnotelist      : no
linenumbers       : yes
mask              : no
draft             : no

documentclass     : "apa6"
classoption       : "man"
output            : papaja::apa6_pdf
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(dev = "png", dev.args = list(type = "cairo-png"))
 library("papaja")
 library(here)
 library(tidyverse)
library(patchwork)
 library(brms)
 library(lme4)
 library(tidybayes)
 library(mgcv)
 library(cowplot)
 library(gridExtra)
library(broom.mixed)
library(tidymv)
library(kableExtra)
r_refs("r-references.bib", append=F)
theme_set(theme_bw())
options(knitr.table.format = "pdf")
```

# Introduction

Two chief results of human language processing research are that comprehension is highly incremental and that comprehension difficulty is differential and localized. Incrementality in comprehension means that our minds do not wait for large stretches of linguistic input to accrue; rather, we eagerly analyze each moment of input and rapidly integrate it into context [@marslen-wilson:1975]. Differential and localized processing difficulty means that different inputs in context present different processing demands during comprehension [@levy:2008]. Due to incrementality these differential processing demands are, by and large, met relatively quickly by the mind once they are presented, and they can be measured in both brain [@kutas-hillyard:1980; @osterhout-holcomb:1992jml] and behavioral [@raynerEyeMovementsReading1998; @mitchell:2004online-methods] responses. These measurements often have low signal-to-noise ratio, and many methods require bringing participants into the lab and often require cumbersome equipment. However, they can provide considerable insight into how language processing unfolds in real time. Developing more sensitive methods that can easily be used with remote participants is thus of considerable interest.

Word-by-word reading or response times are among the most widely used behavioral measurements in language comprehension and give relatively direct insight into processing difficulty. The Maze task [@freedman85; @forsterMazeTaskMeasuring2009], which involves collecting participants' response times in a repeated two-alternative forced-choice between a word that fits the preceding linguistic context and a distractor that doesn't, has recently been proposed as a high-sensitivity method that can easily be used remotely. @boyceMazeMadeEasy2020 introduced several implementational innovations that made it easier for researchers to use Maze, and showed that for several controlled syntactic processing contrasts [@witzelComparisonsOnlineReading2012a] Maze offers better statistical power than self-paced reading, the other word-by-word response time method easy to use remotely. Maze has since had rapid uptake in the language processing community [@chacon2021limits; @ungerer2021using; @orth2022processing] (other cites).

However, there is increasing interest in collecting data during comprehension of more naturalistic materials such as stories and news articles [@demberg-keller:2008; @luke-christianson:2016-limits; @futrellNaturalStoriesCorpus2020], which offer potentially improved ecological validity and larger data scale in comparison with repeated presentation of isolated sentences out of context. These more materials require maintaining and integrating discourse dependencies and other type of information over longer stretches of time and linguistic material. Previous work leaves unclear whether the Maze task would be feasible for this purpose: the increased task demands might interfere with the demands presented by these more naturalistic materials, and vice versa. In this paper we report a new modification of the Maze task and show that it makes reading of extended, naturalistic texts feasible. We also analyze the resulting reaction time profiles and show that they provide strong signal regarding the probabilistic relationship between a word and the context in which it appears, and that the systematic linear relationship between word surprisal and response time observed in other reading paradigms [@smithEffectWordPredictability2013] also arises in the Maze task.
<!-- Behavioral measures of word-by-word reading time provide experimental evidence to test theories of language processing. Maze is a potentially promising method, and it's feasibility has recently increased due to the introduction of A-Maze [@boyceMazeMadeEasy2020]. -->
In the remainder of the Introduction, we lay out the role of RT-based methods in theory testing, describe a few common methods, and review some key influences on reading time. We then proceed to present our modified "error-correction Maze" paradigm, our experiment, and the results of our analyses of the resulting data.

## Why measure RTs?

A major feature of human language is that not all sentences or utterances are equally easy to successfully comprehend. Sometimes this is mostly or entirely due to the linguistic structure of the sentence: for example, *The rat that the cat that the dog chased killed ate the cheese* is more difficult than *The rat that was killed by the cat that was chased by the dog ate the cheese* even though the meaning of the two sentences is (near-)identical. Sometimes the source of difficulty can be a mismatch between expectations set up by the context and the word choice in an utterance: for example, the question *Is the cup red?* may be confusing in a context containing more than one cup. Psycholinguistic theories may differ in their ability to predict what is easy and what is hard. One of the most powerful methods for studying these differential difficulty effects is to turn over control of presentation of the linguistic material to comprehender, and to measure what she takes time on. For this purpose, taking measurements from experimental participants during reading, a widespread, highly practiced skill in diverse populations around the world, is of unparalleled value.

To a first approximation, everyday reading (when the reader's goal is to understand a text's overall content) is *progressive*: we read documents, paragraphs, and sentences from beginning to end. The reader encounters each word with the benefit of the preceding linguistic context. Incrementality in reading involves successively processing each word encountered and integrating it into the context. For a skilled reader experienced with the type of text being read, most words are easy enough that the subjective experience of reading the text is of smooth, continuously unfolding understanding as we construct a mental model of what is being described. But occasionally a word may be sufficiently surprising or otherwise difficult to reconcile with the context that it disrupts comprehension to the level of conscious awareness: in the sentence *I take my coffee with cream and chamomile*, for example, the last word is likely to do so. Behaviorally, this disruption typically manifests as a slowdown or longer *reading time* (RT) on the word itself, on the immediately following words, or in other forms such as regressive eye movements back to earlier parts of the text to check the context.

In fact, RTs and other measures that capture processing disruption vary substantially with the difficulty of words in their context below the level of conscious awareness as well, with millisecond scale differences in reading time between words. 
<!-- Occasionally we read an unexpected word, cannot make sense of it and have to read the sentence again. The words that cause these stumbles are the outliers on a distribution of words varying in how easy they are to process in context. Even when we don't notice a problem, less expected words still take longer to process as we (re)construct a mental model of the sentence. Fortunately for reading and unfortunately for studying language, this process of adjustment is very quick, with millisecond scale differences in reading time between words. -->
That is, the differential difficulty or processing load posed by various parts of a text is to a considerable extent *localizable* to specific words in their context. For this reason, RTs have proven a highly valuable measure for testing the predictions of psycholinguistic theory, ranging from theories of character recognition, memory retrieval, parsing, and beyond. 

For instance, competing theories about why certain types of object-extracted relative clauses, like *the lawyer that the banker irritated*, are harder to understand than the corresponding subject-extracted relative clauses, like *the lawyer that irritated the banker*, make different predictions about which words are the loci of the overall difficulty and slower RTs associated with object relatives [@grodnerConsequencesSerialNature2005; @staubEyeMovementsProcessing2010; @traxlerProcessingSubjectObject2002]. RT measures can potentially also inform theories about the time course of processing (i.e. which steps are parallel versus serial, @bartekSearchOnlineLocality2011) or the functional form of relationships between word characteristics and processing time [@smithEffectWordPredictability2013].

Some of these theories rely on being able to attribute processing slowdowns to a particular word. Determining that object relatives are overall slower that subject relatives is easy. Even an imprecise RT measure will determine that the same set of words in a different order took longer to read at a sentence level. However, many language processing theories make specific (and contrasting) theories about which words in a sentence are harder to process. To adjudicate among these theories, we want methods that are *well-localized*, so it is easy to determine which word is responsible for an observed RT slow-down. Ideally, longer RT on a word would be an indication of that word's increased difficulty, and not the lingering signal of a prior word's increased difficulty. When the signal isn't localized, advanced analysis techniques may be required to disentangle the slow-downs [@shainDeconvolutionalTimeSeries2018]. 

## Eye-tracking and Self-paced reading

<!-- #Behavioral methods that measure how long it takes to read each word in a sentence are referred to as incremental processing methods, and their dependent measures are called reading times or reaction times, both abbreviated RT. -->
The two most commonly used behavioral methods for studying incremental language processing during reading are tracking eye movements and self-paced reading. While both of these have proven powerful and highly flexible, they both have important limitations as well.


In eye-tracking, participants read a text on a screen naturally, while their saccadic eye movements are recorded on a computer-connected camera that is calibrated so that the researcher can reconstruct with high precision where the participant's gaze falls on the screen at all times [@raynerEyeMovementsReading1998]. From these eye movements can be reconstructed various position-specific reading time measures such as *gaze duration* (the total amount of time the eyes spend on a word the first time it is fixated, or zero if the eye skipped the word the first time it was approached from the left) and *total viewing time* (the total amount of time that the word is fixated). Eye tracking data collected with state-of-the-art high-precision recording equipment offers relatively good signal-to-noise ratio, but the difficulty presented by a word can still *spill over* into reading measures on subsequent words, a dynamic that can make it hard to isolate the source of an effect of potential theoretical interest [@raynerEffectsFrequencyPredictability2004; @levyEyeMovementEvidence2009; @frazierMakingCorrectingErrors1982]. Additionally, the equipment is expensive and data collection is laborious and must occur in-lab.

Self-paced reading (SPR; @mitchell:1984) is a somewhat less natural paradigm in which the participant manually control the visual presentation of the text by pressing a button. In its generally preferred variant, moving-window self-paced reading, words are revealed one at a time or one group at a time, wihth every press of the button masks the currently presented word (group) and simultaneously reveals the next. The time spent between button presses is the unique RT measure for that word (group). Self-paced reading requires no special equipment and can be delivered remotely, but the measurements are noisier and even more prone to spillover [@macdonaldInteractionLexicalSyntactic1993; @koornneefUseVerbbasedImplicit2006; @smithEffectWordPredictability2013].


## Maze
The Maze task is an alternative method that is designed to increase localization at the expense of naturalness [@freedman85; @forsterMazeTaskMeasuring2009]. In the Maze task, participants must repeatedly choose between two simultaneously presented options: a correct word that continues the sentence, and a distractor string which does not. Participants must choose the correct word, and their time to selection is treated as the reaction time, or RT. (We intentionally overload the abbreviation "RT" and use it for Maze reaction times as well as reading times from eye tracking and SPR, because the desirable properties of reading times turn out to hold for Maze reaction times as well.) @forsterMazeTaskMeasuring2009 introduced two versions of the Maze task: lexical "L"-maze where the distractors are non-word strings, and grammatical "G"-maze where the distractors are real words that don't fit with the context of the sentence. In theory, participants must fully integrate each word into the sentence in order to confidently select it, which may require mentally reparsing previous material in order to allow the integration and selection of a disambiguating word. @forsterMazeTaskMeasuring2009 call this need for full integration "forced incremental processing" to distinguish from other incremental processing methods where words can be passively read before later committing to a parse. This idea of strong localization is supported by studies finding strongly localized effects for G-maze [@witzelComparisonsOnlineReading2012a; @boyceMazeMadeEasy2020]. 

The downside of G-maze is that materials are effort-intensive to construct because of the need to select infelicitous words as distractors for each spot of each sentence. This burdensome preparation may explain why the Maze task has not been widely adopted. @boyceMazeMadeEasy2020 demonstrated a way to automatically generate Maze distractors by using language models from Natural Language Processing to find words that are high surprisal in the context of the target sentence, and thus likely to be judged infelicitous by human readers. @boyceMazeMadeEasy2020 call Maze with automatically generated distractors A-maze. In a comparison, A-maze distractors had similar results to the hand-generated G-maze distractors from @witzelComparisonsOnlineReading2012a and A-maze outperformed L-maze and an SPR control in detecting and localizing expected slowdown effects. @sloggettAmazeAnyOther2020 also found that A-maze and G-maze distractors yielded similar results on a disambiguation paradigm.

Another recent variant of the Maze task is interpolated I-maze, which uses a mix of real word distractors (generated via the A-maze process) and nonce word distractors [@vaniUsingInterpolatedMaze2021; @wilcoxTargetedAssessmentIncremental2021]. The presence of real word distractors encourages close attention to the sentential context, while nonce words can be used as distractors where the word in the sentence is itself ungrammatical or highly unexpected, and/or it is important that the predictability of the distractor in the context is perfectly well-balanced (at zero) across all experimental conditions.


## Measuring localization: Frequency, length, and surprisal effects

Localized measures can be used to attribute processing difficulty to individual words; however, to determine if a method is localized requires knowing how hard the words were to process. One approach is to look at properties of words that are known to influence reading times across methods such as eye-tracking and SPR. Longer words and lower frequency words tend to take longer to process [@klieglLengthFrequencyPredictability2004], as do less predictable words [@raynerEffectsFrequencyPredictability2004]. 

A word can be unpredictable for a variety of reasons: it could be low frequency, semantically unexpected, the start of a low-frequency syntactic construction, or a word that disambiguates prior words to a less common parse. Many targeted effects of interest are essentially looking at specific features that contribute to how predictable or unpredictable a word is. Thus incremental processing methods that are sensitive to predictability are useful for testing linguistic theories that make predictions about what words are unexpected.

The overall predictability of a word in a context can be estimated using language models that are trained on large corpora of language to predict what word comes next in a sentence. A variety of pre-trained models exist, with varied internal architectures and training methods, but all of them generate measures of predictability. Predictability is often measured in bits of surprisal, which is the negative log probability of a word (1 bit of surprisal means a word is expected to occur half the time, 2 bits is 1/4 of the time etc). 

The functional form of the relationship between RTs from eye-tracking and SPR studies and the predictability of the words is linear in terms of surprisal [@smithEffectWordPredictability2013; @wilcoxPredictivePowerNeural2020; @goodkindPredictivePowerWord2018; @lukeLimitsLexicalPrediction2016], even when two important context-invariant word features known to influence RTs, length and frequency, are controlled for. Predictability reliably correlates with reading time over a wide range of surprisals found in natural-sounding texts, not just for words that are extremely expected or unexpected [@smithEffectWordPredictability2013].  If Maze RTs reflect the same processing as other methods, we expect to find a similar linear relationship with surprisal. 

## Current experiment

The Maze task has thus far primarily been used on constructed sentences focusing on targeted effects and not on the long naturalistic passages used to assess the relationship between RT and surprisal. We tested how A-maze performs on longer naturalistic corpora and compared it with self-paced reading (SPR), with the following main questions in mind:

  1. Do participants engage with these longer passage successfully with the A-maze task?
  2. Is A-maze as powerful and reliable a method as SPR for these longer passages?
  3. What is the functional form between word surprisal and RT for the A-maze task?
  4. Does A-maze have less spillover than SPR?
  5. What types of context-driven expectations, as operationalized in competing computational language models, are deployed to determine RTs in A-maze and SPR?

We used the Natural Stories corpus [@futrellNaturalStoriesCorpus2020], which consists of 10 passages of roughly 1000 words each which are designed to read fluently to native speakers. At the same time, the passages contain copious punctuation, quoted speech, proper nouns, and low frequency grammatical constructions. The corpus is accompanied by binary-choice comprehension questions, 6 per story, which we used to assess comprehension.

We tweaked the A-maze task to accommodate these longer passages and then had participants read the passages in the Maze. We compare our A-maze results with SPR data collected on the Natural Stories corpus by @futrellNaturalStoriesCorpus2020.



# Methods

We constructed A-maze distractors for the Natural Stories corpus [@futrellNaturalStoriesCorpus2020] and recruited 100 crowd-sourced participants to each read a story in the Maze paradigm. The materials, data, and analysis code are all available at REPO. 

  
## Task: Error-correction Maze

(ref:diagram-cap) Schematic of error-correction Maze. A participant reads a sentence word by word, choosing the correct word at each time point (selections marked in blue ovals). When they make a mistake, an error message is displayed, so the participant can try again and continue with the sentence. 

```{r diagram, out.height="25%", fig.width=8, fig.height=3, fig.pos="ht", fig.cap="(ref:diagram-cap)"}
knitr::include_graphics(here("Papers/maze_diagram_cropped.pdf"))
```

In order to support longer materials, we tweaked the Maze task, creating a new variant called error-correction Maze. 

One of the benefits of the Maze task is that it forces incremental processing by having participants make an active choice about what the next word is. But what happens if they choose incorrectly? In the traditional Maze paradigm, any mistake ends the sentence, and the participant moves on to the next item [@forsterMazeTaskMeasuring2009]. An advantage of this is that participants who contribute RT data are very likely to have understood the sentence up to that point. This contrasts with other methods, where determining whether participants are paying attention usually requires separate comprehension check questions, usually not used for Maze. 

However, terminating sentences on errors means that we don't have RTs for words after a participant makes a mistake in an item. In traditional G-maze tasks, with hand-crafted distractors and attentive participants, errors are rare and data loss is a small issue. However, this data loss is much worse with A-maze materials and crowd-sourced participants [@boyceMazeMadeEasy2020]. The high errors are likely from some combination of participants guessing randomly and from auto-generated distractors that in fact fit the sentence; as @boyceMazeMadeEasy2020 noted, some distractors, especially early in the sentence, were problematic and caused considerable data loss. 

The high error rates could be improved by auto-generating better distractors or hand-replacing problematic ones, but that does not solve the fundamental problem with long items. Well-chosen distractors and attentive participants reduce the error rate, but the error rate will still compound over long materials. For instance, with a 1% error rate, `r round(.99**15*100)`%  of participants would complete each 15-word sentence, but only `r round(.99**50*100)`% would complete a 50 word vignette, and `r round(.99**200*100)`% would complete a 200 word passage.  In order to run longer materials, we needed something to do when participants made a mistake other than terminate the entire item.

As a solution, we introduce an *error-correction* variant of Maze shown in Figure \@ref(fig:diagram). When a participant makes an error, they see an error message and must try again to select the correct option, before continuing the sentence as normal. We make error-correction Maze available as an option in a modification of the Ibex Maze implementation introduced in @boyceMazeMadeEasy2020 (https://github.com/vboyce/Ibex-with-Maze). The code records both the RT to the first click and also the total RT until the correct answer is selected as separate values.

Error-correction Maze expands the types of materials that can be used with Maze to include arbitrarily long passages and cushions the impact of occasional problematic distractors. Error-correction Maze is a change in experimental procedure, and is independent of what types of distractors are used. This error-correction presentation is used here with A-maze, but could also be used with G-maze or I-maze. 





## Materials
We used the texts from the Natural Stories corpus [@futrellNaturalStoriesCorpus2020] and their corresponding comprehension questions. To familiarize participants with the task, we wrote a short practice passage and corresponding comprehension questions. See Appendix A for an excerpt of one of the stories. 

To generate distractors, we first split the corpora up into sentences, and then ran the sentences through the A-maze generation process. We used an updated version of the codebase from @boyceMazeMadeEasy2020 which had the capability to match the greater variety of punctuation present in the Natural Stories corpus (updated auto-generation code at https://github.com/vboyce/Maze).  We took the auto-generated distractors as they were, without checking  for quality. 


## Participants

```{r participants}

data <- read_rds(here("Data/cleaned.rds"))

data_filt <- data %>% 
  filter(native %in% c("ENG", "English", "ENGLISH", "english")) #I peeked at what people put that semantically maps to english

data_stories <- data_filt %>% 
  select(type, subject) %>% 
  unique() %>% 
  group_by(type) %>% 
  tally() %>% 
  filter(type!="practice")

data_error_summ <- data_filt %>% 
  mutate(correct.num=ifelse(correct=="yes", 1,0)) %>% 
  group_by(subject, num_correct) %>%
  filter(type!="practice") %>% 
  filter(rt<5000) %>% 
  summarize(mean_rt=mean(rt),
            pct_correct=mean(correct.num)) %>% 
  mutate(good_comp=ifelse(num_correct>4,"5 or 6 correct","4 or fewer correct")) %>% 
    mutate(accurate=ifelse(pct_correct>.8,">80% correct", "<80% correct"),
           is.attentive=ifelse(pct_correct>.8,1,0))

some <- data_filt %>% select(subject, num_correct) %>% 
  unique() %>% 
  left_join(data_error_summ, by=c("subject", "num_correct")) %>% 
  filter(pct_correct>.8)

data_good <- data_filt %>% 
  left_join(data_error_summ, by=c("subject", "num_correct")) %>% 
  filter(type!="practice") %>% 
  filter(pct_correct>.8) %>% 
  mutate(is.correct=ifelse(correct=="yes",1,0))

data_non_error <- data_good %>% 
  filter(is.correct==1) %>% 
  filter(word_num>0)

data_before <- data_good %>% 
  mutate(word_num_mistake=ifelse(correct=="no", word_num,NA)) %>% 
  group_by(sentence, subject) %>% 
  fill(word_num_mistake) %>% ungroup() %>% 
  mutate(after_mistake=word_num-word_num_mistake,
         after_mistake=ifelse(is.na(after_mistake),0,after_mistake)) %>% 
  filter(after_mistake==0) %>% 
  filter(is.correct==1) %>% 
  filter(word_num>0)

data_sentence <- data_before <- data_good %>% 
  mutate(word_num_mistake=ifelse(correct=="no", word_num,NA)) %>% 
  group_by(sentence, subject) %>% 
  fill(word_num_mistake) %>% ungroup() %>% 
  mutate(after_mistake=word_num-word_num_mistake,
         after_mistake=ifelse(is.na(after_mistake),0,after_mistake)) %>% 
  group_by(sentence, subject) %>% 
  summarize(total_mistake=sum(after_mistake)) %>% 
  mutate(sent.correct=ifelse(total_mistake==0,1,0))

spr <- read_rds(here("Data/SPR/first.rds")) %>% filter(correct>4) %>% select(WorkerId) %>% unique()
```

We recruited 100 participants from Amazon Mechanical Turk in April 2020, and paid each participant $3.50 for roughly 20 minutes of work. We excluded data from those who did not report English as their native language, leaving `r data_filt %>% select(subject) %>% unique() %>% nrow()` participants. After examining participants' performance on the task (see Results for details), we excluded data from participants with less than 80% accuracy, removing participants whose behavior was consistent with random guessing. After this exclusion, `r some %>% select(subject) %>% unique() %>% nrow()` participants were left. 

## Procedure
Participants first gave their informed consent and saw task instructions. Then they read a short practice story in the Maze paradigm and answered 2 binary-choice practice comprehension questions, before reading one main story in the A-maze task. After the story, they answered 6 comprehension questions, commented on their experience, answered optional demographic questions, were debriefed, and were given a code to enter for payment. The experiment was implemented in Ibex (https://github.com/addrummond/ibex).  

## Self-paced reading comparison

In addition to the texts, @futrellNaturalStoriesCorpus2020 released reading time data from a SPR study they ran in 2011. They recruited 181 participants from Amazon Mechanical Turk, most of whom read 5 of the stories. After reading each story, each participant answered 6 binary-choice comprehension questions. For comparability with A-maze, we analyze only the first story each participant read, and, in line with @futrellNaturalStoriesCorpus2020, exclude participants who got less than 5/6 of the comprehension questions correct, leaving `r spr %>% nrow()`. 

## SPR-Maze correlation
We compared the correlations between the Maze and SPR RTs to within-Maze and within-SPR correlations. For Maze, within each story, we randomly split subjects into two halves. Within each half, we calculated a per-word average RT for each word and then a per-sentence average RT across word averages. We calculated a within-Maze correlation between these two halves. 

For this comparison, we downsampled the SPR data choosing a number of participants equal to the number we have for Maze to avoid differences due to dataset size. We then used the same procedure to get a within-SPR correlation. For between Maze-SPR correlation, we took the average correlation across each of the 4 pairs of Maze half and SPR half.


```{r}
package_list <- c("tidyverse","brms","rstan","papaja",
                  "lme4", "mgcv", "tidybayes", "here", "patchwork","cowplot","gridExtra","broom.mixed","tidymv","kableExtra")

a <- cite_r("r-references.bib", pkgs=package_list, withhold=F, footnote=T)

```

## Modeling approach

Our analytic questions required multiple modeling approaches. To look at the functional form of the relationship between surprisal and RT data, we fit Generalized Additive Models (GAMs) to allow for non-linear relationships. GAMs are harder to interpret than linear models, so to measure effect sizes and assess spillover, we used linear mixed models. Finally, in order to determine which language model best predicts the RT data, we fit additional linear models with predictors from multiple language models to look at their relative contributions. All these models used surprisal, frequency, and length as predictors for RT. We considered these predictors from both the current and past word to account for the possibility of spillover effects in A-maze. For SPR comparisons, we included predictors from the current and past three words to account for known spillover effects. We conducted data processing and analyses using `r a$r`.

`r a$pkgs`

```{r}
surps <- read_rds(here("Prep_code/natural_stories_surprisals.rds")) 

not_first <- surps %>% filter(Word_In_Sentence_Num!=1)

#nrow(not_first)
  

not_na <- not_first %>% filter(across(ends_with("surp"),~!is.na(.x)),
                               !is.na(freq))


not_ngram <- not_na %>%   filter(ngram_token_count==1)


#nrow(not_ngram)

not_other <- not_ngram %>% 
filter(txl_token_count==1)%>% 
  filter(grnn_token_count==1)
#nrow(not_other)

not_gpt <- not_other %>% filter(gpt_token_count==1)

#nrow(not_gpt)

```


### Predictors 

We created a set of predictor variables of frequency, word length, and surprisals from 4 language models.  For length, we used the length in characters excluding end punctuation. For unigram frequency, we tokenized the training data from @gulordava18 and tallied up instances. We then rescaled the word counts to get the log2 frequency of occurrences per 1 billion words, so higher values indicate higher log frequencies. We got per-word surprisals for each of 4 different language models, covering a range of common architectures: a Kneser-Ney smoothed 5-gram, GRNN [@gulordava18], Transformer-XL [@daiTransformerXLAttentiveLanguage2019], and GPT-2 [@radfordLanguageModelsAre], using lm-zoo [@gauthierSyntaxGymOnlinePlatform2020].  For all of these predictors, we used both the predictor at the current word as well as lagged predictors from the previous word.

### Exclusions

We excluded the first word of every sentence because it had an x-x-x distractor, leaving `r nrow(not_first)` words. We excluded words for which we didn't have surprisal or frequency information, leaving `r nrow(not_na)` words. We additionally excluded words that any model treated as being composed of multiple tokens (primarily words with punctuation),  leaving `r nrow(not_gpt)` words[^1]. We excluded outlier RTs that were <100 or >5000 ms (<100 is likely a recording error, >5000 is likely the participant getting distracted). We exclude RTs from words where mistakes occurred or which occurred after a mistake in the same sentence. We only analyzed words where we had values for all predictors, which meant that if the previous word was unknown to a model, the word was excluded because of missing values for a lagged predictor.

[^1]:Surprisals should be additive, but summing the surprisals for multi-token words gave some unreasonable responses. For instance, in one story the word king!\' has a surprisal of 64 under GRNN (context: The other birds gave out one by one and when the eagle saw this he thought, \'What is the use of flying any higher? This victory is in the bag and I am king!\'). While GPT-2 using byte-pair encoding that can split up words into multiple parts, excluding words it split up only excluded 30 words that were not already excluded by other models.

### Model specification
To infer the shape of the relationship between our predictor variables and RTs, we fitted generalized additive models (GAMs) using `R`'s `mgcv` package to predict the mean RT (after exclusions) for each word, averaging across participants from whom we we obtained an unexcluded RT for that word. We centered but did not rescale the length and frequency predictors, and left surprisal uncentered for interpretability. We used smooth terms (`mgcv`'s `s()`) for surprisal and tensor product terms (`mgcv`'s `ti()`) for frequency-by-length effects and interactions. We use restricted maximum likelihood (REML) smoothing for parameter estimation. To more fully account for the uncertainty in the smoothing parameter estimates, we fitted 101 bootstrap replicates of each GAM model; in Figures \@ref(fig:gam) and \@ref(fig:spr-gam), the best-fit lines derive from the mean estimated effect size across the bootstrap replicates, and the shaded areas indicate a 95\% bootstrap confidence interval on this effect size (the boundaries are the 2.5\% and 97.5\% quantiles of the bootstrapped replicates).

For linear models, we centered all predictors. We modeled the main effects of surprisal, length, and frequency as well as surprisal x length and frequency x length interactions.  For the A-maze data, we used maximal mixed effects, including by-subject slopes and a per-word-token random intercept [@barrRandomEffectsStructure2013]. We used weak priors (normal(1000,1000) for intercept, normal(0,500) for beta and sd, and lkj(1) for correlations) and ran models with brm [@burknerAdvancedBayesianMultilevel2018]. 

For linear models of the SPR data, we were unable to fit the full mixed effects structure. The best model we could fit had by-subject random intercept, uncorrelated by-subject random slopes for surprisal, length and frequency, and a per-word-token random intercept, fit with lme4 [@batesFittingLinearMixedeffects2015], as this structure did not fit reliably in brm.

For model comparison, we took by-item averaged data to aid in fast model fitting.  We included frequency, length, and their interaction in all models. Then we fit models with either 1 or 2 sources of surprisal using lm [@batesFittingLinearMixedeffects2015] and assessed the effect of adding the second surprisal source with an anova. 

# Results

## Do participants engage successfully?

(ref:error-cap) A. Participant's accuracy on the Maze task (fraction of words selected correctly) versus their average reaction time (in ms). Many participants (marked in green) chose the correct word >80% of the time; others (in red) appear to be randomly guessing. B. Performance on the comprehension questions. Participants with low accuracy performed poorly on comprehension questions; Participants with >80% task accuracy tended to do well; their performance was roughly comparable to the performance of SPR participants from  @futrellNaturalStoriesCorpus2020 on their first stories. 

```{r errors, out.height="25%", fig.width=8, fig.height=3, fig.pos="ht", fig.cap="(ref:error-cap)"}
error_plot <- ggplot(data_error_summ, aes(x=pct_correct, y=mean_rt, color=accurate))+
  geom_point(size=1)+
  labs(x="Fraction words selected correctly",
       y="Mean Reaction Time (ms)")+
  coord_cartesian(xlim=c(0.4,1), ylim=c(0,1600), expand=F)+
    scale_color_manual(values=c(">80% correct"="darkgreen","<80% correct"="darkred"))+
  guides(color="none")


comp_comp <- read_rds(here("Analysis/comp.rds")) %>% 
  filter(source %in% c(">80% correct","<80% correct", "SPR 1st story")) 

comp_tally <- read_rds(here("Analysis/comp.rds")) %>% group_by(source) %>% filter(correct>4) %>% 
  summarize(good=round(sum(pct)*100))
comp_plot <- ggplot(comp_comp, aes(x=correct,y=pct,fill=source))+
  geom_col(position="dodge")+
  facet_grid(.~source)+
  labs(x="Comprehension questions correct (out of 6)", y="Percent participants")+
  scale_fill_manual(values=c(">80% correct"="darkgreen","<80% correct"="darkred",  "SPR 1st story"="grey30"))+
guides(fill="none")

error_plot+comp_plot+plot_annotation(tag_levels="A")

```

Our first question was whether participants could engage successfully with the  error-correction Maze task. We assessed engagement by looking at participants' accuracy on the Maze task and performance on the comprehension questions. 

```{r}

data_long <- data_filt %>% filter(type!="practice") %>% filter(rt>5000)
data_no_prac <- data_filt %>% filter(type!="practice")
```

Accuracy, or how often a participant chose the correct word over the distractor, reflects both the quality of the distractors and the focus and skill of the participant. We calculated the per-word accuracy rate for each participant and compared it against their average reaction time [^2]. As seen in Figure \@ref(fig:errors)A, one cluster of participants (marked in green) made relatively few errors, with some reaching 99% accuracy. This high performance confirms that the distractors were generally appropriate and shows that some participants maintained focus on the task for the whole story. These careful participants took around 1 second for each word selection, which is much slower than in eye-tracking or SPR.

[^2]: To avoid biasing the average if a participant took a pause before returning to the task, RTs greater than 5 seconds were excluded. This exclusion removed `r nrow(data_long)` words, or `r nrow(data_long)/nrow(data_no_prac)*100 %>% round(2)`% of trials.

Another cluster of participants (in red) sped through the task, seemingly clicking randomly. This bimodal distribution is likely due to the mix of workers on Mechanical Turk, as we did not use qualification cutoffs. We believe the high level of random guessing is an artifact of the subject population [@hauser2018, and we expect that following current recommendations for participant recruitment, such as using qualification cutoffs or another recruitment site would result in fewer participants answering randomly [@eyal2021;@peer2017]. 



To determine comprehension accuracy, we counted how many of the binary-choice comprehension questions each participant got right (out of 6). As seen in Figure \@ref(fig:errors)B, most participants who were accurate on the task also did well on comprehension questions, while participants who were at chance on the task were also at chance on the comprehension questions. Participants usually answered quickly (within 10 seconds), so we do not believe they were looking up the answers on the Internet. We can't rule out that some participants may have been able to guess the answers without understanding the story. Nonetheless, the accurate answers provide preliminary evidence that people can understand and remember details of stories they read during the Maze task. 

The comprehension question performance of accurate Maze participants is broadly similar to the performance of SPR participants from @futrellNaturalStoriesCorpus2020 on the first story read. Overall, `r comp_tally %>% filter(source=="all Maze") %>% pull(good)`% of Maze participants got 5 or 6 questions right (`r comp_tally %>% filter(source=="<80% correct") %>% pull(good)`% of low-accuracy participants and `r comp_tally %>% filter(source==">80% correct") %>% pull(good)`% of high-accuracy participants) compared to `r comp_tally %>% filter(source=="SPR") %>% pull(good)`% of all SPR reads and `r comp_tally %>% filter(source=="SPR 1st story") %>% pull(good)`% of 1st SPR reads. These differences cannot be directly attributed to methods, as the participant populations differed. While both studies were conducted on Mturk, the quality of Mturk data has decreased from 2011 when the SPR was collected to 2020 when the A-maze was collected [@chmielewski2020].

For the remainder of the analyses, we use task performance as our exclusion metric for A-maze because it is more fine-grained and only analyze data from participants with at least 80% accuracy (in the gap between high-performers and low-performers). For the SPR comparison, we follow @futrellNaturalStoriesCorpus2020's criteria and exclude participants who got less than 5 of the comprehension questions correct. 

## How do A-maze and SPR compare in power and reliability?
 
```{r}
tokenization <- here("Data/SPR/all_stories.tok.txt") %>% 
  read_delim(delim="\t") %>% 
  mutate(Word_In_Story_Num=zone,
         Story_Num=item)

labs <- read_rds(here("Prep_code/natural_stories_surprisals.rds")) %>% 
  left_join(read_delim(here("Materials/natural_stories_sentences.tsv"), delim="\t")) %>% 
  select(word_num=Word_In_Sentence_Num, word=Word, sentence=Sentence, everything())

spr <- read_rds(here("Data/SPR/first.rds")) %>% left_join(tokenization) %>% left_join(labs) %>% select(WorkerId,rt,Story_Num,Sentence_Num, word_num)

maze <- read_rds(here("Data/maze_pre_error.rds")) %>% left_join(labs) %>% select(subject, rt, Story_Num, Sentence_Num, word_num)
```

```{r spr-split-half}
set.seed(42)
spr_subs <- spr %>% filter(!is.na(Story_Num)) %>% select(WorkerId, Story_Num) %>% unique() %>% sample_n(63) %>%  group_by(Story_Num) %>%
  mutate(half=ifelse(row_number() <= .5*n(),"spr1", "spr2"))
#180 participants


spr_split <- spr %>%
  inner_join(spr_subs) %>% 
  group_by(half,Story_Num, Sentence_Num, word_num) %>% summarize(spr=mean(rt)) %>% 
  pivot_wider(names_from=half, values_from=spr)
  
spr_sentence_avg <- spr_split%>% group_by(Story_Num, Sentence_Num) %>% 
  summarize(spr1=mean(spr1),
            spr2=mean(spr2))

spr_spr <- cor(spr_sentence_avg$spr1,spr_sentence_avg$spr2, use="complete.obs")
```

```{r maze-split-half}
set.seed(42)
maze_subs <- maze %>% select(subject, Story_Num) %>% unique() %>% sample_n(63)%>% group_by(Story_Num) %>%
  mutate(half=ifelse(row_number() <= .5*n(),"maze1", "maze2"))

maze_split <- maze %>% 
  left_join(maze_subs) %>% 
  group_by(half,Story_Num, Sentence_Num, word_num) %>% summarize(maze=mean(rt)) %>% 
  pivot_wider(names_from=half, values_from=maze)
  
maze_sentence_avg <- maze_split%>% group_by(Story_Num, Sentence_Num) %>% 
  summarize(maze1=mean(maze1),
            maze2=mean(maze2))

maze_maze <- cor(maze_sentence_avg$maze1,maze_sentence_avg$maze2, use="complete.obs")
```

```{r maze-spr}
summ_spr <- spr %>% inner_join(spr_subs)%>% group_by(Story_Num, Sentence_Num, word_num,half) %>% summarize(spr=mean(rt, na.rm=T)) %>% rename(spr_half=half) %>% filter(!is.na(spr))
  
summ <- maze %>% inner_join(maze_subs) %>% group_by(Story_Num, Sentence_Num, word_num, half) %>% summarize(maze=mean(rt, na.rm=T)) %>% rename(maze_half=half) %>% filter(!is.na(maze))%>% inner_join(summ_spr)

sentence_avg <- summ %>% group_by(Story_Num, Sentence_Num, maze_half, spr_half) %>% 
  summarize(maze=mean(maze),
            spr=mean(spr))

sentence_avg11 <- sentence_avg %>% filter(maze_half=="maze1" & spr_half=="spr1")
sentence_avg12 <- sentence_avg %>% filter(maze_half=="maze1" & spr_half=="spr2")
sentence_avg21 <- sentence_avg %>% filter(maze_half=="maze2" & spr_half=="spr1")
sentence_avg22 <- sentence_avg %>% filter(maze_half=="maze2" & spr_half=="spr2")

spr_maze <-   mean(c(cor(sentence_avg11$spr,sentence_avg11$maze, use="complete.obs"),
  cor(sentence_avg12$spr,sentence_avg12$maze, use="complete.obs"),
  cor(sentence_avg21$spr,sentence_avg21$maze, use="complete.obs"),
  cor(sentence_avg22$spr,sentence_avg22$maze, use="complete.obs")))
```

(ref:scatter) Correlation between SPR and Maze data. RTs were averaged across participants per word and then averaged together within each sentence. RTs in ms. 

```{r scatter, out.height="20%", fig.width=6, fig.height=2, fig.pos="ht", fig.cap="(ref:scatter)"}
spr_collapse <- spr %>% 
  group_by(Story_Num, Sentence_Num,word_num) %>% 
  summarize(spr=mean(rt, na.rm=T)) %>% 
  group_by(Story_Num, Sentence_Num) %>%
  summarize(spr=mean(spr))

both <- maze %>% 
  group_by(Story_Num, Sentence_Num, word_num) %>% 
  summarize(maze=mean(rt, na.rm=T)) %>% 
  group_by(Story_Num, Sentence_Num) %>% 
  summarize(maze=mean(maze)) %>% 
  inner_join(spr_collapse)
ggplot(both, aes(x=maze, y=spr))+geom_point(alpha=.5, size=1)+labs(x="A-Maze", y="SPR")+coord_fixed(ratio=1)+geom_smooth(method="lm")
```

Our second question was whether A-maze is reliable. To assess reliability, we conducted split-half comparisons looking at the correlations between and within SPR and A-maze. If the methods picked up on the same effects, we would expect them to be correlated, with sentences that took longer to read in one method also taking longer in the other. We calculated the average RT at the sentence level to reduce variability from spillover patterns. The correlation between Maze and SPR was `r round(spr_maze,2)`, compared to `r round(spr_spr,2)` within SPR and `r round(maze_maze,2)` within Maze. See Figure \@ref(fig:scatter) for a visual comparison of overall Maze versus SPR RTs.  SPR data is about as correlated with Maze as with another sample of SPR data which provides some evidence that Maze and SPR are measuring the same effects. The superior within-method split-half correlation we see for Maze relative to SPR, despite the smaller number of participants, suggests that it is the more powerful of the two methods (higher signal-to-noise ratio), consistent with the findings of @boyceMazeMadeEasy2020 for factorial experimental designs with isolated-sentence presentation.

## Are the effects of surprisal linear?

We next considered the relationship between surprisal and Maze RT. Surprisal, a measure of overall word predictability in context, is linearly related to RT in eye-tracking and SPR [@smithEffectWordPredictability2013; @wilcoxPredictivePowerNeural2020; @goodkindPredictivePowerWord2018; @lukeLimitsLexicalPrediction2016]. If Maze is measuring the same language processes, we would expect to see a linear relationship between surprisal and Maze RT. 


```{r}
subject_att <- data_error_summ %>% 
  select(subject, is.attentive)

data_low_error <- data_filt %>% 
  left_join(subject_att, by="subject") %>% 
  filter(is.attentive==1) %>% 
  filter(type!="practice")

  data_error_free <- data_low_error %>% 
    mutate(word_num_mistake=ifelse(correct=="no", word_num,NA)) %>% 
    group_by(sentence, subject) %>% fill(word_num_mistake) %>% ungroup() %>% 
    mutate(after_mistake=word_num-word_num_mistake,
           after_mistake=ifelse(is.na(after_mistake),0,after_mistake)) %>% 
    filter(correct=="yes") %>% 
    filter(!after_mistake %in% c(1,2))
  
data_no_first <- data_error_free %>% filter(word_num!=0)

data_ready <- data_no_first %>% filter(rt>100 & rt<5000) %>% 
  select(subject, word_num, word, rt, sentence, type)

data_post_only <- data_no_first %>% filter(rt>100 & rt<5000) %>% 
  filter(!after_mistake==0) %>% 
    select(subject, word_num, word, rt, sentence, type)

data_pre_error <- data_no_first %>% filter(rt>100 & rt<5000) %>% 
  filter(after_mistake==0) %>% 
  select(subject, word_num, word, rt, sentence, type)

data_stories <- data_ready %>% select(type, subject) %>% 
  unique() %>% 
  group_by(type) %>% 
  tally()

data_anything_goes <- data_filt %>% 
  filter(type!="practice") %>% 
  filter(correct=="yes") %>% 
    select(subject, word_num, word, rt, sentence, type)

```

To assess the shape of the RT-surprisal relationship, we fit generalized additive models (GAMs). For these models, we only included data that occurred before any mistakes in the sentence; due to limits of model vocabulary, words with punctuation and some uncommon or proper nouns were excluded. We used surprisals generated by 4 different language models for robustness. (See Methods for details on language models, exclusions, and model fit.)

(ref:gam-cap) GAM results for the effect of current word surprisal (top) or previous word surprisal (bottom) on Maze reaction time (RT). Density of data is shown along the x-axis. The best-fit lines is from the mean estimated effect size across the bootstrap replicates, and the shaded areas indicate a 95\% bootstrap confidence interval on this effect size. For each of the 4 language models used, there is a linear relationship between current word surprisal and RT. The relationship between previous word surprisal and RT is much flatter. 

```{r gam, out.height="40%", fig.width=8, fig.height=3, fig.pos="ht", fig.cap="(ref:gam-cap)"}

gam1 <- read_rds(here("Analysis/bootstrapped_GAM_surprisal_predictions.rds")) %>%  ggplot( aes(x=surprisal, y=rt, ymin=CI_lower, ymax=CI_upper))+
  geom_line()+
  geom_ribbon(alpha=.3)+
  #facet_grid(s~model, scales="free_y")+
  facet_grid(s~model)+
  coord_cartesian(xlim=c(0,28))+
  labs(x="Surprisal (bits)", y="Effect on Reaction Time (ms)")+theme(axis.ticks.x=element_blank(), axis.title.x=element_blank(), axis.text.x=element_blank(), plot.margin=margin(t=0,r=0,b=0,l=0,unit="pt"))

all_mean_data <-  read_rds(here("Analysis/models/meaned_maze.rds"))

dens2 <-   ggplot(all_mean_data, aes(x=value))+
  geom_density(fill="gray",)+
  facet_grid(.~model)+
  labs(x="Surprisal (bits)", y="")+
    coord_cartesian(xlim=c(0,28))+
  theme(axis.text.y = element_blank(), strip.text=element_blank(), axis.ticks.y =element_blank(), axis.title.y=element_blank(), panel.grid=element_blank(), plot.margin = unit(c(0, 0, 0, 0), "cm"))

p2 = cowplot::align_plots(gam1, dens2, align = "v", axis="lr")
plot_grid(p2[[1]], p2[[2]], nrow=2, rel_heights = c(1, .3))


```

The main effects of current and previous word surprisals on RT are shown in Figure \@ref(fig:gam). Note that for each of the models, high-surprisal words are rare, with much of the data from words with between 0 and 15 bits of surprisal. All 4 models show a roughly linear relationship between current word surprisal and RT, especially in the region with more data. 

(ref:spr-gam-cap) GAM results for the effect of current word surprisal (top) or the surprisal of an earlier word, up to 3 words back on SPR RT data [@futrellNaturalStoriesCorpus2020]. Density of data is shown along the x-axis. The best-fit lines is from the mean estimated effect size across the bootstrap replicates, and the shaded areas indicate a 95\% bootstrap confidence interval on this effect size.

```{r spr-gam, out.height="40%", fig.width=8, fig.height=4, fig.pos="ht", fig.cap="(ref:spr-gam-cap)"}
gam1 <- read_rds(here("Analysis/bootstrapped_spr_GAM_surprisal_predictions.rds")) %>%  ggplot( aes(x=surprisal, y=rt, ymin=CI_lower, ymax=CI_upper))+
  geom_line()+
  geom_ribbon(alpha=.3)+
  facet_grid(s~model)+
  coord_cartesian(xlim=c(0,28))+
  labs(x="Surprisal (bits)", y="Effect on Reaction Time (ms)")+theme(axis.ticks.x=element_blank(), axis.title.x=element_blank(), axis.text.x=element_blank(), plot.margin=margin(t=0,r=0,b=0,l=0,unit="pt"))

 all_mean_spr <-  read_rds(here("Analysis/models/meaned_spr.rds"))


dens2 <-   ggplot(all_mean_spr, aes(x=value))+
  geom_density(fill="gray",)+
  facet_grid(.~model)+
  labs(x="Surprisal (bits)", y="")+
    coord_cartesian(xlim=c(0,28))+
  theme(axis.text.y = element_blank(), strip.text=element_blank(), axis.ticks.y =element_blank(), axis.title.y=element_blank(), panel.grid=element_blank(), plot.margin = unit(c(0, 0, 0, 0), "cm"))

p2 = cowplot::align_plots(gam1, dens2, align = "v", axis="lr")
plot_grid(p2[[1]], p2[[2]], nrow=2, rel_heights = c(1, .2))

```

As a comparison, we also ran GAMs on the SPR data collected by @futrellNaturalStoriesCorpus2020. Previous work such as @smithEffectWordPredictability2013 has found positive relationships between RT and the surprisal of earlier words for SPR, so we include predictors from the current and the 3 prior words.  The relationship between surprisals and RT is shown in Figure \@ref(fig:spr-gam); note that the y-axis range is much narrower than for Maze. Both current and previous word surprisals have a roughly linear positive relationship to RT. The surprisal of the word two back also has an influence in some models. 

Comparing Maze and SPR, we see that both show a linear relationship, but Maze has much larger effects of surprisal on the current word.

## Does A-maze have less spillover?

One of the main claimed advantages of the Maze task is that it has better localization and less spillover than SPR. We examined how much spillover A-maze and SPR each had by fitting linear models with predictors from current and previous words. Large effects from previous words are evidence for spillover; effects of the current word that dwarf any lagged effects is evidence for localization.

We modeled reading time as a function of surprisal, frequency, and length as well as surprisal x length and frequency x length interactions. For all of these, we included the predictors for the current and previous word, and we centered, but did not rescale, all predictors. (See Methods for more details on these predictors and model fit process.) As with the GAM models, we used surprisal calculations from 4 different language models for robustness. 


(ref:coeffs) Point estimates and 95% credible intervals for coefficients predicted by fitted Bayesian regression models predicting A-maze RT. Units are in ms. Surprisal is per bit, length per character, and frequency per $log_2$ occurrence per billion words.

```{r coeffs-maze, out.height="35%", fig.width=8, fig.height=3, fig.pos="ht", fig.cap="(ref:coeffs)"}


test_plot <- read_rds(here("Analysis/models/maze_spr_for_graph.rds"))


# test_plot %>% filter(time %in% c("Current", "Previous")) %>% ggplot(aes(y=Term, x=Estimate, group=model, color=model, shape=model))+
#   ggstance::geom_pointrangeh(aes(xmin = Lower, xmax = Upper), position=position_dodge(width=.6))+
#   facet_grid(time~type)+
#   geom_vline(xintercept=0)+
#   scale_color_brewer(palette="Dark2")+
#   theme(legend.position = "right")+
#   labs(x="Coefficient estimate", y="", color="Model", shape="Model")


test_plot %>% filter(type=="Maze") %>% ggplot(aes(y=Term, x=Estimate, group=model, color=model, shape=model))+
  ggstance::geom_pointrangeh(aes(xmin = Lower, xmax = Upper), position=position_dodge2(width=.6, reverse=T))+
  facet_grid(.~time)+
  geom_vline(xintercept=0)+
ggthemes::scale_color_solarized(accent="violet")+
  theme(legend.position = "right")+
  labs(x="Coefficient estimates for Maze data", y="", color="Model", shape="Model")



```

The Maze linear model effects are shown in Figure \@ref(fig:coeffs-maze) (See also Appendix B for a table of effects). Across all models, there were consistent large effects of length and surprisal at the current word, but minimal effects of frequency. The lack of frequency effects is unexpected, but consistent with @shainLargescaleStudyEffects2019. There was a small interaction between surprisal and length at the current word. 

Crucially, the effects of previous word predictors are close to zero, and much smaller than the effects of surprisal and length of the current word, an indication that spillover is limited and effects are strongly localized.  


(ref:coeffs-spr) Point estimates and 95% credible intervals for coefficients predicted by fitted Bayesian regression models predicting SPR RT. Units are in ms. Surprisal is per bit, length per character, and frequency per $log_2$ occurrence per billion words.

```{r coeffs-spr, out.height="35%", fig.width=8, fig.height=3, fig.pos="ht", fig.cap="(ref:coeffs-spr)"}
test_plot %>% filter(type=="SPR") %>% mutate(time=factor(time, levels=c("Current","Previous","2Previous","3Previous")))%>% ggplot(aes(y=Term, x=Estimate, group=model, color=model, shape=model))+
  ggstance::geom_pointrangeh(aes(xmin = Lower, xmax = Upper), position=position_dodge2(width=.6, reverse=T))+
  facet_grid(.~time)+
  geom_vline(xintercept=0)+
ggthemes::scale_color_solarized(accent="violet")+
  theme(legend.position = "right")+
  labs(x="Coefficient estimate for SPR data", y="", color="Model", shape="Model")

```


We ran similar models for SPR, although to account for known spillover effects, we consider predictors from the current and 3 previous words. Due to issues fitting models, the details of the models differed (see Methods).  The SPR coefficients are shown in Figure \@ref(fig:coeffs-spr) (see also Appendix B for a table of coefficients). Surprisal, length, and frequency effects are all evident for the current word and surprisal and frequency show effects from the previous word as well. Unlike for Maze, with SPR there is not a clear diminishing of the size of the effects as one goes from current word to prior word predictors. 

Whereas Maze showed surprisal effects in the 10 to 25 ms/bit range and length effects 15 to 20 ms/character range, SPR effects are about 1-2 ms per bit or character. This difference in effect size is disproportionate to the overall speed of the methods; the predicted intercept for the Maze task was roughly 880 ms and for SPR was roughly 360 ms. Thus Maze is is 2-3 times as slow as SPR but has roughly 10 times larger effects. 


## Which language model fits best? 

Our last analysis question is whether some of the language models fit the human RT data better than others.  We assessed each model's fit to A-maze data using log likelihood and R-squared. Then we did a nested model comparison, looking at whether a model with multiple surprisal predictors (ex, GRNN and GPT-2) had a better fit than a model with only one (ex GRNN alone).  

As shown in Table \@ref(tab:maze-compare), GPT-2 provides a lot of additional predictive value over each other model, GRNN provides a lot over 5-gram and TXL and a little complementary information over GPT-2. TXL provides a lot over 5-gram, and 5-gram provides little over any model. The single-model measures of log likelihood confirm this hierarchy, as GPT-2 is better than GRNN is better than TXL is better than 5-gram.

```{r maze-compare}

read_rds(here("Analysis/maze_model_compare.rds")) %>% knitr::kable(format="latex", position="ht",caption="Results of model comparisons on Maze data. Each row shows the additional predictive value gained from adding that model to another model. F values and p values from ANOVA tests between 1-surprisal-source and 2-source models are reported. We also report log likelihoods of models with only one surprisal source and the r-squared correlation between the model's predictions and the data.")


```

We followed the same process for the SPR data with results shown in Table \@ref(tab:spr-compare).  For SPR, GPT-2 and 5-gram models contain some value over each other model, which is less clear for TXL and GRNN. In terms of log likelihoods, we find that GPT-2 is better than 5-gram is better than GRNN is better than TXL, although differences are small. The relatively good fit of 5-gram models to SPR data compared with neural models matches results from @huSystematicAssessmentSyntactic2020 and @wilcoxPredictivePowerNeural2020, and contrasts with the Maze results, where the 5-gram model had the worst fit and did not provide additional predictive value over the other models. 

```{r}
maze <- read_rds(here("Analysis/maze_model_compare.rds"))
spr <- read_rds(here("Analysis/spr_model_compare.rds"))
```

As an overall measure of fit to data, we calculate multiple R-squared for the single surprisal source models for both A-maze and SPR. The models predict A-maze better than SPR with R-squared values for A-maze ranging from `r maze %>% filter(Model=="5-gram") %>% pull(r_squared)` for the 5-gram model to `r maze %>% filter(Model=="GPT-2") %>% pull(r_squared)` for GPT-2. For SPR, the R-squared values range from from `r spr %>% filter(Model=="5-gram") %>% pull(r_squared)` to `r spr %>% filter(Model=="GPT-2") %>% pull(r_squared)`. This pattern suggests that the effect size differences are not due merely to the larger overall reading time for A-maze, but that instead A-maze is more sensitive to surprisal and length effects. 


```{r spr-compare}

read_rds(here("Analysis/spr_model_compare.rds")) %>% knitr::kable(format="latex", position="ht",caption="Results of model comparisons on SPR data. Each row shows the additional predictive value gained from adding that model to another model. F values and p values from ANOVA tests between 1-surprisal-source and 2-source models are reported. We also report log likelihoods of models with only one surprisal source and the r-squared correlation between the model's predictions and the data.")

```


# Discussion

We introduced error-correction Maze, a tweak on the presentation of Maze materials that makes Maze feasible for multi-sentence passages. We then used A-maze distractors and the error-correction Maze presentation to gather data on participants reading stories from the Natural Stories corpus in the Maze. As laid out in the Introduction, this current study addressed five main questions. 

First, we found that participants could read and comprehend the 1000 word stories, despite the slowness and added overhead of reading in the Maze task. This result expands the domain of materials usable with Maze beyond targeted single-sentence items to longer, naturalistic texts with sentence-to-sentence coherency. 

Second, we took advantage of the pre-existing SPR corpus on Natural Stories to compare the RT profiles between Maze and SPR.  Maze and SPR pick up on similar features in words, as shown by the high correlations between Maze and SPR RTs on the sentence level. 

Third, we addressed whether the A-maze RT for a word showed a linear relationship with that word's surprisal. We found that A-maze RTs are linearly related to surprisal, matching the functional profile found with other incremental processing methods. 

Fourth, we compared the spillover profiles between Maze and SPR. For Maze, we found large effects of the current words surprisal and length, which dwarfed any spillover effects from previous word predictors. In contrast, for SPR, we found effects of roughly equal sizes from the current and previous words. Overall, Maze is a slower task than SPR, but it also has much larger effects of length and surprisal, perhaps due to requiring more focus and thus generating less noisy data. 

Lastly, we examined how different language models fare at predicting human RT data. We found that overall, the models were more predictive of the A-maze data than SPR data; however, the hierarchy of the model's predictive performance also differed between the A-maze and SPR datasets. This difference suggests that how predictive a language model is of RTs may depend on the task.  Further comparisons between different processing methods on the same materials could be useful for identifying how task demands influence language processing [ex. @bartekSearchOnlineLocality2011]. 


Overall, A-maze has good localization, although some  models showed small but statistically reliable effects of the past word. On the whole, however, our results support the idea that Maze forces language processing to be close to word-by-word, and thus the Maze task can be used under the assumption that the RT of a word primarily reflects its own properties and not those of earlier words. 

## Limitations

While we expect these patterns of results reflect features of the A-maze task, the effects could be moderated by quirks of the materials or the participant population. We excluded a large number of participants for having low accuracy on the task and appearing to guess randomly. We compared RTs collected on the A-maze task to SPR RTs previously collected on the same corpus, but we did not randomly assign participants to SPR and Maze conditions. This study suggests that A-maze is a localized and widely-usable method, but only broader applications can confirm these findings.

## Future directions

Error-correction Maze  reduces perverse incentives from the desire to complete the task quickly compared to traditional Maze, but clicking randomly is still more efficient than doing the task. In discussing this work, we received the suggestion that one way to further disincentivize random clicking would be to add a pause when a participant makes a mistake, forcing them to wait some short period of time (ex 500ms) before correcting their mistake. This delay would make randomly hitting buttons slower than doing the task as intended. 

Error-correction Maze records RTs for words after a participant makes a mistake in the sentence. In our analyses, we excluded this post-error data, but we believe it is an open question whether data from after a participant makes a mistake is usable. That is, does it show the same profile as RTs from pre-error words, or are there traces from recovering from the mistake, and if so, how long do these effects take to fade? Whether post-mistake data is high-quality and trustworthy enough to be included in analyses is hard-to-assess; if it can be used, it would make the Maze task more data efficient.  

The Maze task is versatile and can be used or adapted for a wide range of materials and questions of interest. The extreme incrementality makes the Maze task a good target for any question that requires precisely determining the locus of incremental processing difficulty.  We encourage researchers to use Maze as an incremental processing method, alone or in comparison with other methods, and we suggest that the error-correction mode be the default choice for presenting Maze materials. 


 
\newpage

# References

\begingroup
\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}

<div id="refs" custom-style="Bibliography"></div>
\endgroup

# Appendix A
The beginning of one of the stories. This excerpt is the first 200 words of a 1000 word story.  

Tulip mania was a period in the Dutch Golden Age during which contract prices for bulbs of the recently introduced tulip reached extraordinarily high levels and then suddenly collapsed. At the peak of tulip mania in February sixteen thirty-seven, tulip contracts sold for more than ten times the annual income of a skilled craftsman. It is generally considered the first recorded economic bubble. The tulip, introduced to Europe in the mid sixteenth century from the Ottoman Empire, became very popular in the United Provinces, which we now know as the Netherlands. Tulip cultivation in the United Provinces is generally thought to have started in earnest around fifteen ninety-three, after the Flemish botanist Charles de l'Ecluse had taken up a post at the University of Leiden and established a botanical garden, which is famous as one of the oldest in the world. There, he planted his collection of tulip bulbs that the Emperor's ambassador sent to him from Turkey, which were able to tolerate the harsher conditions of the northern climate. It was shortly thereafter that the tulips began to grow in popularity. The flower rapidly became a coveted luxury item and a status symbol, and a profusion of varieties followed.

The first 2 out of the 6 comprehension questions.

When did tulip mania reach its peak? 1630's, 1730's

From which country did tulips come to Europe? Turkey, Egypt

# Appendix B

```{r pre-error}
summ <- read_rds(here("Analysis/models/brms_maze_summ.rds")) %>% pivot_wider(names_from="model", values_from=c(`Estimate`)) %>% 
  mutate(Term=factor(Term, 
                        levels=c("Intercept", 
                                 "surprisal",
                                 "len",
                                 "freq",
                                 "surprisal:len",
                                 "len:freq",
                                 "prev_surp",
                                 "prev_len",
                                 "prev_freq",
                                 "prev_surp:prev_len",
                                 "prev_len:prev_freq")
                                 )) %>% 
  arrange(Term) %>% 
  mutate(Term=c("Intercept", "Surprisal", "Length", "Frequency",
                  "Surp x Length", "Freq x Length", "Past Surprisal",
                  "Past Length", "Past Freq", "Past Surp x Length", "Past Freq x Length")) 

knitr::kable(summ, format="latex", position="ht",caption="Predictions from fitted Bayesian regression models. All terms were centered, but not rescaled. Units are in ms. Surprisal is per bit, length per character, and frequency per $log_2$ occurance per billion words. Interval is 2.5th quantile to 97.5th quantile of model draws.")%>%   kable_styling(font_size = 10)
```


```{r spr-table}

summ_spr <- read_rds(here("Analysis/models/lmer_spr_summ.rds")) %>% 
  pivot_wider(names_from="model", values_from=c(`Estimate`)) %>% 
  mutate(Term=factor(Term, 
                        levels=c("(Intercept)", 
                                 "surp_center",
                                 "length_center",
                                 "freq_center",
                                 "surp_center:length_center",
                                 "length_center:freq_center",
                                 "past_surp_center",
                                 "past_length_center",
                                 "past_freq_center",
                                 "past_surp_center:past_length_center",
                                 "past_length_center:past_freq_center",
                                 "past2_surp_center",
                                 "past2_length_center",
                                 "past2_freq_center",
                                 "past2_surp_center:past2_length_center",
                                 "past2_length_center:past2_freq_center",
                                 "past3_surp_center",
                                 "past3_length_center",
                                 "past3_freq_center",
                                 "past3_surp_center:past3_length_center",
                                 "past3_length_center:past3_freq_center"
                                 )
                                 )) %>% 
  arrange(Term) %>% 
  mutate(Term=c("Intercept", "Surprisal", "Length", "Frequency","Surp x Length", "Freq x Length",
                "Past Surprisal", "Past Length", "Past Freq", "Past Surp x Length", "Past Freq x Length",
                "2Past Surprisal", "2Past Length", "2Past Freq", "2Past Surp x Length", "2Past Freq x Length",
                "3Past Surprisal", "3Past Length", "3Past Freq", "3Past Surp x Length", "3Past Freq x Length")) 

knitr::kable(summ_spr, format="latex", position="ht",caption="Predictions from fitted regression models for SPR data. All terms were centered, but not rescaled. Units are in ms. Surprisal is per bit, length per character, and frequency per $log_2$ occurance per billion words. Uncertainty interval is +/- 1.97 standard error.") %>%   kable_styling(font_size = 10)

```

# Appendix C

```{r gam-linearity-table}

gam_linear <- read_rds(here("Analysis/models/gams_linearity_test.rds")) %>% 
  mutate(Term=factor(Term, 
                        levels=c("s(surprisal)",
                                 "ti(freq)",
                                 "ti(len)",
                                 "s(prev_surp)",
                                 "ti(prev_freq)",
                                 "ti(prev_len)",
                                 "(Intercept)",
                                 "surprisal",
                                 "prev_surp"
                                 )
                                 )) %>% 
  arrange(Term) %>% filter(Term %in% c("s(surprisal)", "s(prev_surp)", "surprisal","prev_surp")) %>% 
  mutate(Term=c("Spline Surprisal", "Spline Past Surprisal", "Linear Surprisal", "Linear Past Surprisal")) 

knitr::kable(gam_linear, format="latex", position="ht",caption="Comparison of significances for linear and spline terms of suprisals from a GAM. We fit GAM models with current and past word surprisal as parametric terms, current and past word surprisal and spline terms, and current and past frequence and length as tensors to predict reading time. Here we show the estimated pvalues for the linear and spline surprisal terms at current and past words. The spline terms account for any non-linear surprisal effects.") %>%   kable_styling(font_size = 10)

```

# Appendix D
(ref:gam-grid) CAPTION GOES HERE

```{r gam-grid, out.height="100%",  fig.pos="ht", fig.cap="(ref:gam-grid)"}


knitr::include_graphics(here("Analysis/models/gam_grid.png"))

```

